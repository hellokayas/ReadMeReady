SourceCode
"import path from 'path';
import { AutodocRepoConfig } from '../../../types.js';
import { spinnerSuccess, updateSpinnerText } from '../../spinner.js';
import { processRepository } from '../index/processRepository.js';
import {
  printModelDetails,
  totalIndexCostEstimate,
} from '../../utils/LLMUtil.js';
import chalk from 'chalk';

export const estimate = async ({
  name,
  repositoryUrl,
  root,
  output,
  llms,
  priority,
  maxConcurrentCalls,
  addQuestions,
  ignore,
  filePrompt,
  folderPrompt,
  chatPrompt,
  contentType,
  targetAudience,
  linkHosted,
}: AutodocRepoConfig) => {
  const json = path.join(output, 'docs', 'json/');

  /**
   * Dry run of the processRepository command
   * to get the estimated price for indexing the repo
   */
  updateSpinnerText('Estimating cost...');

  const runDetails = await processRepository(
    {
      name,
      repositoryUrl,
      root,
      output: json,
      llms,
      priority,
      maxConcurrentCalls,
      addQuestions,
      ignore,
      filePrompt,
      folderPrompt,
      chatPrompt,
      contentType,
      targetAudience,
      linkHosted,
    },
    true,
  );
  spinnerSuccess();

  /**
   * Print Results
   */
  printModelDetails(Object.values(runDetails));
  const total = totalIndexCostEstimate(Object.values(runDetails));
  console.log(
    chalk.redBright(
      `Cost estimate to process this repository: $${total.toFixed(
        2,
      )}\nThis is just an estimate. Actual cost may vary.\nIt recommended that you set a limit in your OpenAI account to prevent unexpected charges.`,
    ),
  );
};

import fs from 'node:fs/promises';
import path from 'path';
import {
  AutodocRepoConfig,
  FileSummary,
  FolderSummary,
  ProcessFile,
} from '../../../types';
import { traverseFileSystem } from '../../utils/traverseFileSystem.js';
import { spinnerSuccess, updateSpinnerText } from '../../spinner.js';
import { getFileName } from '../../utils/FileUtil.js';

export const convertJsonToMarkdown = async ({
  name: projectName,
  root: inputRoot,
  output: outputRoot,
  filePrompt: filePrompt,
  folderPrompt: folderPrompt,
  contentType: contentType,
  targetAudience: targetAudience,
  linkHosted: linkHosted,
}: AutodocRepoConfig) => {
  /**
   * Count the number of files in the project
   */
  let files = 0;
  await traverseFileSystem({
    inputPath: inputRoot,
    projectName,
    processFile: () => {
      files++;
      return Promise.resolve();
    },
    ignore: [],
    filePrompt,
    folderPrompt,
    contentType,
    targetAudience,
    linkHosted,
  });

  /**
   * Create markdown files for each code file in the project
   */

  const processFile: ProcessFile = async ({
    fileName,
    filePath,
  }): Promise<void> => {
    const content = await fs.readFile(filePath, 'utf-8');

    // TODO: Handle error
    if (!content) return;

    const markdownFilePath = path
      .join(outputRoot, filePath)
      .replace(inputRoot, '');

    /**
     * Create the output directory if it doesn't exist
     */
    try {
      await fs.mkdir(markdownFilePath.replace(fileName, ''), {
        recursive: true,
      });
    } catch (error) {
      console.error(error);
      return;
    }

    const { url, summary, questions } =
      fileName === 'summary.json'
        ? (JSON.parse(content) as FolderSummary)
        : (JSON.parse(content) as FileSummary);

    /**
     * Only include the file if it has a summary
     */
    const markdown =
      summary.length > 0
        ? `[View code on GitHub](${url})\n\n${summary}\n${
            questions ? '## Questions: \n ' + questions : ''
          }`
        : '';

    const outputPath = getFileName(markdownFilePath, '.', '.md');
    await fs.writeFile(outputPath, markdown, 'utf-8');
  };

  updateSpinnerText(`Creating ${files} markdown files...`);
  await traverseFileSystem({
    inputPath: inputRoot,
    projectName,
    processFile,
    ignore: [],
    filePrompt,
    folderPrompt,
    contentType,
    targetAudience,
    linkHosted,
  });
  spinnerSuccess(`Created ${files} markdown files...`);
};

import { OpenAIEmbeddings } from 'langchain/embeddings';
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';
import * as fs from 'fs';
import { Document } from 'langchain/document';
import { BaseDocumentLoader } from 'langchain/document_loaders';
import path from 'path';
import { AutodocRepoConfig } from '../../../types.js';
import { HNSWLib } from '../../../langchain/hnswlib.js';

async function processFile(filePath: string): Promise<Document> {
  return await new Promise<Document>((resolve, reject) => {
    fs.readFile(filePath, 'utf8', (err, fileContents) => {
      if (err) {
        reject(err);
      } else {
        const metadata = { source: filePath };
        const doc = new Document({
          pageContent: fileContents,
          metadata: metadata,
        });
        resolve(doc);
      }
    });
  });
}

async function processDirectory(directoryPath: string): Promise<Document[]> {
  const docs: Document[] = [];
  let files: string[];
  try {
    files = fs.readdirSync(directoryPath);
  } catch (err) {
    console.error(err);
    throw new Error(
      `Could not read directory: ${directoryPath}. Did you run \`sh download.sh\`?`,
    );
  }
  for (const file of files) {
    const filePath = path.join(directoryPath, file);
    const stat = fs.statSync(filePath);
    if (stat.isDirectory()) {
      const newDocs = processDirectory(filePath);
      const nestedDocs = await newDocs;
      docs.push(...nestedDocs);
    } else {
      const newDoc = processFile(filePath);
      const doc = await newDoc;
      docs.push(doc);
    }
  }
  return docs;
}

class RepoLoader extends BaseDocumentLoader {
  constructor(public filePath: string) {
    super();
  }
  async load(): Promise<Document[]> {
    return await processDirectory(this.filePath);
  }
}

export const createVectorStore = async ({
  root,
  output,
}: AutodocRepoConfig): Promise<void> => {
  const loader = new RepoLoader(root);
  const rawDocs = await loader.load();
  /* Split the text into chunks */
  const textSplitter = new RecursiveCharacterTextSplitter({
    chunkSize: 8000,
    chunkOverlap: 100,
  });
  const docs = await textSplitter.splitDocuments(rawDocs);
  /* Create the vectorstore */
  const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());
  await vectorStore.save(output);
};

import path from 'path';
import { AutodocRepoConfig } from '../../../types.js';
import { spinnerSuccess, updateSpinnerText } from '../../spinner.js';
import { convertJsonToMarkdown } from './convertJsonToMarkdown.js';
import { createVectorStore } from './createVectorStore.js';
import { processRepository } from './processRepository.js';

export const index = async ({
  name,
  repositoryUrl,
  root,
  output,
  llms,
  priority,
  maxConcurrentCalls,
  addQuestions,
  ignore,
  filePrompt,
  folderPrompt,
  chatPrompt,
  contentType,
  targetAudience,
  linkHosted,
}: AutodocRepoConfig) => {
  const json = path.join(output, 'docs', 'json/');
  const markdown = path.join(output, 'docs', 'markdown/');
  const data = path.join(output, 'docs', 'data/');

  /**
   * Traverse the repository, call LLMS for each file,
   * and create JSON files with the results.
   */

  updateSpinnerText('Processing repository...');
  await processRepository({
    name,
    repositoryUrl,
    root,
    output: json,
    llms,
    priority,
    maxConcurrentCalls,
    addQuestions,
    ignore,
    filePrompt,
    folderPrompt,
    chatPrompt,
    contentType,
    targetAudience,
    linkHosted,
  });
  updateSpinnerText('Processing repository...');
  spinnerSuccess();

  /**
   * Create markdown files from JSON files
   */
  updateSpinnerText('Creating markdown files...');
  await convertJsonToMarkdown({
    name,
    repositoryUrl,
    root: json,
    output: markdown,
    llms,
    priority,
    maxConcurrentCalls,
    addQuestions,
    ignore,
    filePrompt,
    folderPrompt,
    chatPrompt,
    contentType,
    targetAudience,
    linkHosted,
  });
  spinnerSuccess();

  updateSpinnerText('Create vector files...');
  await createVectorStore({
    name,
    repositoryUrl,
    root: markdown,
    output: data,
    llms,
    priority,
    maxConcurrentCalls,
    addQuestions,
    ignore,
    filePrompt,
    folderPrompt,
    chatPrompt,
    contentType,
    targetAudience,
    linkHosted,
  });
  spinnerSuccess();
};

export default {
  index,
};

import fs from 'node:fs/promises';
import path from 'node:path';
import { Md5 } from 'ts-md5';
import { OpenAIChat } from 'langchain/llms';
import { encoding_for_model } from '@dqbd/tiktoken';
import { APIRateLimit } from '../../utils/APIRateLimit.js';
import {
  createCodeFileSummary,
  createCodeQuestions,
  folderSummaryPrompt,
} from './prompts.js';
import {
  AutodocRepoConfig,
  FileSummary,
  FolderSummary,
  LLMModelDetails,
  LLMModels,
  ProcessFile,
  ProcessFolder,
} from '../../../types.js';
import { traverseFileSystem } from '../../utils/traverseFileSystem.js';
import {
  spinnerSuccess,
  stopSpinner,
  updateSpinnerText,
} from '../../spinner.js';
import {
  getFileName,
  githubFileUrl,
  githubFolderUrl,
} from '../../utils/FileUtil.js';
import { models } from '../../utils/LLMUtil.js';
import { selectModel } from './selectModel.js';

export const processRepository = async (
  {
    name: projectName,
    repositoryUrl,
    root: inputRoot,
    output: outputRoot,
    llms,
    priority,
    maxConcurrentCalls,
    addQuestions,
    ignore,
    filePrompt,
    folderPrompt,
    contentType,
    targetAudience,
    linkHosted,
  }: AutodocRepoConfig,
  dryRun?: boolean,
) => {
  const rateLimit = new APIRateLimit(maxConcurrentCalls);

  const callLLM = async (
    prompt: string,
    model: OpenAIChat,
  ): Promise<string> => {
    return rateLimit.callApi(() => model.call(prompt));
  };

  const isModel = (model: LLMModelDetails | null): model is LLMModelDetails =>
    model !== null;

  const processFile: ProcessFile = async ({
    fileName,
    filePath,
    projectName,
    contentType,
    filePrompt,
    targetAudience,
    linkHosted,
  }): Promise<void> => {
    const content = await fs.readFile(filePath, 'utf-8');

    /**
     * Calculate the checksum of the file content
     */
    const newChecksum = await calculateChecksum([content]);

    /**
     * if an existing .json file exists,
     * it will check the checksums and decide if a reindex is needed
     */
    const reindex = await shouldReindex(
      path.join(outputRoot, filePath.substring(0, filePath.lastIndexOf('\\'))),
      fileName.replace(/\.[^/.]+$/, '.json'),
      newChecksum,
    );
    if (!reindex) {
      return;
    }

    const markdownFilePath = path.join(outputRoot, filePath);
    const url = githubFileUrl(repositoryUrl, inputRoot, filePath, linkHosted);

    const summaryPrompt = createCodeFileSummary(
      projectName,
      projectName,
      content,
      contentType,
      filePrompt,
    );
    const questionsPrompt = createCodeQuestions(
      projectName,
      projectName,
      content,
      contentType,
      targetAudience,
    );

    const prompts = addQuestions
      ? [summaryPrompt, questionsPrompt]
      : [summaryPrompt];

    const model = selectModel(prompts, llms, models, priority);

    if (!isModel(model)) {
      // console.log(`Skipped ${filePath} | Length ${max}`);
      return;
    }

    const encoding = encoding_for_model(model.name);
    const summaryLength = encoding.encode(summaryPrompt).length;
    const questionLength = encoding.encode(questionsPrompt).length;

    try {
      if (!dryRun) {
        /** Call LLM */
        const response = await Promise.all(
          prompts.map(async (prompt) => callLLM(prompt, model.llm)),
        );

        /**
         * Create file and save to disk
         */
        const file: FileSummary = {
          fileName,
          filePath,
          url,
          summary: response[0],
          questions: addQuestions ? response[1] : '',
          checksum: newChecksum,
        };

        const outputPath = getFileName(markdownFilePath, '.', '.json');
        const content =
          file.summary.length > 0 ? JSON.stringify(file, null, 2) : '';

        /**
         * Create the output directory if it doesn't exist
         */
        try {
          await fs.mkdir(markdownFilePath.replace(fileName, ''), {
            recursive: true,
          });
          await fs.writeFile(outputPath, content, 'utf-8');
        } catch (error) {
          console.error(error);
          return;
        }

        // console.log(`File: ${fileName} => ${outputPath}`);
      }

      /**
       * Track usage for end of run summary
       */
      model.inputTokens += summaryLength;
      if (addQuestions) model.inputTokens += questionLength;
      model.total++;
      model.outputTokens += 1000;
      model.succeeded++;
    } catch (e) {
      console.log(e);
      console.error(`Failed to get summary for file ${fileName}`);
      model.failed++;
    }
  };

  const processFolder: ProcessFolder = async ({
    folderName,
    folderPath,
    projectName,
    contentType,
    folderPrompt,
    shouldIgnore,
    linkHosted,
  }): Promise<void> => {
    /**
     * For now we don't care about folders
     *
     * TODO: Add support for folders during estimation
     */
    if (dryRun) return;

    const contents = (await fs.readdir(folderPath)).filter(
      (fileName) => !shouldIgnore(fileName),
    );

    /**
     * Get the checksum of the folder
     */
    const newChecksum = await calculateChecksum(contents);

    /**
     * If an existing summary.json file exists,
     * it will check the checksums and decide if a reindex is needed
     */
    const reindex = await shouldReindex(
      folderPath,
      'summary.json',
      newChecksum,
    );
    if (!reindex) {
      return;
    }

    // eslint-disable-next-line prettier/prettier
    const url = githubFolderUrl(
      repositoryUrl,
      inputRoot,
      folderPath,
      linkHosted,
    );
    const allFiles: (FileSummary | null)[] = await Promise.all(
      contents.map(async (fileName) => {
        const entryPath = path.join(folderPath, fileName);
        const entryStats = await fs.stat(entryPath);

        if (entryStats.isFile() && fileName !== 'summary.json') {
          const file = await fs.readFile(entryPath, 'utf8');
          return file.length > 0 ? JSON.parse(file) : null;
        }

        return null;
      }),
    );

    try {
      const files = allFiles.filter(
        (file): file is FileSummary => file !== null,
      );
      const allFolders: (FolderSummary | null)[] = await Promise.all(
        contents.map(async (fileName) => {
          const entryPath = path.join(folderPath, fileName);
          const entryStats = await fs.stat(entryPath);

          if (entryStats.isDirectory()) {
            try {
              const summaryFilePath = path.resolve(entryPath, 'summary.json');
              const file = await fs.readFile(summaryFilePath, 'utf8');
              return JSON.parse(file);
            } catch (e) {
              console.log(`Skipped: ${folderPath}`);
              return null;
            }
          }

          return null;
        }),
      );

      const folders = allFolders.filter(
        (folder): folder is FolderSummary => folder !== null,
      );

      const summaryPrompt = folderSummaryPrompt(
        folderPath,
        projectName,
        files,
        folders,
        contentType,
        folderPrompt,
      );

      const model = selectModel([summaryPrompt], llms, models, priority);

      if (!isModel(model)) {
        // console.log(`Skipped ${filePath} | Length ${max}`);
        return;
      }

      const summary = await callLLM(summaryPrompt, model.llm);

      const folderSummary: FolderSummary = {
        folderName,
        folderPath,
        url,
        files,
        folders: folders.filter(Boolean),
        summary,
        questions: '',
        checksum: newChecksum,
      };

      const outputPath = path.join(folderPath, 'summary.json');
      await fs.writeFile(
        outputPath,
        JSON.stringify(folderSummary, null, 2),
        'utf-8',
      );

      // console.log(`Folder: ${folderName} => ${outputPath}`);
    } catch (e) {
      console.log(e);
      console.log(`Failed to get summary for folder: ${folderPath}`);
    }
  };

  /**
   * Get the number of files and folders in the project
   */

  const filesAndFolders = async (): Promise<{
    files: number;
    folders: number;
  }> => {
    let files = 0;
    let folders = 0;

    await Promise.all([
      traverseFileSystem({
        inputPath: inputRoot,
        projectName,
        processFile: () => {
          files++;
          return Promise.resolve();
        },
        ignore,
        filePrompt,
        folderPrompt,
        contentType,
        targetAudience,
        linkHosted,
      }),
      traverseFileSystem({
        inputPath: inputRoot,
        projectName,
        processFolder: () => {
          folders++;
          return Promise.resolve();
        },
        ignore,
        filePrompt,
        folderPrompt,
        contentType,
        targetAudience,
        linkHosted,
      }),
    ]);

    return {
      files,
      folders,
    };
  };

  const { files, folders } = await filesAndFolders();

  /**
   * Create markdown files for each code file in the project
   */

  updateSpinnerText(`Processing ${files} files...`);
  await traverseFileSystem({
    inputPath: inputRoot,
    projectName,
    processFile,
    ignore,
    filePrompt,
    folderPrompt,
    contentType,
    targetAudience,
    linkHosted,
  });
  spinnerSuccess(`Processing ${files} files...`);

  /**
   * Create markdown summaries for each folder in the project
   */
  updateSpinnerText(`Processing ${folders} folders... `);
  await traverseFileSystem({
    inputPath: outputRoot,
    projectName,
    processFolder,
    ignore,
    filePrompt,
    folderPrompt,
    contentType,
    targetAudience,
    linkHosted,
  });
  spinnerSuccess(`Processing ${folders} folders... `);
  stopSpinner();

  /**
   * Print results
   */
  return models;
};

/**
 * Calculates the checksum of all the files in a folder
 */
async function calculateChecksum(contents: string[]): Promise<string> {
  const checksums: string[] = [];
  for (const content of contents) {
    const checksum = Md5.hashStr(content);
    checksums.push(checksum);
  }
  const concatenatedChecksum = checksums.join('');
  const finalChecksum = Md5.hashStr(concatenatedChecksum);
  return finalChecksum;
}

/**
 * Checks if a summary.json file exists.
 * If it does, compares the checksums to see if it
 * needs to be re-indexed or not.
 */

async function shouldReindex(
  contentPath: string,
  name: string,
  newChecksum: string,
): Promise<boolean> {
  const jsonPath = path.join(contentPath, name);

  let summaryExists = false;
  try {
    await fs.access(jsonPath);
    summaryExists = true;
  } catch (error) {}

  if (summaryExists) {
    const fileContents = await fs.readFile(jsonPath, 'utf8');
    const fileContentsJSON = JSON.parse(fileContents);

    const oldChecksum = fileContentsJSON.checksum;

    if (oldChecksum === newChecksum) {
      console.log(`Skipping ${jsonPath} because it has not changed`);
      return false;
    } else {
      console.log(`Reindexing ${jsonPath} because it has changed`);
      return true;
    }
  }
  //if no summary then generate one
  return true;
}

import { FileSummary, FolderSummary } from '../../../types.js';

export const createCodeFileSummary = (
  filePath: string,
  projectName: string,
  fileContents: string,
  contentType: string,
  filePrompt: string,
): string => {
  return `
    You are acting as a ${contentType} documentation expert for a project called ${projectName}.
    Below is the ${contentType} from a file located at \`${filePath}\`. 
    ${filePrompt}
    Do not say ""this file is a part of the ${projectName} project"".

    ${contentType}:
    ${fileContents}

    Response:

  `;
};

export const createCodeQuestions = (
  filePath: string,
  projectName: string,
  fileContents: string,
  contentType: string,
  targetAudience: string,
): string => {
  return `
    You are acting as a ${contentType} documentation expert for a project called ${projectName}.
    Below is the ${contentType} from a file located at \`${filePath}\`. 
    What are 3 questions that a ${targetAudience} might have about this ${contentType}? 
    Answer each question in 1-2 sentences. Output should be in markdown format.

    ${contentType}:
    ${fileContents}

    Questions and Answers:
    
  `;
};

export const folderSummaryPrompt = (
  folderPath: string,
  projectName: string,
  files: FileSummary[],
  folders: FolderSummary[],
  contentType: string,
  folderPrompt: string,
): string => {
  return `
    You are acting as a ${contentType} documentation expert for a project called ${projectName}.
    You are currently documenting the folder located at \`${folderPath}\`. 
    
    Below is a list of the files in this folder and a summary of the contents of each file:

    ${files.map((file) => {
      return `
        Name: ${file.fileName}
        Summary: ${file.summary}    

      `;
    })}

    And here is a list of the subfolders in this folder and a summary of the contents of each subfolder:

    ${folders.map((folder) => {
      return `
        Name: ${folder.folderName}
        Summary: ${folder.summary}    

      `;
    })}

    ${folderPrompt}
    Do not say ""this file is a part of the ${projectName} project"".
    Do not just list the files and folders.

    Response:
  `;
};

import { encoding_for_model } from '@dqbd/tiktoken';
import { LLMModelDetails, LLMModels, Priority } from '../../../types.js';

export const selectModel = (
  prompts: string[],
  llms: LLMModels[],
  models: Record<LLMModels, LLMModelDetails>,
  priority: Priority,
): LLMModelDetails | null => {
  if (priority === Priority.COST) {
    if (
      llms.includes(LLMModels.GPT3) &&
      models[LLMModels.GPT3].maxLength >
        getMaxPromptLength(prompts, LLMModels.GPT3)
    ) {
      return models[LLMModels.GPT3];
    } else if (
      llms.includes(LLMModels.GPT4) &&
      models[LLMModels.GPT4].maxLength >
        getMaxPromptLength(prompts, LLMModels.GPT4)
    ) {
      return models[LLMModels.GPT4];
    } else if (
      llms.includes(LLMModels.GPT432k) &&
      models[LLMModels.GPT432k].maxLength >
        getMaxPromptLength(prompts, LLMModels.GPT432k)
    ) {
      return models[LLMModels.GPT432k];
    } else {
      return null;
    }
  } else {
    if (llms.includes(LLMModels.GPT4)) {
      if (
        models[LLMModels.GPT4].maxLength >
        getMaxPromptLength(prompts, LLMModels.GPT4)
      ) {
        return models[LLMModels.GPT4];
      } else if (
        llms.includes(LLMModels.GPT432k) &&
        models[LLMModels.GPT432k].maxLength >
          getMaxPromptLength(prompts, LLMModels.GPT432k)
      ) {
        return models[LLMModels.GPT432k];
      } else {
        return null;
      }
    } else {
      return models[LLMModels.GPT3];
    }
  }

  function getMaxPromptLength(prompts: string[], model: LLMModels) {
    const encoding = encoding_for_model(model);
    return Math.max(...prompts.map((p) => encoding.encode(p).length));
  }
};

import chalk from 'chalk';
import inquirer from 'inquirer';
import fs from 'node:fs';
import path from 'node:path';
import { AutodocRepoConfig, LLMModels, Priority } from '../../../types.js';

export const makeConfigTemplate = (
  config?: AutodocRepoConfig,
): AutodocRepoConfig => {
  return {
    name: config?.name ?? '',
    repositoryUrl: config?.repositoryUrl ?? '',
    root: '.',
    output: './.autodoc',
    llms:
      config?.llms?.length ?? 0 > 0
        ? (config as AutodocRepoConfig).llms
        : [LLMModels.GPT3],
    priority: Priority.COST,
    maxConcurrentCalls: 25,
    addQuestions: true,
    ignore: [
      '.*',
      '*package-lock.json',
      '*package.json',
      'node_modules',
      '*dist*',
      '*build*',
      '*test*',
      '*.svg',
      '*.md',
      '*.mdx',
      '*.toml',
      '*autodoc*',
    ],
    filePrompt:
      config?.filePrompt ??
      'Write a detailed technical explanation of what this code does. \n\
      Focus on the high-level purpose of the code and how it may be used in the larger project.\n\
      Include code examples where appropriate. Keep you response between 100 and 300 words. \n\
      DO NOT RETURN MORE THAN 300 WORDS.\n\
      Output should be in markdown format.\n\
      Do not just list the methods and classes in this file.',
    folderPrompt:
      config?.folderPrompt ??
      'Write a technical explanation of what the code in this folder does\n\
      and how it might fit into the larger project or work with other parts of the project.\n\
      Give examples of how this code might be used. Include code examples where appropriate.\n\
      Be concise. Include any information that may be relevant to a developer who is curious about this code.\n\
      Keep you response under 400 words. Output should be in markdown format.\n\
      Do not just list the files and folders in this folder.',
    chatPrompt: '',
    contentType: 'code',
    targetAudience: 'smart developer',
    linkHosted: false,
  };
};

export const init = async (
  config: AutodocRepoConfig = makeConfigTemplate(),
) => {
  const configPath = path.join(config.root, 'autodoc.config.json');

  if (fs.existsSync(configPath)) {
    const questions = [
      {
        type: 'confirm',
        name: 'continue',
        message:
          'An autodoc.config.json file already exists in this location. The existing configuration will be overwritten. Do you want to continue? ',
        default: false,
      },
    ];

    const answers = await inquirer.prompt(questions);
    if (!answers.continue) {
      process.exit(0);
    }
  }

  const questions = [
    {
      type: 'input',
      name: 'name',
      message: chalk.yellow(`Enter the name of your repository:`),
      default: config.name,
    },
    {
      type: 'input',
      name: 'repositoryUrl',
      message: chalk.yellow(`Enter the GitHub URL of your repository:`),
      default: config.repositoryUrl,
    },
    {
      type: 'list',
      name: 'llms',
      message: chalk.yellow(
        `Select which LLMs you have access to (use GPT-3.5 Turbo if you aren't sure):`,
      ),
      default: 0,
      choices: [
        {
          name: 'GPT-3.5 Turbo',
          value: [LLMModels.GPT3],
        },
        {
          name: 'GPT-3.5 Turbo, GPT-4 8K (Early Access)',
          value: [LLMModels.GPT3, LLMModels.GPT4],
        },
        {
          name: 'GPT-3.5 Turbo, GPT-4 8K (Early Access), GPT-4 32K (Early Access)',
          value: [LLMModels.GPT3, LLMModels.GPT4, LLMModels.GPT432k],
        },
      ],
    },
    {
      type: 'input',
      name: 'filePrompt',
      message: chalk.yellow(
        `Enter the prompt you want to use for generating file-level documentation:`,
      ),
      default: config.filePrompt,
    },
    {
      type: 'input',
      name: 'folderPrompt',
      message: chalk.yellow(
        `Enter the prompt you want to use for generating folder-level documentation:`,
      ),
      default: config.folderPrompt,
    },
  ];

  const { name, repositoryUrl, llms, filePrompt, folderPrompt } =
    await inquirer.prompt(questions);

  const newConfig = makeConfigTemplate({
    ...config,
    name,
    repositoryUrl,
    llms,
    filePrompt,
    folderPrompt,
  });

  fs.writeFileSync(
    path.join(newConfig.root, 'autodoc.config.json'),
    JSON.stringify(newConfig, null, 2),
    'utf-8',
  );

  console.log(
    chalk.green('Autodoc initialized. Run `doc index` to get started.'),
  );
};

import { OpenAIChat } from 'langchain/llms';
import { LLMChain, ChatVectorDBQAChain, loadQAChain } from 'langchain/chains';
import { PromptTemplate } from 'langchain/prompts';
import { HNSWLib } from '../../../langchain/hnswlib.js';
import { LLMModels } from '../../../types.js';

const CONDENSE_PROMPT =
  PromptTemplate.fromTemplate(`Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.

Chat History:
{chat_history}
Follow Up Input: {question}
Standalone question:`);

// eslint-disable-next-line prettier/prettier
const makeQAPrompt = (projectName: string, repositoryUrl: string, contentType: string, chatPrompt: string, targetAudience: string) =>
  PromptTemplate.fromTemplate(
    `You are an AI assistant for a software project called ${projectName}. You are trained on all the ${contentType} that makes up this project.
  The ${contentType} for the project is located at ${repositoryUrl}.
You are given the following extracted parts of a technical summary of files in a ${contentType} and a question. 
Provide a conversational answer with hyperlinks back to GitHub.
You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.
Include lots of ${contentType} examples and links to the ${contentType} examples, where appropriate.
Assume the reader is a ${targetAudience} but is not deeply familiar with ${projectName}.
Assume the reader does not know anything about how the project is strucuted or which folders/files are provided in the context.
Do not reference the context in your answer. Instead use the context to inform your answer.
If you don't know the answer, just say ""Hmm, I'm not sure."" Don't try to make up an answer.
If the question is not about the ${projectName}, politely inform them that you are tuned to only answer questions about the ${projectName}.
Your answer should be at least 100 words and no more than 300 words.
Do not include information that is not directly relevant to the question, even if the context includes it.
Always include a list of reference links to GitHub from the context. Links should ONLY come from the context.

${
  chatPrompt.length > 0
    ? `Here are some additional instructions for answering questions about ${contentType}:\n${chatPrompt}`
    : ''
}

Question: {question}

Context:
{context}


Answer in Markdown:`,
  );

export const makeChain = (
  projectName: string,
  repositoryUrl: string,
  contentType: string,
  chatPrompt: string,
  targetAudience: string,
  vectorstore: HNSWLib,
  llms: LLMModels[],
  onTokenStream?: (token: string) => void,
) => {
  /**
   * GPT-4 or GPT-3
   */
  const llm = llms?.[1] ?? llms[0];
  const questionGenerator = new LLMChain({
    llm: new OpenAIChat({ temperature: 0.1, modelName: llm }),
    prompt: CONDENSE_PROMPT,
  });

  // eslint-disable-next-line prettier/prettier
  const QA_PROMPT = makeQAPrompt(projectName, repositoryUrl, contentType, chatPrompt, targetAudience);
  const docChain = loadQAChain(
    new OpenAIChat({
      temperature: 0.2,
      frequencyPenalty: 0,
      presencePenalty: 0,
      modelName: llm,
      streaming: Boolean(onTokenStream),
      callbackManager: {
        handleLLMNewToken: onTokenStream,
        handleLLMStart: () => null,
        handleLLMEnd: () => null,
      } as any,
    }),
    { prompt: QA_PROMPT },
  );

  return new ChatVectorDBQAChain({
    vectorstore,
    combineDocumentsChain: docChain,
    questionGeneratorChain: questionGenerator,
  });
};

import chalk from 'chalk';
import inquirer from 'inquirer';
import { marked } from 'marked';
import TerminalRenderer from 'marked-terminal';
import { OpenAIEmbeddings } from 'langchain/embeddings';
import path from 'path';
import { HNSWLib } from '../../../langchain/hnswlib.js';
import { AutodocRepoConfig, AutodocUserConfig } from '../../../types.js';
import { makeChain } from './createChatChain.js';
import { stopSpinner, updateSpinnerText } from '../../spinner.js';

const chatHistory: [string, string][] = [];

marked.setOptions({
  // Define custom renderer
  renderer: new TerminalRenderer(),
});

const displayWelcomeMessage = (projectName: string) => {
  console.log(chalk.bold.blue(`Welcome to the ${projectName} chatbot.`));
  console.log(
    `Ask any questions related to the ${projectName} codebase, and I'll try to help. Type 'exit' to quit.\n`,
  );
};

const clearScreenAndMoveCursorToTop = () => {
  process.stdout.write('\x1B[2J\x1B[0f');
};

export const query = async (
  { name, repositoryUrl, output, contentType, chatPrompt, targetAudience}: AutodocRepoConfig,
  { llms }: AutodocUserConfig,
) => {
  const data = path.join(output, 'docs', 'data/');
  const vectorStore = await HNSWLib.load(data, new OpenAIEmbeddings());
  const chain = makeChain(
    name,
    repositoryUrl,
    contentType,
    chatPrompt,
    targetAudience,
    vectorStore,
    llms,
    (token: string) => {
      stopSpinner();
      process.stdout.write(token);
    },
  );

  clearScreenAndMoveCursorToTop();
  displayWelcomeMessage(name);

  const getQuestion = async () => {
    const { question } = await inquirer.prompt([
      {
        type: 'input',
        name: 'question',
        message: chalk.yellow(`How can I help with ${name}?\n`),
      },
    ]);

    return question;
  };

  let question = await getQuestion();

  while (question !== 'exit') {
    updateSpinnerText('Thinking...');
    try {
      const { text } = await chain.call({
        question,
        chat_history: chatHistory,
      });

      chatHistory.push([question, text]);

      console.log('\n\nMarkdown:\n');
      console.log(marked(text));

      question = await getQuestion();
    } catch (error: any) {
      console.log(chalk.red(`Something went wrong: ${error.message}`));
      question = await getQuestion();
    }
  }
};

import chalk from 'chalk';
import inquirer from 'inquirer';
import fsSync from 'node:fs';
import fs from 'node:fs/promises';
import { userConfigFileName, userConfigFilePath } from '../../../const.js';
import { AutodocUserConfig, LLMModels } from '../../../types.js';

export const makeConfigTemplate = (
  config?: AutodocUserConfig,
): AutodocUserConfig => {
  return {
    llms: config?.llms ?? [LLMModels.GPT3],
  };
};

export const user = async (
  config: AutodocUserConfig = makeConfigTemplate(),
) => {
  if (fsSync.existsSync(userConfigFilePath)) {
    const questions = [
      {
        type: 'confirm',
        name: 'continue',
        message:
          'A user configuration already exists. It will be overwritten. Do you want to continue?',
        default: false,
      },
    ];

    const answers = await inquirer.prompt(questions);
    if (!answers.continue) {
      process.exit(0);
    }
  } else {
    try {
      fs.mkdir(userConfigFilePath.replace(userConfigFileName, ''), {
        recursive: true,
      });
    } catch (error) {
      console.error(error);
      process.exit(1);
    }
  }

  const questions = [
    {
      type: 'list',
      name: 'llms',
      message: chalk.yellow(
        `Select which LLMs you have access to (use GPT-3.5 Turbo if you aren't sure):`,
      ),
      default: 0,
      choices: [
        {
          name: 'GPT-3.5 Turbo',
          value: [LLMModels.GPT3],
        },
        {
          name: 'GPT-3.5 Turbo, GPT-4 8K (Early Access)',
          value: [LLMModels.GPT3, LLMModels.GPT4],
        },
        {
          name: 'GPT-3.5 Turbo, GPT-4 8K (Early Access), GPT-4 32K (Early Access)',
          value: [LLMModels.GPT3, LLMModels.GPT4, LLMModels.GPT432k],
        },
      ],
    },
  ];

  const { llms } = await inquirer.prompt(questions);

  const newConfig = makeConfigTemplate({
    ...config,
    llms,
  });

  await fs.writeFile(
    userConfigFilePath,
    JSON.stringify(newConfig, null, 2),
    'utf-8',
  );

  console.log(
    chalk.green(
      'Autodoc user configuration saved. Run `doc q` to start querying.',
    ),
  );
};

import ora from 'ora';

const spinner = ora({
  // make a singleton so we don't ever have 2 spinners
  spinner: 'dots',
});

export const updateSpinnerText = (message: string) => {
  if (spinner.isSpinning) {
    spinner.text = message;
    return;
  }
  spinner.start(message);
};

export const stopSpinner = () => {
  if (spinner.isSpinning) {
    spinner.stop();
  }
};

export const spinnerError = (message?: string) => {
  if (spinner.isSpinning) {
    spinner.fail(message);
  }
};
export const spinnerSuccess = (message?: string) => {
  if (spinner.isSpinning) {
    spinner.succeed(message);
  }
};
export const spinnerInfo = (message: string) => {
  spinner.info(message);
};

export class APIRateLimit {
  private queue: (() => void)[] = [];
  private inProgress = 0;

  constructor(private maxConcurrentCalls: number = 50) {}

  async callApi<T>(apiFunction: () => Promise<T>): Promise<T> {
    return new Promise<T>((resolve, reject) => {
      const executeCall = async () => {
        this.inProgress++;
        try {
          const result = await apiFunction();
          resolve(result);
        } catch (error) {
          reject(error);
        } finally {
          this.inProgress--;
          this.dequeueAndExecute();
        }
      };

      this.queue.push(executeCall);

      // Trigger the dequeue and execute operation when there are available slots for concurrent calls
      if (this.inProgress < this.maxConcurrentCalls) {
        this.dequeueAndExecute();
      }
    });
  }

  private dequeueAndExecute() {
    while (this.queue.length > 0 && this.inProgress < this.maxConcurrentCalls) {
      const nextCall = this.queue.shift();
      if (nextCall) {
        nextCall();
      }
    }
  }
}

export function getFileName(
  input: string,
  delimiter = '.',
  extension = '.md',
): string {
  const lastDelimiterIndex = input.lastIndexOf(delimiter);
  if (lastDelimiterIndex === -1) {
    // delimiter not found in string
    return input + extension;
  } else {
    return input.slice(0, lastDelimiterIndex) + extension;
  }
}

export const githubFileUrl = (
  githubRoot: string,
  inputRoot: string,
  filePath: string,
  linkHosted: boolean,
): string => {
  if (linkHosted) {
    return `${githubRoot}/${filePath.substring(inputRoot.length - 1)}`;
  } else {
    return `${githubRoot}/blob/master/${filePath.substring(
      inputRoot.length - 1,
    )}`;
  }
};

export const githubFolderUrl = (
  githubRoot: string,
  inputRoot: string,
  folderPath: string,
  linkHosted: boolean,
): string => {
  if (linkHosted) {
    return `${githubRoot}/${folderPath.substring(inputRoot.length - 1)}`;
  } else {
    return `${githubRoot}/tree/master/${folderPath.substring(
      inputRoot.length - 1,
    )}`;
  }
};

import { OpenAIChat } from 'langchain/llms';
import { LLMModelDetails, LLMModels } from '../../types.js';

export const models: Record<LLMModels, LLMModelDetails> = {
  [LLMModels.GPT3]: {
    name: LLMModels.GPT3,
    inputCostPer1KTokens: 0.0015,
    outputCostPer1KTokens: 0.002,
    maxLength: 3050,
    llm: new OpenAIChat({
      temperature: 0.1,
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: LLMModels.GPT3,
    }),
    inputTokens: 0,
    outputTokens: 0,
    succeeded: 0,
    failed: 0,
    total: 0,
  },
  [LLMModels.GPT4]: {
    name: LLMModels.GPT4,
    inputCostPer1KTokens: 0.03,
    outputCostPer1KTokens: 0.06,
    maxLength: 8192,
    llm: new OpenAIChat({
      temperature: 0.1,
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: LLMModels.GPT4,
    }),
    inputTokens: 0,
    outputTokens: 0,
    succeeded: 0,
    failed: 0,
    total: 0,
  },
  [LLMModels.GPT432k]: {
    name: LLMModels.GPT432k,
    inputCostPer1KTokens: 0.06,
    outputCostPer1KTokens: 0.12,
    maxLength: 32768,
    llm: new OpenAIChat({
      temperature: 0.1,
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: LLMModels.GPT4,
    }),
    inputTokens: 0,
    outputTokens: 0,
    succeeded: 0,
    failed: 0,
    total: 0,
  },
};

export const printModelDetails = (models: LLMModelDetails[]): void => {
  const output = models.map((model) => {
    return {
      Model: model.name,
      'File Count': model.total,
      Succeeded: model.succeeded,
      Failed: model.failed,
      Tokens: model.inputTokens + model.outputTokens,
      Cost:
        (model.inputTokens / 1000) * model.inputCostPer1KTokens +
        (model.outputTokens / 1000) * model.outputCostPer1KTokens,
    };
  });

  const totals = output.reduce(
    (cur: any, next) => {
      return {
        ...cur,
        'File Count': cur['File Count'] + next['File Count'],
        Succeeded: cur.Succeeded + next.Succeeded,
        Failed: cur.Failed + next.Failed,
        Tokens: cur.Tokens + next.Tokens,
        Cost: cur.Cost + next.Cost,
      };
    },
    {
      Model: 'Total',
      'File Count': 0,
      Succeeded: 0,
      Failed: 0,
      Tokens: 0,
      Cost: 0,
    },
  );

  const all = [...output, totals];
  console.table(all);
};

export const totalIndexCostEstimate = (models: LLMModelDetails[]): number => {
  const totalCost = models.reduce((cur, model) => {
    return (
      cur +
      (model.inputTokens / 1000) * model.inputCostPer1KTokens +
      (model.outputTokens / 1000) * model.outputCostPer1KTokens
    );
  }, 0);

  return totalCost;
};

export async function wait(timeoutMs: number, value: any = null): Promise<any> {
  return new Promise((resolve) => {
    setTimeout(() => resolve(value), timeoutMs);
  });
}

export async function forTrue(fn: () => boolean) {
  const count = 0;
  return new Promise((resolve, reject) => {
    if (fn()) {
      resolve(true);
      return;
    }

    const interval = setInterval(() => {
      if (fn()) {
        clearInterval(interval);
        resolve(true);
        return;
      }
      if (count >= 200) reject();
    }, 50);
  });
}

import fs from 'node:fs/promises';
import path from 'path';
import minimatch from 'minimatch';
import { isText } from 'istextorbinary';
import { TraverseFileSystemParams } from '../../types.js';

export const traverseFileSystem = async (
  params: TraverseFileSystemParams,
): Promise<void> => {
  try {
    const {
      inputPath,
      projectName,
      processFile,
      processFolder,
      ignore,
      filePrompt,
      folderPrompt,
      contentType,
      targetAudience,
      linkHosted,
    } = params;

    try {
      await fs.access(inputPath);
    } catch (error) {
      console.error('The provided folder path does not exist.');
      return;
    }

    const shouldIgnore = (fileName: string): boolean => {
      return ignore.some((pattern) => minimatch(fileName, pattern));
    };

    const dfs = async (currentPath: string): Promise<void> => {
      const contents = (await fs.readdir(currentPath)).filter(
        (fileName) => !shouldIgnore(fileName),
      );

      await Promise.all(
        contents.map(async (folderName) => {
          const folderPath = path.join(currentPath, folderName);
          const entryStats = await fs.stat(folderPath);

          if (entryStats.isDirectory()) {
            await dfs(folderPath);

            await processFolder?.({
              inputPath,
              folderName,
              folderPath,
              projectName,
              shouldIgnore,
              folderPrompt,
              contentType,
              targetAudience,
              linkHosted,
            });
          }
        }),
      );

      await Promise.all(
        contents.map(async (fileName) => {
          const filePath = path.join(currentPath, fileName);
          const entryStats = await fs.stat(filePath);

          if (!entryStats.isFile()) {
            return;
          }

          const buffer = await fs.readFile(filePath);

          if (isText(fileName, buffer)) {
            await processFile?.({
              fileName,
              filePath,
              projectName,
              filePrompt,
              contentType,
              targetAudience,
              linkHosted,
            });
          }
        }),
      );
    };

    await dfs(inputPath);
  } catch (e: any) {
    console.error(`Error during traversal: ${e.message}`);
    throw e;
  }
};

import path from 'node:path';
import os from 'node:os';

export const userConfigFileName = 'autodoc.user.json';

export const userConfigFilePath = path.resolve(
  os.homedir(),
  './.config/autodoc/',
  userConfigFileName,
);

#!/usr/bin/env node

import fs from 'node:fs/promises';
import { Command } from 'commander';
import { spinnerError, stopSpinner } from './cli/spinner.js';
import { init } from './cli/commands/init/index.js';
import { estimate } from './cli/commands/estimate/index.js';
import { index } from './cli/commands/index/index.js';
import { query } from './cli/commands/query/index.js';
import { AutodocRepoConfig, AutodocUserConfig } from './types.js';
import inquirer from 'inquirer';
import chalk from 'chalk';
import { user } from './cli/commands/user/index.js';
import { userConfigFilePath } from './const.js';

const program = new Command();
program.description('Autodoc CLI Tool');
program.version('0.0.9');

program
  .command('init')
  .description(
    'Initialize repository by creating a `autodoc.config.json` file in the current directory.',
  )
  .action(async () => {
    try {
      const config: AutodocRepoConfig = JSON.parse(
        await fs.readFile('./autodoc.config.json', 'utf8'),
      );
      init(config);
    } catch (e) {
      init();
    }
  });

program
  .command('estimate')
  .description('Estimate the cost of running `index` on your respository.')
  .action(async () => {
    try {
      const config: AutodocRepoConfig = JSON.parse(
        await fs.readFile('./autodoc.config.json', 'utf8'),
      );
      estimate(config);
    } catch (e) {
      console.error(
        'Failed to find `autodoc.config.json` file. Did you run `doc init`?',
      );
      console.error(e);
      process.exit(1);
    }
  });

program
  .command('index')
  .description(
    'Traverse your codebase, write docs via LLM, and create a locally stored index.',
  )
  .action(async () => {
    try {
      const config: AutodocRepoConfig = JSON.parse(
        await fs.readFile('./autodoc.config.json', 'utf8'),
      );

      await estimate(config);

      const questions = [
        {
          type: 'confirm',
          name: 'continue',
          message: 'Do you want to continue with indexing?',
          default: true,
        },
      ];

      const answers = await inquirer.prompt(questions);

      if (answers.continue) {
        console.log(chalk.green('Starting crawl...'));
        index(config);
      } else {
        console.log('Exiting...');
        process.exit(0);
      }
    } catch (e) {
      console.error(
        'Failed to find `autodoc.config.json` file. Did you run `doc init`?',
      );
      console.error(e);
      process.exit(1);
    }
  });

program
  .command('user')
  .description('Set the Autodoc user config')
  .action(async () => {
    try {
      const config: AutodocUserConfig = JSON.parse(
        await fs.readFile(userConfigFilePath, 'utf8'),
      );
      user(config);
    } catch (e) {
      user();
    }
  });

program
  .command('q')
  .description('Query an Autodoc index')
  .action(async () => {
    let repoConfig: AutodocRepoConfig;
    try {
      repoConfig = JSON.parse(
        await fs.readFile('./autodoc.config.json', 'utf8'),
      );
    } catch (e) {
      console.error(
        'Failed to find `autodoc.config.json` file. Did you run `doc init`?',
      );
      console.error(e);
      process.exit(1);
    }

    try {
      const userConfig: AutodocUserConfig = JSON.parse(
        await fs.readFile(userConfigFilePath, 'utf8'),
      );

      query(repoConfig, userConfig);
    } catch (e) {
      try {
        await user();
        const userConfig: AutodocRepoConfig = JSON.parse(
          await fs.readFile(userConfigFilePath, 'utf8'),
        );
        query(repoConfig, userConfig);
      } catch (e) {
        console.error('Failed to config file. Did you run `doc init`?');
        console.error(e);
        process.exit(1);
      }
    }
  });

/**
 * Listen for unhandled promise rejections
 */
process.on('unhandledRejection', function (err: Error) {
  console.error(err.stack);

  spinnerError(); // show an error spinner
  stopSpinner(); // stop the spinner
  program.error('', { exitCode: 1 }); // exit with error code 1
});

program.parse();

import fs from 'node:fs/promises';
import path from 'node:path';
import HierarchicalNSW from 'hnswlib-node';
import type {
  HierarchicalNSW as HierarchicalNSWT,
  SpaceName,
} from 'hnswlib-node';
import { Document, InMemoryDocstore } from 'langchain/docstore';
import { Embeddings } from 'langchain/embeddings';
import { SaveableVectorStore } from 'langchain/vectorstores';

export interface HNSWLibBase {
  space: SpaceName;
  numDimensions?: number;
}

export interface HNSWLibArgs extends HNSWLibBase {
  docstore?: InMemoryDocstore;
  index?: HierarchicalNSWT;
}

export class HNSWLib extends SaveableVectorStore {
  _index?: HierarchicalNSWT;

  docstore: InMemoryDocstore;

  args: HNSWLibBase;

  constructor(embeddings: Embeddings, args: HNSWLibArgs) {
    super(embeddings, args);
    this._index = args.index;
    this.args = args;
    this.embeddings = embeddings;
    this.docstore = args?.docstore ?? new InMemoryDocstore();
  }

  async addDocuments(documents: Document[]): Promise<void> {
    const texts = documents.map(({ pageContent }) => pageContent);
    return this.addVectors(
      await this.embeddings.embedDocuments(texts),
      documents,
    );
  }

  private static async getHierarchicalNSW(args: HNSWLibBase) {
    const { HierarchicalNSW } = await HNSWLib.imports();
    if (!args.space) {
      throw new Error('hnswlib-node requires a space argument');
    }
    if (args.numDimensions === undefined) {
      throw new Error('hnswlib-node requires a numDimensions argument');
    }
    return new HierarchicalNSW(args.space, args.numDimensions);
  }

  private async initIndex(vectors: number[][]) {
    if (!this._index) {
      if (this.args.numDimensions === undefined) {
        this.args.numDimensions = vectors[0].length;
      }
      this.index = await HNSWLib.getHierarchicalNSW(this.args);
    }
    if (!this.index.getCurrentCount()) {
      this.index.initIndex(vectors.length);
    }
  }

  public get index(): HierarchicalNSWT {
    if (!this._index) {
      throw new Error(
        'Vector store not initialised yet. Try calling `addTexts` first.',
      );
    }
    return this._index;
  }

  private set index(index: HierarchicalNSWT) {
    this._index = index;
  }

  async addVectors(vectors: number[][], documents: Document[]) {
    if (vectors.length === 0) {
      return;
    }
    await this.initIndex(vectors);

    // TODO here we could optionally normalise the vectors to unit length
    // so that dot product is equivalent to cosine similarity, like this
    // https://github.com/nmslib/hnswlib/issues/384#issuecomment-1155737730
    // While we only support OpenAI embeddings this isn't necessary
    if (vectors.length !== documents.length) {
      throw new Error(`Vectors and metadatas must have the same length`);
    }
    if (vectors[0].length !== this.args.numDimensions) {
      throw new Error(
        `Vectors must have the same length as the number of dimensions (${this.args.numDimensions})`,
      );
    }
    const capacity = this.index.getMaxElements();
    const needed = this.index.getCurrentCount() + vectors.length;
    if (needed > capacity) {
      this.index.resizeIndex(needed);
    }
    const docstoreSize = this.docstore.count;
    for (let i = 0; i < vectors.length; i += 1) {
      this.index.addPoint(vectors[i], docstoreSize + i);
      this.docstore.add({ [docstoreSize + i]: documents[i] });
    }
  }

  async similaritySearchVectorWithScore(query: number[], k: number) {
    if (query.length !== this.args.numDimensions) {
      throw new Error(
        `Query vector must have the same length as the number of dimensions (${this.args.numDimensions})`,
      );
    }
    if (k > this.index.getCurrentCount()) {
      const total = this.index.getCurrentCount();
      console.warn(
        `k (${k}) is greater than the number of elements in the index (${total}), setting k to ${total}`,
      );
      // eslint-disable-next-line no-param-reassign
      k = total;
    }
    const result = this.index.searchKnn(query, k);
    return result.neighbors.map(
      (docIndex, resultIndex) =>
        [
          this.docstore.search(String(docIndex)),
          result.distances[resultIndex],
        ] as [Document, number],
    );
  }

  async save(directory: string) {
    await fs.mkdir(directory, { recursive: true });
    await Promise.all([
      this.index.writeIndex(path.join(directory, 'hnswlib.index')),
      await fs.writeFile(
        path.join(directory, 'args.json'),
        JSON.stringify(this.args),
      ),
      await fs.writeFile(
        path.join(directory, 'docstore.json'),
        JSON.stringify(Array.from(this.docstore._docs.entries())),
      ),
    ]);
  }

  static async load(directory: string, embeddings: Embeddings) {
    const args = JSON.parse(
      await fs.readFile(path.join(directory, 'args.json'), 'utf8'),
    );
    const index = await HNSWLib.getHierarchicalNSW(args);
    const [docstoreFiles] = await Promise.all([
      fs
        .readFile(path.join(directory, 'docstore.json'), 'utf8')
        .then(JSON.parse),
      index.readIndex(path.join(directory, 'hnswlib.index')),
    ]);
    args.docstore = new InMemoryDocstore(new Map(docstoreFiles));

    args.index = index;

    return new HNSWLib(embeddings, args);
  }

  static async fromTexts(
    texts: string[],
    metadatas: object[],
    embeddings: Embeddings,
    dbConfig?: {
      docstore?: InMemoryDocstore;
    },
  ): Promise<HNSWLib> {
    const docs: Document[] = [];
    for (let i = 0; i < texts.length; i += 1) {
      const newDoc = new Document({
        pageContent: texts[i],
        metadata: metadatas[i],
      });
      docs.push(newDoc);
    }
    return HNSWLib.fromDocuments(docs, embeddings, dbConfig);
  }

  static async fromDocuments(
    docs: Document[],
    embeddings: Embeddings,
    dbConfig?: {
      docstore?: InMemoryDocstore;
    },
  ): Promise<HNSWLib> {
    const args: HNSWLibArgs = {
      docstore: dbConfig?.docstore,
      space: 'cosine',
    };
    const instance = new this(embeddings, args);
    await instance.addDocuments(docs);
    return instance;
  }

  static async imports(): Promise<{
    HierarchicalNSW: typeof HierarchicalNSWT;
  }> {
    return HierarchicalNSW;
  }
}

import { OpenAIChat } from 'langchain/llms';

export type AutodocUserConfig = {
  llms: LLMModels[];
};

export type AutodocRepoConfig = {
  name: string;
  repositoryUrl: string;
  root: string;
  output: string;
  llms: LLMModels[];
  priority: Priority;
  maxConcurrentCalls: number;
  addQuestions: boolean;
  ignore: string[];
  filePrompt: string;
  folderPrompt: string;
  chatPrompt: string;
  contentType: string;
  targetAudience: string;
  linkHosted: boolean;
};

export type FileSummary = {
  fileName: string;
  filePath: string;
  url: string;
  summary: string;
  questions?: string;
  checksum: string;
};

export type ProcessFileParams = {
  fileName: string;
  filePath: string;
  projectName: string;
  contentType: string;
  filePrompt: string;
  targetAudience: string;
  linkHosted: boolean;
};

export type ProcessFile = (params: ProcessFileParams) => Promise<void>;

export type FolderSummary = {
  folderName: string;
  folderPath: string;
  url: string;
  files: FileSummary[];
  folders: FolderSummary[];
  summary: string;
  questions: string;
  checksum: string;
};

export type ProcessFolderParams = {
  inputPath: string;
  folderName: string;
  folderPath: string;
  projectName: string;
  contentType: string;
  folderPrompt: string;
  targetAudience: string;
  linkHosted: boolean;
  shouldIgnore: (fileName: string) => boolean;
};

export type ProcessFolder = (params: ProcessFolderParams) => Promise<void>;

export type TraverseFileSystemParams = {
  inputPath: string;
  projectName: string;
  processFile?: ProcessFile;
  processFolder?: ProcessFolder;
  ignore: string[];
  filePrompt: string;
  folderPrompt: string;
  contentType: string;
  targetAudience: string;
  linkHosted: boolean;
};

export enum LLMModels {
  GPT3 = 'gpt-3.5-turbo',
  GPT4 = 'gpt-4',
  GPT432k = 'gpt-4-32k',
}

export type LLMModelDetails = {
  name: LLMModels;
  inputCostPer1KTokens: number;
  outputCostPer1KTokens: number;
  maxLength: number;
  llm: OpenAIChat;
  inputTokens: number;
  outputTokens: number;
  succeeded: number;
  failed: number;
  total: number;
};

export enum Priority {
  COST = 'cost',
  PERFORMANCE = 'performance',
}
"
