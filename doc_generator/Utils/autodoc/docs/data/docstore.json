[["0",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\commands\\estimate\\index.ts)\n\nThe `estimate` function in this code is responsible for providing an estimated cost of processing a given repository using the Autodoc project. It takes an `AutodocRepoConfig` object as input, which contains various configuration options such as the repository name, URL, root directory, output directory, and other settings related to the processing of the repository.\n\nThe function starts by constructing the path to the JSON output directory, which will be used to store the intermediate results of the processing. It then updates the spinner text to indicate that the cost estimation is in progress.\n\nNext, the `processRepository` function is called with the provided configuration options and a `true` flag to indicate that this is a dry run. This means that the repository will not actually be processed, but the function will return the details of what would happen if it were processed. This is used to calculate the estimated cost of processing the repository.\n\nOnce the dry run is complete, the spinner is updated to show success, and the results are printed using the `printModelDetails` function. The total estimated cost is then calculated using the `totalIndexCostEstimate` function, which takes the values of the `runDetails` object as input.\n\nFinally, the estimated cost is displayed in the console using the `chalk.redBright` function to format the text in a red color. The message also includes a disclaimer that the actual cost may vary and recommends setting a limit in the user's OpenAI account to prevent unexpected charges.\n\nHere's an example of how the `estimate` function might be used in the larger project:\n\n```javascript\nimport { estimate } from './path/to/this/file';\n\nconst config = {\n  name: 'my-repo',\n  repositoryUrl: 'https://github.com/user/my-repo.git',\n  root: './',\n  output: './output',\n  llms: ['en'],\n  ignore: ['.git', 'node_modules'],\n  filePrompt: true,\n  folderPrompt: true,\n  chatPrompt: true,\n  contentType: 'code',\n  targetAudience: 'developers',\n  linkHosted: true,\n};\n\nestimate(config);\n```\n\nThis example would estimate the cost of processing the \"my-repo\" repository with the specified configuration options.\n## Questions: \n 1. **What is the purpose of the `estimate` function?**\n\n   The `estimate` function is used to perform a dry run of the `processRepository` command to get an estimated price for indexing the given repository. It then prints the model details and the total estimated cost.\n\n2. **What are the parameters passed to the `processRepository` function?**\n\n   The `processRepository` function is called with an object containing the following properties: `name`, `repositoryUrl`, `root`, `output`, `llms`, `ignore`, `filePrompt`, `folderPrompt`, `chatPrompt`, `contentType`, `targetAudience`, and `linkHosted`. Additionally, a second argument `true` is passed to indicate that it's a dry run.\n\n3. **How is the total estimated cost calculated and displayed?**\n\n   The total estimated cost is calculated using the `totalIndexCostEstimate` function, which takes an array of values from the `runDetails` object. The cost is then displayed using `console.log` with `chalk.redBright` for formatting, showing the cost with two decimal places and a note that the actual cost may vary.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\estimate\\index.md"}}],["1",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\commands\\estimate)\n\nThe `estimate` function in `index.ts` is a crucial part of the Autodoc project, as it provides an estimated cost of processing a given repository. It takes an `AutodocRepoConfig` object as input, containing various configuration options such as repository name, URL, root directory, output directory, and other settings related to the processing of the repository.\n\nThe function begins by constructing the path to the JSON output directory, which stores intermediate results of the processing. It then updates the spinner text to indicate that cost estimation is in progress. The `processRepository` function is called with the provided configuration options and a `true` flag, signifying a dry run. This dry run returns the details of what would happen if the repository were processed, which is used to calculate the estimated cost.\n\nUpon completion of the dry run, the spinner is updated to show success, and the results are printed using the `printModelDetails` function. The total estimated cost is calculated using the `totalIndexCostEstimate` function, which takes the values of the `runDetails` object as input.\n\nFinally, the estimated cost is displayed in the console using the `chalk.redBright` function to format the text in red. The message also includes a disclaimer that the actual cost may vary and recommends setting a limit in the user's OpenAI account to prevent unexpected charges.\n\nHere's an example of how the `estimate` function might be used in the larger project:\n\n```javascript\nimport { estimate } from './path/to/this/file';\n\nconst config = {\n  name: 'my-repo',\n  repositoryUrl: 'https://github.com/user/my-repo.git',\n  root: './',\n  output: './output',\n  llms: ['en'],\n  ignore: ['.git', 'node_modules'],\n  filePrompt: true,\n  folderPrompt: true,\n  chatPrompt: true,\n  contentType: 'code',\n  targetAudience: 'developers',\n  linkHosted: true,\n};\n\nestimate(config);\n```\n\nThis example would estimate the cost of processing the \"my-repo\" repository with the specified configuration options.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\estimate\\summary.md"}}],["2",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\commands\\index\\convertJsonToMarkdown.ts)\n\nThe `convertJsonToMarkdown` function in this code is responsible for converting JSON files containing documentation information into Markdown files. This function is part of the larger Autodoc project, which aims to automate the process of generating documentation for code repositories.\n\nThe function takes an `AutodocRepoConfig` object as input, which contains various configuration options such as the project name, input and output directories, and other settings related to the documentation generation process.\n\nThe code first counts the number of files in the project by traversing the file system using the `traverseFileSystem` utility function. This is done to provide a progress update to the user via the `updateSpinnerText` function.\n\nNext, the `processFile` function is defined, which is responsible for reading the content of each JSON file, parsing it, and converting it into a Markdown format. The function checks if the file has a summary, and if so, it generates the Markdown content with a link to the code on GitHub, the summary, and any questions if present. The output Markdown file is then saved in the specified output directory.\n\nFinally, the `traverseFileSystem` function is called again, this time with the `processFile` function as an argument. This allows the code to process each JSON file in the project and convert it into a Markdown file. Once the process is complete, a success message is displayed to the user using the `spinnerSuccess` function.\n\nExample usage:\n\n```javascript\nconvertJsonToMarkdown({\n  name: \"myProject\",\n  root: \"./input\",\n  output: \"./output\",\n  filePrompt: true,\n  folderPrompt: true,\n  contentType: \"code\",\n  targetAudience: \"developers\",\n  linkHosted: \"https://github.com/user/myProject\",\n});\n```\n\nThis will convert all JSON files in the `./input` directory into Markdown files and save them in the `./output` directory.\n## Questions: \n 1. **Question:** What is the purpose of the `convertJsonToMarkdown` function and what are the expected inputs?\n   **Answer:** The `convertJsonToMarkdown` function is used to convert JSON files to Markdown files for each code file in the project. It takes an `AutodocRepoConfig` object as input, which contains various properties like projectName, root, output, filePrompt, folderPrompt, contentType, targetAudience, and linkHosted.\n\n2. **Question:** How does the `traverseFileSystem` function work and what is its role in this code?\n   **Answer:** The `traverseFileSystem` function is a utility function that recursively traverses the file system, starting from the inputPath, and processes each file using the provided `processFile` function. In this code, it is used twice: first to count the number of files in the project, and then to create Markdown files for each code file in the project.\n\n3. **Question:** How are the output directories and Markdown files created, and what is the structure of the generated Markdown content?\n   **Answer:** The output directories are created using the `fs.mkdir` function with the `recursive: true` option. The Markdown files are created using the `fs.writeFile` function. The structure of the generated Markdown content includes a link to view the code on GitHub, the summary, and optionally, a list of questions if they exist.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\index\\convertJsonToMarkdown.md"}}],["3",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\commands\\index\\createVectorStore.ts)\n\nThe code in this file is responsible for processing a directory of text files, splitting the text into chunks, and creating a vector store using the HNSWLib library and OpenAIEmbeddings. This vector store can be used for efficient similarity search and retrieval of documents in the larger project.\n\nThe `processFile` function reads a file's content and creates a `Document` object with the content and metadata (source file path). It returns a Promise that resolves to the created Document.\n\nThe `processDirectory` function is a recursive function that processes a directory and its subdirectories. It reads the files in the directory, and for each file, it checks if it's a directory or a regular file. If it's a directory, the function calls itself with the new directory path. If it's a file, it calls the `processFile` function to create a Document object. The function returns an array of Document objects.\n\nThe `RepoLoader` class extends the `BaseDocumentLoader` class and has a constructor that takes a file path as an argument. It has a `load` method that calls the `processDirectory` function with the given file path and returns the array of Document objects.\n\nThe `createVectorStore` function is an async function that takes an `AutodocRepoConfig` object as an argument, which contains the root directory and output file path. It creates a `RepoLoader` instance with the root directory and loads the documents using the `load` method. It then creates a `RecursiveCharacterTextSplitter` instance with a specified chunk size and chunk overlap and splits the documents into chunks. Finally, it creates a vector store using the HNSWLib library and OpenAIEmbeddings with the processed documents and saves the vector store to the output file path.\n\nExample usage:\n\n```javascript\nconst config = {\n  root: './data/documents',\n  output: './data/vector_store',\n};\n\ncreateVectorStore(config).then(() => {\n  console.log('Vector store created successfully');\n});\n```\n## Questions: \n 1. **Question:** What is the purpose of the `processFile` function and what does it return?\n   **Answer:** The `processFile` function is an asynchronous function that reads the content of a file given its file path, creates a `Document` object with the file contents and metadata (source file path), and returns a Promise that resolves to the created `Document` object.\n\n2. **Question:** How does the `processDirectory` function work and what does it return?\n   **Answer:** The `processDirectory` function is an asynchronous function that takes a directory path as input, reads all the files and subdirectories within it, and processes them recursively. It returns a Promise that resolves to an array of `Document` objects created from the files in the directory and its subdirectories.\n\n3. **Question:** What is the purpose of the `createVectorStore` function and how does it work?\n   **Answer:** The `createVectorStore` function is an asynchronous function that takes an `AutodocRepoConfig` object as input, which contains the root directory path and output file path. The function loads all the documents from the root directory using the `RepoLoader`, splits the text into chunks using the `RecursiveCharacterTextSplitter`, creates a vector store from the documents using the `HNSWLib` and `OpenAIEmbeddings`, and saves the vector store to the specified output file.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\index\\createVectorStore.md"}}],["4",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\commands\\index\\index.ts)\n\nThe code in this file is responsible for processing a given repository and generating documentation in JSON, Markdown, and vector formats. It exports a single function `index` that takes an `AutodocRepoConfig` object as its argument, which contains various configuration options for processing the repository.\n\nThe `index` function performs three main tasks:\n\n1. **Process the repository**: It traverses the repository, calls the LLMS (Language Learning Management System) for each file, and creates JSON files with the results. This is done using the `processRepository` function, which takes the same configuration options as the `index` function. The JSON files are stored in the `output/docs/json/` directory.\n\n   ```javascript\n   updateSpinnerText('Processing repository...');\n   await processRepository({ /* configuration options */ });\n   spinnerSuccess();\n   ```\n\n2. **Create Markdown files**: It converts the generated JSON files into Markdown files using the `convertJsonToMarkdown` function. This function also takes the same configuration options as the `index` function. The Markdown files are stored in the `output/docs/markdown/` directory.\n\n   ```javascript\n   updateSpinnerText('Creating markdown files...');\n   await convertJsonToMarkdown({ /* configuration options */ });\n   spinnerSuccess();\n   ```\n\n3. **Create vector files**: It creates vector files from the generated Markdown files using the `createVectorStore` function. This function also takes the same configuration options as the `index` function. The vector files are stored in the `output/docs/data/` directory.\n\n   ```javascript\n   updateSpinnerText('Create vector files...');\n   await createVectorStore({ /* configuration options */ });\n   spinnerSuccess();\n   ```\n\nThroughout the execution of these tasks, the code uses `updateSpinnerText` and `spinnerSuccess` functions to provide visual feedback on the progress of the tasks.\n\nIn the larger project, this code would be used to automatically generate documentation for a given repository based on the provided configuration options. The generated documentation can then be used for various purposes, such as displaying it on a website or analyzing the content for specific insights.\n## Questions: \n 1. **What does the `index` function do in this code?**\n\n   The `index` function is the main entry point for the autodoc project. It takes an `AutodocRepoConfig` object as input and performs three main tasks: processing the repository and creating JSON files, converting JSON files to markdown files, and creating vector files.\n\n2. **What is the purpose of the `processRepository`, `convertJsonToMarkdown`, and `createVectorStore` functions?**\n\n   The `processRepository` function traverses the repository, calls LLMS for each file, and creates JSON files with the results. The `convertJsonToMarkdown` function creates markdown files from the generated JSON files. The `createVectorStore` function creates vector files from the markdown files.\n\n3. **What are the different types of prompts (`filePrompt`, `folderPrompt`, `chatPrompt`) used for in this code?**\n\n   These prompts are likely used to interact with the user during the processing of the repository. The `filePrompt` might be used to ask the user for input regarding specific files, the `folderPrompt` for input regarding folders, and the `chatPrompt` for general input or feedback during the processing.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\index\\index.md"}}],["5",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\commands\\index\\processRepository.ts)\n\nThe `processRepository` function in this code is responsible for generating summaries and questions for code files and folders in a given repository. It takes an `AutodocRepoConfig` object as input, which contains information about the project, repository URL, input and output paths, language models, and other configurations. An optional `dryRun` parameter can be provided to skip actual API calls and file writing.\n\nThe function starts by initializing the encoding and rate limit for API calls. It then defines two main helper functions: `processFile` and `processFolder`. The `processFile` function is responsible for processing individual code files. It reads the file content, calculates a checksum, and checks if reindexing is needed. If reindexing is required, it creates prompts for summaries and questions, selects the appropriate language model based on the input length, and calls the language model API to generate the summaries and questions. The results are then saved to a JSON file in the output directory.\n\nThe `processFolder` function is responsible for processing folders. It reads the folder content, calculates a checksum, and checks if reindexing is needed. If reindexing is required, it reads the summaries and questions of all files and subfolders in the folder, calls the language model API to generate a summary for the folder, and saves the result to a `summary.json` file in the folder.\n\nThe main function then counts the number of files and folders in the project and processes them using the `traverseFileSystem` utility function. It processes all files first, followed by all folders. Finally, it returns the language model usage statistics.\n\nThe `calculateChecksum` function calculates the checksum of a list of file contents, while the `reindexCheck` function checks if reindexing is needed by comparing the new and old checksums of a file or folder.\n## Questions: \n 1. **Question:** What is the purpose of the `processRepository` function and what are its inputs and outputs?\n   **Answer:** The `processRepository` function processes a given code repository, generating summaries and questions for each file and folder within the repository. It takes an `AutodocRepoConfig` object and an optional `dryRun` boolean as inputs. The function returns a `Promise` that resolves to an object containing the models used during processing.\n\n2. **Question:** How does the `calculateChecksum` function work and what is its purpose?\n   **Answer:** The `calculateChecksum` function takes an array of file contents as input and calculates a checksum for each file using the MD5 hashing algorithm. It then concatenates all the checksums and calculates a final checksum using MD5 again. The purpose of this function is to generate a unique identifier for the contents of the files, which can be used to determine if the files have changed and need to be reprocessed.\n\n3. **Question:** How does the `reindexCheck` function work and when is it used?\n   **Answer:** The `reindexCheck` function checks if a summary.json file exists in the given file or folder path and compares the stored checksum with the new checksum to determine if the file or folder needs to be reindexed. It is used in the `processFile` and `processFolder` functions to decide whether to regenerate summaries and questions for a file or folder based on changes in their contents.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\index\\processRepository.md"}}],["6",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\commands\\index\\prompts.ts)\n\nThis code defines three utility functions that generate prompts for documentation experts working on a project. These functions are used to create documentation for code files and folders within a project. The generated prompts are in markdown format and include specific instructions for the documentation expert.\n\n1. `createCodeFileSummary`: This function generates a prompt for creating a summary of a code file. It takes five parameters: `filePath`, `projectName`, `fileContents`, `contentType`, and `filePrompt`. The function returns a markdown formatted string that includes the file's content and a custom prompt for the documentation expert.\n\nExample usage:\n```javascript\nconst prompt = createCodeFileSummary('path/to/file.js', 'MyProject', 'const x = 10;', 'JavaScript', 'Write a detailed technical explanation of this code.');\n```\n\n2. `createCodeQuestions`: This function generates a prompt for creating a list of questions and answers about a code file. It takes five parameters: `filePath`, `projectName`, `fileContents`, `contentType`, and `targetAudience`. The function returns a markdown formatted string that includes the file's content and a custom prompt for the documentation expert to provide questions and answers.\n\nExample usage:\n```javascript\nconst prompt = createCodeQuestions('path/to/file.js', 'MyProject', 'const x = 10;', 'JavaScript', 'beginner');\n```\n\n3. `folderSummaryPrompt`: This function generates a prompt for creating a summary of a folder containing code files and subfolders. It takes six parameters: `folderPath`, `projectName`, `files`, `folders`, `contentType`, and `folderPrompt`. The `files` parameter is an array of `FileSummary` objects, and the `folders` parameter is an array of `FolderSummary` objects. The function returns a markdown formatted string that includes a list of files and folders with their summaries and a custom prompt for the documentation expert.\n\nExample usage:\n```javascript\nconst prompt = folderSummaryPrompt('path/to/folder', 'MyProject', fileSummaries, folderSummaries, 'JavaScript', 'Write a detailed technical explanation of this folder structure.');\n```\n\nThese functions can be used in the larger project to generate documentation tasks for experts, ensuring consistent formatting and instructions across different parts of the project.\n## Questions: \n 1. **What is the purpose of the `createCodeFileSummary` function?**\n\n   The `createCodeFileSummary` function generates a string template for a code file summary prompt, which includes the file path, project name, file contents, content type, and a file prompt.\n\n2. **How does the `createCodeQuestions` function differ from the `createCodeFileSummary` function?**\n\n   The `createCodeQuestions` function generates a string template for a code documentation prompt that asks for 3 questions and their answers, while the `createCodeFileSummary` function generates a string template for a code file summary prompt.\n\n3. **What is the role of the `folderSummaryPrompt` function?**\n\n   The `folderSummaryPrompt` function generates a string template for a folder summary prompt, which includes the folder path, project name, lists of files and folders with their summaries, content type, and a folder prompt.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\index\\prompts.md"}}],["7",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\commands\\index)\n\nThe code in this folder is responsible for processing a given repository and generating documentation in JSON, Markdown, and vector formats. It consists of several functions and utilities that work together to automate the documentation generation process.\n\nThe main function, `index`, takes an `AutodocRepoConfig` object as input, which contains various configuration options for processing the repository. It performs three main tasks:\n\n1. **Process the repository**: It calls the `processRepository` function to traverse the repository, generate summaries and questions for code files and folders using the LLMS (Language Learning Management System), and create JSON files with the results. These JSON files are stored in the `output/docs/json/` directory.\n\n2. **Create Markdown files**: It uses the `convertJsonToMarkdown` function to convert the generated JSON files into Markdown files. These Markdown files are stored in the `output/docs/markdown/` directory.\n\n3. **Create vector files**: It calls the `createVectorStore` function to create vector files from the generated Markdown files. These vector files are stored in the `output/docs/data/` directory.\n\nThroughout the execution of these tasks, the code provides visual feedback on the progress of the tasks using `updateSpinnerText` and `spinnerSuccess` functions.\n\nHere's an example of how this code might be used:\n\n```javascript\nindex({\n  name: \"myProject\",\n  root: \"./input\",\n  output: \"./output\",\n  filePrompt: true,\n  folderPrompt: true,\n  contentType: \"code\",\n  targetAudience: \"developers\",\n  linkHosted: \"https://github.com/user/myProject\",\n});\n```\n\nThis will process the repository located at `./input`, generate documentation in JSON, Markdown, and vector formats, and save the results in the `./output` directory.\n\nThe `prompts.ts` file contains utility functions that generate prompts for documentation experts. These functions create markdown formatted strings with specific instructions for the documentation expert, ensuring consistent formatting and instructions across different parts of the project.\n\nIn summary, the code in this folder automates the process of generating documentation for a given repository based on the provided configuration options. The generated documentation can be used for various purposes, such as displaying it on a website or analyzing the content for specific insights.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\index\\summary.md"}}],["8",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\commands\\init\\index.ts)\n\nThis code is responsible for initializing the configuration of the Autodoc project. It provides a template for the configuration and prompts the user to input necessary information to set up the project. The main functionality is exposed through the `init` function, which is an asynchronous function that takes an optional `AutodocRepoConfig` object as an argument.\n\nThe `makeConfigTemplate` function creates a default configuration object with pre-defined values for various properties. It takes an optional `config` parameter and returns a new `AutodocRepoConfig` object with the provided values or default values if not provided.\n\nThe `init` function first checks if an `autodoc.config.json` file already exists in the project root. If it does, the user is prompted to confirm whether they want to overwrite the existing configuration. If the user chooses not to overwrite, the process exits.\n\nNext, the user is prompted to enter the name of their repository, the GitHub URL of their repository, and the LLMs they have access to. The LLMs are language models used for generating documentation. The user can choose between GPT-3.5 Turbo, GPT-4 8K (Early Access), and GPT-4 32K (Early Access).\n\nAfter the user provides the necessary information, a new configuration object is created using the `makeConfigTemplate` function with the user's input. The new configuration is then written to the `autodoc.config.json` file in the project root.\n\nFinally, a success message is displayed, instructing the user to run `doc index` to get started with the Autodoc project.\n\nExample usage:\n\n```javascript\nimport { init } from './path/to/this/file';\n\n// Initialize the configuration with default values\nawait init();\n\n// Initialize the configuration with custom values\nawait init({\n  name: 'My Custom Repository',\n  repositoryUrl: 'https://github.com/user/repo',\n});\n```\n## Questions: \n 1. **What is the purpose of the `makeConfigTemplate` function?**\n\n   The `makeConfigTemplate` function is used to create a default configuration object for the Autodoc project. It takes an optional `config` parameter of type `AutodocRepoConfig` and returns a new configuration object with default values for various properties.\n\n2. **How does the `init` function work and when is it called?**\n\n   The `init` function is an asynchronous function that initializes the Autodoc configuration by creating an `autodoc.config.json` file in the specified location. It takes an optional `config` parameter of type `AutodocRepoConfig` and prompts the user for input to set the configuration values. It is called when the user wants to set up the Autodoc configuration for their project.\n\n3. **What is the purpose of the `inquirer.prompt` calls in the `init` function?**\n\n   The `inquirer.prompt` calls are used to interactively prompt the user for input to set the configuration values for the Autodoc project. The user is asked for the repository name, repository URL, and the LLMs they have access to. The input is then used to create a new configuration object and write it to the `autodoc.config.json` file.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\init\\index.md"}}],["9",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\commands\\init)\n\nThe `index.ts` file in the `.autodoc\\docs\\json\\src\\cli\\commands\\init` folder is responsible for initializing the configuration of the Autodoc project. It provides a template for the configuration and prompts the user to input necessary information to set up the project. The main functionality is exposed through the `init` function, which is an asynchronous function that takes an optional `AutodocRepoConfig` object as an argument.\n\nThe `makeConfigTemplate` function creates a default configuration object with pre-defined values for various properties. It takes an optional `config` parameter and returns a new `AutodocRepoConfig` object with the provided values or default values if not provided.\n\nThe `init` function first checks if an `autodoc.config.json` file already exists in the project root. If it does, the user is prompted to confirm whether they want to overwrite the existing configuration. If the user chooses not to overwrite, the process exits.\n\nNext, the user is prompted to enter the name of their repository, the GitHub URL of their repository, and the LLMs they have access to. The LLMs are language models used for generating documentation. The user can choose between GPT-3.5 Turbo, GPT-4 8K (Early Access), and GPT-4 32K (Early Access).\n\nAfter the user provides the necessary information, a new configuration object is created using the `makeConfigTemplate` function with the user's input. The new configuration is then written to the `autodoc.config.json` file in the project root.\n\nFinally, a success message is displayed, instructing the user to run `doc index` to get started with the Autodoc project.\n\nExample usage:\n\n```javascript\nimport { init } from './path/to/this/file';\n\n// Initialize the configuration with default values\nawait init();\n\n// Initialize the configuration with custom values\nawait init({\n  name: 'My Custom Repository',\n  repositoryUrl: 'https://github.com/user/repo',\n});\n```\n\nThis code is essential for setting up the Autodoc project, as it creates the necessary configuration file and gathers user input to customize the project. It works in conjunction with other parts of the project, such as the CLI and the documentation generation process, which rely on the configuration file to function correctly.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\init\\summary.md"}}],["10",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\commands\\query\\createChatChain.ts)\n\nThis code defines a function `makeChain` that creates a chatbot for answering questions about a software project called `projectName`. The chatbot is trained on the content of the project, which is located at `repositoryUrl`. The content type of the project is specified by the `contentType` parameter. The chatbot is designed to provide conversational answers with hyperlinks back to GitHub, including code examples and links to the examples where appropriate. The target audience for the chatbot is specified by the `targetAudience` parameter.\n\nThe `makeChain` function takes several parameters:\n\n- `projectName`: The name of the software project.\n- `repositoryUrl`: The URL of the project's repository.\n- `contentType`: The type of content the chatbot is trained on.\n- `chatPrompt`: Additional instructions for answering questions about the content type.\n- `targetAudience`: The intended audience for the chatbot's answers.\n- `vectorstore`: An instance of HNSWLib for efficient nearest neighbor search.\n- `llms`: An array of LLMModels, which are language models used for generating answers.\n- `onTokenStream`: An optional callback function that is called when a new token is generated by the language model.\n\nThe `makeChain` function first creates a question generator using the `LLMChain` class. This generator is responsible for rephrasing follow-up questions to be standalone questions. It uses the `CONDENSE_PROMPT` template, which is defined at the beginning of the code.\n\nNext, the function creates a `QA_PROMPT` template using the `makeQAPrompt` function. This template is used to generate answers to the questions in a conversational manner, with hyperlinks back to GitHub and code examples where appropriate.\n\nFinally, the function creates and returns a new instance of the `ChatVectorDBQAChain` class, which combines the question generator and the document chain to create a chatbot that can answer questions about the software project. The chatbot uses the `vectorstore` for efficient nearest neighbor search and the `llms` language models for generating answers. If the `onTokenStream` callback is provided, it will be called when a new token is generated by the language model.\n## Questions: \n 1. **Question:** What is the purpose of the `makeChain` function and what are its input parameters?\n\n   **Answer:** The `makeChain` function is used to create a `ChatVectorDBQAChain` instance, which is responsible for generating questions and answers based on the given input parameters. The input parameters include `projectName`, `repositoryUrl`, `contentType`, `chatPrompt`, `targetAudience`, `vectorstore`, `llms`, and an optional `onTokenStream` function.\n\n2. **Question:** What are the roles of `CONDENSE_PROMPT` and `QA_PROMPT` in this code?\n\n   **Answer:** `CONDENSE_PROMPT` is a template for generating standalone questions from a given chat history and follow-up question. `QA_PROMPT` is a template for generating conversational answers with hyperlinks to GitHub, based on the provided context and question. Both templates are used in the `LLMChain` and `loadQAChain` instances, respectively.\n\n3. **Question:** How does the `onTokenStream` function work and when is it used?\n\n   **Answer:** The `onTokenStream` function is an optional callback that can be provided to the `makeChain` function. It is used to handle the streaming of tokens generated by the OpenAIChat instance. If provided, it will be called with each new token generated during the chat process.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\query\\createChatChain.md"}}],["11",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\commands\\query\\index.ts)\n\nThis code defines a chatbot interface for the Autodoc project, which allows users to ask questions related to a specific codebase and receive answers in a conversational manner. The chatbot uses a combination of the `inquirer` library for user input, `marked` and `marked-terminal` for rendering Markdown output, and the `langchain` library for handling natural language processing tasks.\n\nThe `query` function is the main entry point for the chatbot. It takes two arguments: an `AutodocRepoConfig` object containing information about the code repository, and an `AutodocUserConfig` object containing user-specific settings. The function initializes a vector store using the `HNSWLib` and `OpenAIEmbeddings` classes, and creates a chat chain using the `makeChain` function.\n\nThe chatbot interface is displayed using the `displayWelcomeMessage` function, which prints a welcome message to the console. The `getQuestion` function is used to prompt the user for a question using the `inquirer` library. The chatbot then enters a loop, where it processes the user's question, generates a response using the chat chain, and displays the response as Markdown in the terminal.\n\nIf an error occurs during the processing of a question, the chatbot will display an error message and continue to prompt the user for a new question. The loop continues until the user types 'exit', at which point the chatbot terminates.\n\nHere's an example of how the `query` function might be used:\n\n```javascript\nimport { query } from './autodoc';\n\nconst repoConfig = {\n  name: 'MyProject',\n  repositoryUrl: 'https://github.com/user/myproject',\n  output: 'path/to/output',\n  contentType: 'code',\n  chatPrompt: 'Ask me anything about MyProject',\n  targetAudience: 'developers',\n};\n\nconst userConfig = {\n  llms: 'path/to/llms',\n};\n\nquery(repoConfig, userConfig);\n```\n\nThis example would initialize the chatbot with the specified repository and user configurations, and start the chatbot interface for the user to ask questions about the \"MyProject\" codebase.\n## Questions: \n 1. **What is the purpose of the `query` function in this code?**\n\n   The `query` function is responsible for handling user interactions with the chatbot. It takes in an AutodocRepoConfig object and an AutodocUserConfig object, sets up the necessary data structures, and then enters a loop where it prompts the user for questions, processes them, and displays the results.\n\n2. **How does the code handle rendering Markdown text in the terminal?**\n\n   The code uses the `marked` library along with a custom `TerminalRenderer` to render Markdown text in the terminal. The `marked` library is configured with the custom renderer using `marked.setOptions({ renderer: new TerminalRenderer() });`.\n\n3. **What is the purpose of the `chatHistory` variable and how is it used?**\n\n   The `chatHistory` variable is an array that stores the history of questions and answers in the chat session. It is used to keep track of the conversation between the user and the chatbot. When a new question is asked, the chat history is passed to the `chain.call()` function, and the new question and its corresponding answer are added to the `chatHistory` array.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\query\\index.md"}}],["12",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\commands\\query)\n\nThe `query` folder in the Autodoc project contains code for creating a chatbot that can answer questions about a specific software project in a conversational manner. The chatbot is trained on the content of the project and provides answers with hyperlinks back to GitHub, including code examples and links to the examples where appropriate.\n\nThe main entry point for the chatbot is the `query` function in `index.ts`. It takes two arguments: an `AutodocRepoConfig` object containing information about the code repository, and an `AutodocUserConfig` object containing user-specific settings. The function initializes a vector store and creates a chat chain using the `makeChain` function from `createChatChain.ts`.\n\nHere's an example of how the `query` function might be used:\n\n```javascript\nimport { query } from './autodoc';\n\nconst repoConfig = {\n  name: 'MyProject',\n  repositoryUrl: 'https://github.com/user/myproject',\n  output: 'path/to/output',\n  contentType: 'code',\n  chatPrompt: 'Ask me anything about MyProject',\n  targetAudience: 'developers',\n};\n\nconst userConfig = {\n  llms: 'path/to/llms',\n};\n\nquery(repoConfig, userConfig);\n```\n\nThis example initializes the chatbot with the specified repository and user configurations and starts the chatbot interface for the user to ask questions about the \"MyProject\" codebase.\n\nThe `createChatChain.ts` file defines the `makeChain` function, which creates a chatbot for answering questions about a software project. The chatbot is designed to provide conversational answers with hyperlinks back to GitHub, including code examples and links to the examples where appropriate. The target audience for the chatbot is specified by the `targetAudience` parameter.\n\nThe `makeChain` function takes several parameters, such as `projectName`, `repositoryUrl`, `contentType`, `chatPrompt`, `targetAudience`, `vectorstore`, `llms`, and `onTokenStream`. It first creates a question generator using the `LLMChain` class, then creates a `QA_PROMPT` template using the `makeQAPrompt` function, and finally creates and returns a new instance of the `ChatVectorDBQAChain` class, which combines the question generator and the document chain to create a chatbot that can answer questions about the software project.\n\nIn summary, the code in the `query` folder is responsible for creating a chatbot that can answer questions about a specific software project in a conversational manner. The chatbot uses a combination of natural language processing techniques and efficient nearest neighbor search to generate accurate and relevant answers for the user.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\query\\summary.md"}}],["13",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\commands)\n\nThe code in the `.autodoc\\docs\\json\\src\\cli\\commands` folder is responsible for various tasks related to the Autodoc project, such as initializing the configuration, processing repositories, generating documentation, and creating a chatbot for answering questions about a specific software project. The folder contains several subfolders, each with a specific purpose.\n\n### estimate\n\nThe `estimate` function provides an estimated cost of processing a given repository. It takes an `AutodocRepoConfig` object as input and performs a dry run of the repository processing to calculate the estimated cost. Example usage:\n\n```javascript\nimport { estimate } from './path/to/this/file';\n\nconst config = {\n  name: 'my-repo',\n  repositoryUrl: 'https://github.com/user/my-repo.git',\n  root: './',\n  output: './output',\n  llms: ['en'],\n  ignore: ['.git', 'node_modules'],\n  filePrompt: true,\n  folderPrompt: true,\n  chatPrompt: true,\n  contentType: 'code',\n  targetAudience: 'developers',\n  linkHosted: true,\n};\n\nestimate(config);\n```\n\n### index\n\nThe code in this folder processes a given repository and generates documentation in JSON, Markdown, and vector formats. It takes an `AutodocRepoConfig` object as input and performs three main tasks: processing the repository, creating Markdown files, and creating vector files. Example usage:\n\n```javascript\nindex({\n  name: \"myProject\",\n  root: \"./input\",\n  output: \"./output\",\n  filePrompt: true,\n  folderPrompt: true,\n  contentType: \"code\",\n  targetAudience: \"developers\",\n  linkHosted: \"https://github.com/user/myProject\",\n});\n```\n\n### init\n\nThe `init` function initializes the configuration of the Autodoc project. It prompts the user to input necessary information to set up the project and creates the `autodoc.config.json` file in the project root. Example usage:\n\n```javascript\nimport { init } from './path/to/this/file';\n\n// Initialize the configuration with default values\nawait init();\n\n// Initialize the configuration with custom values\nawait init({\n  name: 'My Custom Repository',\n  repositoryUrl: 'https://github.com/user/repo',\n});\n```\n\n### query\n\nThe `query` folder contains code for creating a chatbot that can answer questions about a specific software project. The main entry point is the `query` function, which takes an `AutodocRepoConfig` object and an `AutodocUserConfig` object as input. Example usage:\n\n```javascript\nimport { query } from './autodoc';\n\nconst repoConfig = {\n  name: 'MyProject',\n  repositoryUrl: 'https://github.com/user/myproject',\n  output: 'path/to/output',\n  contentType: 'code',\n  chatPrompt: 'Ask me anything about MyProject',\n  targetAudience: 'developers',\n};\n\nconst userConfig = {\n  llms: 'path/to/llms',\n};\n\nquery(repoConfig, userConfig);\n```\n\n### user\n\nThe `user` folder manages the user configuration for the Autodoc project. It allows users to create, update, and save their configuration file, which stores information about their access to different Language Learning Models (LLMs). Example usage:\n\n```javascript\nimport { user } from './path/to/this/file';\n\n// Create a new user configuration with default settings\nawait user();\n\n// Update the user configuration with a custom config object\nawait user({ llms: [LLMModels.GPT3, LLMModels.GPT4] });\n```\n\nIn summary, the code in this folder is essential for various tasks related to the Autodoc project, such as initializing the configuration, processing repositories, generating documentation, and creating a chatbot for answering questions about a specific software project.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\summary.md"}}],["14",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\commands\\user\\index.ts)\n\nThis code is responsible for managing the user configuration for the Autodoc project. It provides a way to create, update, and save the user configuration file, which stores information about the user's access to different Language Learning Models (LLMs) such as GPT-3, GPT-4, and GPT-4 32K.\n\nThe `makeConfigTemplate` function is used to create a default configuration object with the provided `config` parameter or with GPT-3 as the default LLM. This function is used to generate a new configuration object when needed.\n\nThe main function, `user`, is an asynchronous function that takes an optional `config` parameter. It first checks if a user configuration file already exists at the `userConfigFilePath`. If it does, the user is prompted to confirm whether they want to overwrite the existing configuration. If the user chooses not to overwrite, the process exits.\n\nIf the user configuration file does not exist, the code attempts to create the necessary directories for the file. If there's an error during this process, it logs the error and exits with a non-zero status code.\n\nNext, the user is prompted to select which LLMs they have access to. The available options are GPT-3.5 Turbo, GPT-3.5 Turbo with GPT-4 8K (Early Access), and GPT-3.5 Turbo with GPT-4 8K and GPT-4 32K (Early Access). The user's selection is then used to create a new configuration object using the `makeConfigTemplate` function.\n\nFinally, the new configuration object is written to the user configuration file in JSON format. A success message is displayed to the user, indicating that the configuration has been saved and they can start querying using the `doc q` command.\n\nExample usage:\n\n```javascript\nimport { user } from './path/to/this/file';\n\n// Create a new user configuration with default settings\nawait user();\n\n// Update the user configuration with a custom config object\nawait user({ llms: [LLMModels.GPT3, LLMModels.GPT4] });\n```\n## Questions: \n 1. **What is the purpose of the `makeConfigTemplate` function?**\n\n   The `makeConfigTemplate` function is used to create a default configuration object for the Autodoc user. It takes an optional `config` parameter and returns an object with a `llms` property, which is an array of LLM models.\n\n2. **How does the `user` function handle existing user configuration files?**\n\n   The `user` function checks if a user configuration file already exists using `fsSync.existsSync`. If it does, the user is prompted with a confirmation message to overwrite the existing configuration. If the user chooses not to overwrite, the process exits with a status code of 0.\n\n3. **What are the available choices for LLM models in the `user` function?**\n\n   The available choices for LLM models are GPT-3.5 Turbo, GPT-3.5 Turbo and GPT-4 8K (Early Access), and GPT-3.5 Turbo, GPT-4 8K (Early Access), and GPT-4 32K (Early Access). The user can select one of these options, and the selected value is stored in the `llms` property of the new configuration object.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\user\\index.md"}}],["15",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\commands\\user)\n\nThe `index.ts` file in the `user` folder is responsible for managing the user configuration for the Autodoc project. It allows users to create, update, and save their configuration file, which stores information about their access to different Language Learning Models (LLMs) such as GPT-3, GPT-4, and GPT-4 32K.\n\nThe `makeConfigTemplate` function creates a default configuration object with either the provided `config` parameter or GPT-3 as the default LLM. This function is useful for generating a new configuration object when needed.\n\nThe main function, `user`, is an asynchronous function that takes an optional `config` parameter. It first checks if a user configuration file already exists at the `userConfigFilePath`. If it does, the user is prompted to confirm whether they want to overwrite the existing configuration. If the user chooses not to overwrite, the process exits.\n\nIf the user configuration file does not exist, the code attempts to create the necessary directories for the file. If there's an error during this process, it logs the error and exits with a non-zero status code.\n\nNext, the user is prompted to select which LLMs they have access to. The available options are GPT-3.5 Turbo, GPT-3.5 Turbo with GPT-4 8K (Early Access), and GPT-3.5 Turbo with GPT-4 8K and GPT-4 32K (Early Access). The user's selection is then used to create a new configuration object using the `makeConfigTemplate` function.\n\nFinally, the new configuration object is written to the user configuration file in JSON format. A success message is displayed to the user, indicating that the configuration has been saved and they can start querying using the `doc q` command.\n\nThis code is essential for the Autodoc project as it allows users to manage their access to different LLMs and store their preferences in a configuration file. This configuration file can then be used by other parts of the project to determine which LLMs the user has access to and tailor the querying process accordingly.\n\nExample usage:\n\n```javascript\nimport { user } from './path/to/this/file';\n\n// Create a new user configuration with default settings\nawait user();\n\n// Update the user configuration with a custom config object\nawait user({ llms: [LLMModels.GPT3, LLMModels.GPT4] });\n```\n\nIn summary, the `index.ts` file in the `user` folder is a crucial part of the Autodoc project, allowing users to manage their LLM access and preferences. This configuration is then used by other parts of the project to provide a tailored experience based on the user's access to different LLMs.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\commands\\user\\summary.md"}}],["16",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\spinner.ts)\n\nThis code is responsible for managing a spinner, which is a visual element that indicates a process is running in the background. The spinner is created using the `ora` library, which provides a simple and customizable way to create spinners for command-line interfaces.\n\nThe code starts by importing the `ora` library and creating a singleton spinner instance with the 'dots' style. This ensures that there will only be one spinner active at any given time.\n\nThere are several functions exported by this module to interact with the spinner:\n\n1. `updateSpinnerText(message: string)`: This function updates the spinner's text with the provided message. If the spinner is already spinning, it simply updates the text; otherwise, it starts the spinner with the new message.\n\n   Example usage:\n   ```javascript\n   updateSpinnerText('Loading data...');\n   ```\n\n2. `stopSpinner()`: This function stops the spinner if it is currently spinning.\n\n   Example usage:\n   ```javascript\n   stopSpinner();\n   ```\n\n3. `spinnerError(message?: string)`: This function stops the spinner and marks it as failed with an optional error message. It only takes effect if the spinner is currently spinning.\n\n   Example usage:\n   ```javascript\n   spinnerError('Failed to load data');\n   ```\n\n4. `spinnerSuccess(message?: string)`: This function stops the spinner and marks it as successful with an optional success message. It only takes effect if the spinner is currently spinning.\n\n   Example usage:\n   ```javascript\n   spinnerSuccess('Data loaded successfully');\n   ```\n\n5. `spinnerInfo(message: string)`: This function displays an informational message without affecting the spinner's state.\n\n   Example usage:\n   ```javascript\n   spinnerInfo('Connecting to server...');\n   ```\n\nIn the larger project, this module can be used to provide visual feedback to users when a background process is running, such as loading data, connecting to a server, or performing a complex calculation. By using the exported functions, developers can easily update the spinner's text, stop it, or change its state to indicate success, failure, or display informational messages.\n## Questions: \n 1. **What is the purpose of the `ora` package in this code?**\n\n   The `ora` package is used to create a spinner in the command line interface, providing a visual indication of a running process. In this code, it is used to create a singleton spinner with the 'dots' style.\n\n2. **How does the `updateSpinnerText` function work?**\n\n   The `updateSpinnerText` function takes a message as an input and updates the spinner's text with the given message. If the spinner is already spinning, it updates the text directly; otherwise, it starts the spinner with the new message.\n\n3. **What are the differences between `spinnerError`, `spinnerSuccess`, and `spinnerInfo` functions?**\n\n   These functions are used to update the spinner's state and message based on the outcome of a process. `spinnerError` is called when there is an error, and it stops the spinner with a failure message. `spinnerSuccess` is called when the process is successful, and it stops the spinner with a success message. `spinnerInfo` is used to display an informational message without stopping the spinner.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\spinner.md"}}],["17",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli)\n\nThe code in the `spinner.ts` file, located in the `.autodoc\\docs\\json\\src\\cli` folder, is responsible for managing a spinner, a visual element that indicates a background process is running. The spinner is created using the `ora` library, which provides a simple and customizable way to create spinners for command-line interfaces.\n\nThe module exports several functions to interact with the spinner:\n\n1. `updateSpinnerText(message: string)`: Updates the spinner's text with the provided message. If the spinner is already spinning, it simply updates the text; otherwise, it starts the spinner with the new message.\n\n   Example usage:\n   ```javascript\n   updateSpinnerText('Loading data...');\n   ```\n\n2. `stopSpinner()`: Stops the spinner if it is currently spinning.\n\n   Example usage:\n   ```javascript\n   stopSpinner();\n   ```\n\n3. `spinnerError(message?: string)`: Stops the spinner and marks it as failed with an optional error message. It only takes effect if the spinner is currently spinning.\n\n   Example usage:\n   ```javascript\n   spinnerError('Failed to load data');\n   ```\n\n4. `spinnerSuccess(message?: string)`: Stops the spinner and marks it as successful with an optional success message. It only takes effect if the spinner is currently spinning.\n\n   Example usage:\n   ```javascript\n   spinnerSuccess('Data loaded successfully');\n   ```\n\n5. `spinnerInfo(message: string)`: Displays an informational message without affecting the spinner's state.\n\n   Example usage:\n   ```javascript\n   spinnerInfo('Connecting to server...');\n   ```\n\nIn the larger project, this module can be used to provide visual feedback to users when a background process is running, such as loading data, connecting to a server, or performing a complex calculation. By using the exported functions, developers can easily update the spinner's text, stop it, or change its state to indicate success, failure, or display informational messages.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\summary.md"}}],["18",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\utils\\APIRateLimit.ts)\n\nThe `APIRateLimit` class in this code snippet is designed to manage and limit the number of concurrent API calls made by the application. This is useful in situations where the API being called has a rate limit or when the application needs to prevent overwhelming the server with too many requests at once.\n\nThe class constructor takes an optional parameter `maxConcurrentCalls`, which defaults to 50, to set the maximum number of concurrent API calls allowed. It maintains a queue of API calls and keeps track of the number of calls in progress.\n\nThe main method of this class is `callApi<T>(apiFunction: () => Promise<T>): Promise<T>`. It takes a function `apiFunction` that returns a promise and wraps it in a new promise. The purpose of this wrapping is to control the execution of the API calls and ensure that they do not exceed the specified rate limit.\n\nWhen `callApi` is called, the provided `apiFunction` is added to the queue and the `dequeueAndExecute` method is triggered if there are available slots for concurrent calls. The `dequeueAndExecute` method checks if there are any API calls in the queue and if the number of in-progress calls is below the maximum limit. If both conditions are met, it dequeues the next API call and executes it.\n\nThe `executeCall` function inside `callApi` is responsible for actually calling the API function, resolving or rejecting the promise based on the result, and updating the number of in-progress calls. Once an API call is completed, the `dequeueAndExecute` method is called again to process any remaining calls in the queue.\n\nHere's an example of how this class can be used in the larger project:\n\n```javascript\nconst apiRateLimiter = new APIRateLimit(10); // Limit to 10 concurrent calls\n\nasync function fetchSomeData(id) {\n  // Call the API using the rate limiter\n  const result = await apiRateLimiter.callApi(() => fetch(`https://api.example.com/data/${id}`));\n  return result;\n}\n```\n\nIn this example, the `APIRateLimit` class is used to limit the number of concurrent calls to the `fetch` function, ensuring that no more than 10 calls are made at once.\n## Questions: \n 1. **What is the purpose of the `APIRateLimit` class?**\n\n   The `APIRateLimit` class is designed to manage and limit the number of concurrent API calls to a specified maximum, preventing the application from overwhelming the API with too many requests at once.\n\n2. **How does the `callApi` method work and what is its return type?**\n\n   The `callApi` method takes an `apiFunction` as an argument, which is a function that returns a Promise. It adds the API call to a queue and executes it when there are available slots for concurrent calls. The method returns a Promise of type `T`, where `T` is the expected return type of the `apiFunction`.\n\n3. **How can the maximum number of concurrent calls be configured?**\n\n   The maximum number of concurrent calls can be configured by passing a value to the `maxConcurrentCalls` parameter in the constructor of the `APIRateLimit` class. If no value is provided, the default value is set to 50.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\utils\\APIRateLimit.md"}}],["19",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\utils\\FileUtil.ts)\n\nThis code provides utility functions for handling file and folder paths in the autodoc project. The main purpose of these functions is to generate file names and GitHub URLs for documentation files.\n\n1. `getFileName(input, delimiter, extension)`: This function takes an input string, an optional delimiter (default is '.'), and an optional extension (default is '.md'). It returns a new string with the given extension. If the delimiter is found in the input string, the function removes the part of the string after the last occurrence of the delimiter and appends the extension. If the delimiter is not found, the function simply appends the extension to the input string. This function can be used to generate file names for documentation files with the desired extension.\n\n   Example usage:\n\n   ```\n   getFileName('example.txt'); // returns 'example.md'\n   getFileName('example', '_', '.html'); // returns 'example.html'\n   ```\n\n2. `githubFileUrl(githubRoot, inputRoot, filePath, linkHosted)`: This function generates a GitHub URL for a file. It takes the GitHub repository root URL, the input root folder path, the file path, and a boolean flag indicating whether the URL should be for the hosted version of the file or the source code. It returns a string with the generated URL.\n\n   Example usage:\n\n   ```\n   githubFileUrl('https://github.com/user/repo', '/input', '/input/example.md', true);\n   // returns 'https://github.com/user/repo/example.md'\n   ```\n\n3. `githubFolderUrl(githubRoot, inputRoot, folderPath, linkHosted)`: This function is similar to `githubFileUrl`, but it generates a GitHub URL for a folder instead of a file. It takes the same arguments as `githubFileUrl` and returns a string with the generated URL.\n\n   Example usage:\n\n   ```\n   githubFolderUrl('https://github.com/user/repo', '/input', '/input/folder', true);\n   // returns 'https://github.com/user/repo/folder'\n   ```\n\nThese utility functions can be used throughout the autodoc project to generate file names and GitHub URLs for documentation files and folders, ensuring consistent naming and URL generation across the project.\n## Questions: \n 1. **What is the purpose of the `getFileName` function?**\n\n   The `getFileName` function takes an input string, an optional delimiter, and an optional extension, and returns a new string with the given extension. If the delimiter is not found in the input string, the extension is simply appended to the input string. If the delimiter is found, the input string is sliced up to the last delimiter index and the extension is appended.\n\n2. **What are the differences between the `githubFileUrl` and `githubFolderUrl` functions?**\n\n   Both functions take the same parameters: `githubRoot`, `inputRoot`, a path (either `filePath` or `folderPath`), and a `linkHosted` boolean. The main difference is in the returned URL: `githubFileUrl` returns a URL pointing to a file in the GitHub repository, while `githubFolderUrl` returns a URL pointing to a folder in the GitHub repository. The URL structure differs slightly, with `/blob/master/` for files and `/tree/master/` for folders.\n\n3. **What is the purpose of the `linkHosted` parameter in the `githubFileUrl` and `githubFolderUrl` functions?**\n\n   The `linkHosted` parameter is a boolean that determines whether the returned URL should point to the hosted version of the file or folder on GitHub Pages (if `true`) or to the file or folder within the GitHub repository itself (if `false`). Depending on the value of `linkHosted`, the functions will return different URL structures.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\utils\\FileUtil.md"}}],["20",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\utils\\LLMUtil.ts)\n\nThis code defines and manages different language models (LLMs) and their associated costs for a project that utilizes OpenAI's GPT models. It imports the `OpenAIChat` class from the `langchain/llms` module and the `LLMModelDetails` and `LLMModels` types from the `../../types.js` file.\n\nThe `models` object contains three LLMs: GPT3, GPT4, and GPT432k. Each model has its own properties, such as `name`, `inputCostPer1KTokens`, `outputCostPer1KTokens`, `maxLength`, and an instance of the `OpenAIChat` class with the respective model name and API key. Additionally, each model has counters for input tokens, output tokens, succeeded, failed, and total files processed.\n\nThe `printModelDetails` function takes an array of `LLMModelDetails` and prints a summary table to the console. It calculates the total cost for each model based on the input and output tokens and their respective costs per 1,000 tokens. It also calculates the total file count, succeeded, failed, tokens, and cost across all models.\n\nThe `totalIndexCostEstimate` function calculates the total cost of indexing all models in the input array. It uses the same cost calculation as in `printModelDetails` but returns the total cost as a number.\n\nThese functions can be used in the larger project to manage and analyze the usage and costs of different LLMs. For example, the `printModelDetails` function can be called to display a summary of the models' usage and costs:\n\n```javascript\nimport { models, printModelDetails } from './path/to/this/file';\n\n// Process files with models...\n// Update models' properties...\n\nprintModelDetails(Object.values(models));\n```\n\nAnd the `totalIndexCostEstimate` function can be used to estimate the total cost of indexing all models:\n\n```javascript\nimport { models, totalIndexCostEstimate } from './path/to/this/file';\n\n// Process files with models...\n// Update models' properties...\n\nconst totalCost = totalIndexCostEstimate(Object.values(models));\nconsole.log(`Total cost: ${totalCost}`);\n```\n## Questions: \n 1. **Question:** What is the purpose of the `models` object and how are the different GPT models being used?\n   **Answer:** The `models` object is a record that maps different GPT models (GPT3, GPT4, and GPT432k) to their respective details, such as cost per tokens, maximum length, and an instance of `OpenAIChat` with the corresponding model configuration.\n\n2. **Question:** How does the `printModelDetails` function work and what information does it display?\n   **Answer:** The `printModelDetails` function takes an array of `LLMModelDetails` as input, processes the information for each model, and then prints a summary table to the console. The table includes the model name, file count, succeeded and failed counts, total tokens, and cost.\n\n3. **Question:** What is the purpose of the `totalIndexCostEstimate` function and how is it calculating the total cost?\n   **Answer:** The `totalIndexCostEstimate` function calculates the total cost of processing the given models by iterating through the input `models` array and summing up the costs based on the input and output tokens and their respective costs per 1K tokens.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\utils\\LLMUtil.md"}}],["21",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\utils)\n\nThe `.autodoc\\docs\\json\\src\\cli\\utils` folder contains utility functions and classes that assist in managing API rate limits, handling file and folder paths, managing language models, traversing file systems, and controlling asynchronous operations. These utilities can be used throughout the autodoc project to ensure consistent behavior and improve code organization.\n\n`APIRateLimit.ts` provides the `APIRateLimit` class, which manages and limits the number of concurrent API calls made by the application. This is useful when working with rate-limited APIs or preventing server overload. Example usage:\n\n```javascript\nconst apiRateLimiter = new APIRateLimit(10); // Limit to 10 concurrent calls\nasync function fetchSomeData(id) {\n  const result = await apiRateLimiter.callApi(() => fetch(`https://api.example.com/data/${id}`));\n  return result;\n}\n```\n\n`FileUtil.ts` offers utility functions for generating file names and GitHub URLs for documentation files. These functions ensure consistent naming and URL generation across the project. Example usage:\n\n```javascript\ngetFileName('example.txt'); // returns 'example.md'\ngithubFileUrl('https://github.com/user/repo', '/input', '/input/example.md', true); // returns 'https://github.com/user/repo/example.md'\n```\n\n`LLMUtil.ts` defines and manages different language models (LLMs) and their associated costs for a project utilizing OpenAI's GPT models. Functions like `printModelDetails` and `totalIndexCostEstimate` can be used to manage and analyze the usage and costs of different LLMs. Example usage:\n\n```javascript\nimport { models, printModelDetails } from './path/to/this/file';\nprintModelDetails(Object.values(models));\nconst totalCost = totalIndexCostEstimate(Object.values(models));\nconsole.log(`Total cost: ${totalCost}`);\n```\n\n`traverseFileSystem.ts` contains the `traverseFileSystem` function, which recursively traverses a given file system, processing files and folders based on provided parameters. This is useful for generating documentation or performing tasks that require processing files and folders in a directory structure. Example usage:\n\n```javascript\nawait traverseFileSystem({\n  inputPath: './src',\n  projectName: 'myProject',\n  processFile: (params) => { /* Process file logic */ },\n  processFolder: (params) => { /* Process folder logic */ },\n  ignore: ['node_modules/**', '.git/**'],\n});\n```\n\n`WaitUtil.ts` provides two utility functions, `wait` and `forTrue`, which help manage asynchronous operations by introducing delays and waiting for specific conditions to be met. These functions can be used to control the flow of asynchronous code execution. Example usage:\n\n```javascript\nasync function delayedEcho() {\n  console.log(\"Start\");\n  await wait(1000, \"Hello\");\n  console.log(\"End\");\n}\n\nasync function waitForCondition() {\n  console.log(\"Waiting for condition...\");\n  await forTrue(() => condition);\n  console.log(\"Condition met!\");\n}\n```\n\nIn summary, the utilities in this folder enhance the autodoc project by providing consistent behavior, improving code organization, and managing various aspects of the project, such as API rate limits, file and folder paths, language models, file system traversal, and asynchronous operations.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\utils\\summary.md"}}],["22",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\utils\\traverseFileSystem.ts)\n\nThe `traverseFileSystem` function in this code is an asynchronous function that recursively traverses a given file system, processing files and folders based on the provided parameters. It is designed to be used in the larger project for generating documentation or performing other tasks that require processing files and folders in a directory structure.\n\nThe function takes an object of type `TraverseFileSystemParams` as its input, which contains various properties to control the traversal and processing behavior. These properties include:\n\n- `inputPath`: The root path to start the traversal from.\n- `projectName`: The name of the project being processed.\n- `processFile`: An optional callback function to process a file.\n- `processFolder`: An optional callback function to process a folder.\n- `ignore`: An array of patterns to ignore during traversal.\n- `filePrompt`, `folderPrompt`: Optional prompts for user interaction.\n- `contentType`, `targetAudience`, `linkHosted`: Additional metadata for processing.\n\nThe function first checks if the provided `inputPath` exists using `fs.access`. If the path does not exist, it logs an error message and returns. It then defines a helper function `shouldIgnore` that checks if a given file or folder should be ignored based on the `ignore` patterns.\n\nThe main logic of the function is implemented in the `dfs` (depth-first search) function, which is called recursively to traverse the file system. It reads the contents of the current directory using `fs.readdir`, filters out ignored items, and processes the remaining items.\n\nFor each item, if it is a directory, the `dfs` function is called recursively, and the `processFolder` callback is invoked if provided. If it is a file and its content is text (checked using `isText`), the `processFile` callback is invoked if provided.\n\nThe traversal is performed using `Promise.all` to process items concurrently, improving performance. If an error occurs during traversal, it is logged and rethrown.\n\nHere's an example of how this function might be used in the larger project:\n\n```javascript\nawait traverseFileSystem({\n  inputPath: './src',\n  projectName: 'myProject',\n  processFile: (params) => {\n    // Process file logic here\n  },\n  processFolder: (params) => {\n    // Process folder logic here\n  },\n  ignore: ['node_modules/**', '.git/**'],\n});\n```\n## Questions: \n 1. **What is the purpose of the `traverseFileSystem` function?**\n\n   The `traverseFileSystem` function is an asynchronous function that traverses a given file system, processes folders and files based on the provided parameters, and ignores files and folders based on the given ignore patterns.\n\n2. **How does the `shouldIgnore` function work?**\n\n   The `shouldIgnore` function takes a file name as input and returns a boolean value indicating whether the file should be ignored or not. It checks if the file name matches any of the ignore patterns provided in the `ignore` parameter using the `minimatch` library.\n\n3. **What is the role of the `dfs` function inside `traverseFileSystem`?**\n\n   The `dfs` function is an asynchronous function that performs a depth-first search on the file system starting from the given `currentPath`. It processes folders and files based on the provided parameters and recursively calls itself for each subdirectory found.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\utils\\traverseFileSystem.md"}}],["23",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\cli\\utils\\WaitUtil.ts)\n\nThe code in this file provides two utility functions, `wait` and `forTrue`, which are designed to help manage asynchronous operations in the larger project. Both functions return a `Promise`, making them suitable for use with `async/await` syntax.\n\n### wait\n\nThe `wait` function takes two arguments: `timeoutMs`, a number representing the desired waiting time in milliseconds, and an optional `value` that defaults to `null`. It returns a `Promise` that resolves with the provided `value` after the specified `timeoutMs` has elapsed. This function can be used to introduce a delay in the execution of asynchronous code.\n\nExample usage:\n\n```javascript\nasync function delayedEcho() {\n  console.log(\"Start\");\n  await wait(1000, \"Hello\");\n  console.log(\"End\");\n}\n\ndelayedEcho(); // Output: Start -> (1 second delay) -> End\n```\n\n### forTrue\n\nThe `forTrue` function takes a single argument, `fn`, which is a function that returns a boolean value. It returns a `Promise` that resolves with `true` when the provided function `fn` returns `true`. The function `fn` is checked every 50 milliseconds, up to a maximum of 200 times (i.e., 10 seconds). If `fn` does not return `true` within this time, the `Promise` is rejected.\n\nThis function can be used to wait for a specific condition to be met before continuing the execution of asynchronous code.\n\nExample usage:\n\n```javascript\nlet condition = false;\n\nsetTimeout(() => {\n  condition = true;\n}, 3000);\n\nasync function waitForCondition() {\n  console.log(\"Waiting for condition...\");\n  await forTrue(() => condition);\n  console.log(\"Condition met!\");\n}\n\nwaitForCondition(); // Output: Waiting for condition... -> (3 second delay) -> Condition met!\n```\n\nIn summary, this file provides two utility functions that help manage asynchronous operations by introducing delays and waiting for specific conditions to be met. These functions can be used in the larger project to control the flow of asynchronous code execution.\n## Questions: \n 1. **What is the purpose of the `wait` function?**\n\n   The `wait` function is an asynchronous utility function that resolves a promise after a specified timeout in milliseconds, optionally returning a value when the promise is resolved.\n\n2. **How does the `forTrue` function work?**\n\n   The `forTrue` function takes a function `fn` as an argument, which should return a boolean value. It checks the result of `fn` every 50 milliseconds and resolves the promise when `fn` returns `true`. If `fn` does not return `true` after 200 attempts, the promise is rejected.\n\n3. **What is the use case for the `forTrue` function?**\n\n   The `forTrue` function can be used to wait for a certain condition to be met before proceeding with the execution of the code. This can be useful in situations where you need to wait for an asynchronous operation to complete or a specific state to be reached before continuing.","metadata":{"source":".autodoc\\docs\\markdown\\src\\cli\\utils\\WaitUtil.md"}}],["24",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\const.ts)\n\nThe code in this file is responsible for managing the user configuration file for the autodoc project. It imports two Node.js built-in modules, `path` and `os`, which are used to handle file paths and operating system-related utility functions, respectively.\n\nThe `userConfigFileName` constant is defined as `'autodoc.user.json'`, which represents the name of the user configuration file. This file is expected to store user-specific settings for the autodoc project in JSON format.\n\nThe `userConfigFilePath` constant is created using the `path.resolve()` function, which combines the provided arguments into an absolute file path. The `os.homedir()` function is used to get the current user's home directory, and `./.config/autodoc/` is appended to it as the folder where the user configuration file should be stored. Finally, the `userConfigFileName` constant is appended to the path, resulting in the complete file path for the user configuration file.\n\nBy exporting both `userConfigFileName` and `userConfigFilePath`, other parts of the autodoc project can easily access and use these constants to read or write user-specific settings. For example, when the autodoc application starts, it can read the user configuration file from the specified path, and apply the settings accordingly.\n\nHere's a code example of how these constants might be used in another part of the autodoc project:\n\n```javascript\nimport { userConfigFilePath } from './path/to/this/file';\n\n// Read user configuration from the file\nconst userConfig = JSON.parse(fs.readFileSync(userConfigFilePath, 'utf-8'));\n\n// Apply user settings\napplyUserSettings(userConfig);\n```\n\nIn summary, this code is responsible for defining the name and file path of the user configuration file for the autodoc project, allowing other parts of the project to easily access and manage user-specific settings.\n## Questions: \n 1. **What is the purpose of the `userConfigFileName` and `userConfigFilePath` constants?**\n\n   The `userConfigFileName` constant defines the name of the user configuration file for the autodoc project, while the `userConfigFilePath` constant defines the absolute path to this file, which is located in the user's home directory under the `.config/autodoc/` folder.\n\n2. **Why are the `node:path` and `node:os` modules being imported?**\n\n   The `node:path` module is imported to provide utilities for working with file and directory paths, such as resolving the absolute path to the user configuration file. The `node:os` module is imported to provide operating system-related utility methods, such as getting the user's home directory.\n\n3. **Is this code compatible with different operating systems?**\n\n   Yes, this code is compatible with different operating systems. The `os.homedir()` method returns the home directory of the current user, which is platform-specific, and the `path.resolve()` method takes care of handling the correct path separators for the current operating system.","metadata":{"source":".autodoc\\docs\\markdown\\src\\const.md"}}],["25",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\index.ts)\n\nThis code is the main entry point for the Autodoc CLI tool, which provides a set of commands to help developers automatically generate documentation for their codebase. The tool uses the `commander` library to define and handle commands, and `inquirer` for interactive prompts.\n\nThe available commands are:\n\n1. `init`: Initializes the repository by creating an `autodoc.config.json` file in the current directory. If the file already exists, it uses the existing configuration.\n   ```bash\n   autodoc init\n   ```\n\n2. `estimate`: Estimates the cost of running the `index` command on the repository. It requires the `autodoc.config.json` file to be present.\n   ```bash\n   autodoc estimate\n   ```\n\n3. `index`: Traverses the codebase, writes documentation using LLM, and creates a locally stored index. Before starting the indexing process, it prompts the user for confirmation. It requires the `autodoc.config.json` file to be present.\n   ```bash\n   autodoc index\n   ```\n\n4. `user`: Sets the Autodoc user configuration. If a user configuration file exists, it uses the existing configuration.\n   ```bash\n   autodoc user\n   ```\n\n5. `q`: Queries an Autodoc index. It requires both the `autodoc.config.json` and user configuration files to be present.\n   ```bash\n   autodoc q\n   ```\n\nThe code also listens for unhandled promise rejections and handles them gracefully by showing an error spinner, stopping the spinner, and exiting with an error code.\n\nIn the larger project, this CLI tool serves as the primary interface for users to interact with Autodoc, allowing them to easily generate and manage documentation for their codebase.\n## Questions: \n 1. **What is the purpose of the Autodoc CLI Tool?**\n\n   The Autodoc CLI Tool is designed to help developers automatically generate documentation for their codebase by traversing the code, writing docs via LLM, and creating a locally stored index.\n\n2. **How does the `estimate` command work and what does it return?**\n\n   The `estimate` command reads the `autodoc.config.json` file and estimates the cost of running the `index` command on the repository. It provides an estimation of the resources required to generate the documentation.\n\n3. **What is the role of the `user` command and how does it interact with the user configuration file?**\n\n   The `user` command is responsible for setting the Autodoc user configuration. It reads the user configuration file (if it exists) and allows the user to update or create a new configuration. This configuration is then used in other commands, such as the `query` command, to interact with the Autodoc index.","metadata":{"source":".autodoc\\docs\\markdown\\src\\index.md"}}],["26",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\langchain\\hnswlib.ts)\n\nThe `HNSWLib` class in this code is a specialized vector store that uses the Hierarchical Navigable Small World (HNSW) algorithm for efficient similarity search. It is built on top of the `hnswlib-node` library and extends the `SaveableVectorStore` class. The main purpose of this class is to store and search for documents based on their embeddings, which are high-dimensional vectors representing the documents' content.\n\nThe constructor of the `HNSWLib` class takes an `Embeddings` object and an `HNSWLibArgs` object as arguments. The `Embeddings` object is used to convert documents into their corresponding vector representations, while the `HNSWLibArgs` object contains configuration options for the HNSW index and an optional `InMemoryDocstore` object for storing the documents.\n\nThe `addDocuments` method takes an array of `Document` objects, converts them into embeddings using the `Embeddings` object, and adds them to the HNSW index. The `similaritySearchVectorWithScore` method takes a query vector and a number `k`, and returns the top `k` most similar documents along with their similarity scores.\n\nThe `save` and `load` methods allow for persisting the HNSW index, document store, and configuration options to disk and loading them back into memory. The `fromTexts` and `fromDocuments` static methods provide convenient ways to create an `HNSWLib` instance from an array of texts or documents, respectively.\n\nHere's an example of how to use the `HNSWLib` class:\n\n```javascript\nconst embeddings = new Embeddings(/* ... */);\nconst args = { space: 'cosine' };\nconst hnswLib = new HNSWLib(embeddings, args);\n\n// Add documents to the index\nawait hnswLib.addDocuments(documents);\n\n// Perform a similarity search\nconst queryVector = /* ... */;\nconst k = 10;\nconst results = await hnswLib.similaritySearchVectorWithScore(queryVector, k);\n```\n\nIn the larger project, the `HNSWLib` class can be used to efficiently store and search for documents based on their content similarity, which can be useful for tasks such as document clustering, recommendation systems, or information retrieval.\n## Questions: \n 1. **Question**: What is the purpose of the `HNSWLib` class and how does it relate to the `SaveableVectorStore` class?\n   **Answer**: The `HNSWLib` class is an implementation of a vector store using the Hierarchical Navigable Small World (HNSW) algorithm from the `hnswlib-node` library. It extends the `SaveableVectorStore` class, which provides a base class for vector stores that can be saved and loaded from disk.\n\n2. **Question**: How does the `addDocuments` method work and what is its purpose?\n   **Answer**: The `addDocuments` method takes an array of `Document` objects, extracts their `pageContent`, and embeds them using the provided `Embeddings` instance. It then adds the resulting vectors and documents to the HNSW index and the `InMemoryDocstore`, respectively.\n\n3. **Question**: How does the `similaritySearchVectorWithScore` method work and what does it return?\n   **Answer**: The `similaritySearchVectorWithScore` method takes a query vector and a number `k` as input, and searches for the `k` most similar vectors in the HNSW index. It returns an array of tuples, where each tuple contains a `Document` object and its corresponding similarity score to the query vector.","metadata":{"source":".autodoc\\docs\\markdown\\src\\langchain\\hnswlib.md"}}],["27",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\langchain)\n\nThe `hnswlib.ts` file in the `.autodoc\\docs\\json\\src\\langchain` folder contains the `HNSWLib` class, which is a specialized vector store utilizing the Hierarchical Navigable Small World (HNSW) algorithm for efficient similarity search. This class is built on top of the `hnswlib-node` library and extends the `SaveableVectorStore` class. Its primary purpose is to store and search for documents based on their embeddings, which are high-dimensional vectors representing the documents' content.\n\nThe `HNSWLib` class constructor takes an `Embeddings` object and an `HNSWLibArgs` object as arguments. The `Embeddings` object is responsible for converting documents into their corresponding vector representations, while the `HNSWLibArgs` object contains configuration options for the HNSW index and an optional `InMemoryDocstore` object for storing the documents.\n\nThe `addDocuments` method accepts an array of `Document` objects, converts them into embeddings using the `Embeddings` object, and adds them to the HNSW index. The `similaritySearchVectorWithScore` method takes a query vector and a number `k`, and returns the top `k` most similar documents along with their similarity scores.\n\nThe `save` and `load` methods enable persisting the HNSW index, document store, and configuration options to disk and loading them back into memory. The `fromTexts` and `fromDocuments` static methods provide convenient ways to create an `HNSWLib` instance from an array of texts or documents, respectively.\n\nIn the larger project, the `HNSWLib` class can be employed to efficiently store and search for documents based on their content similarity, which can be beneficial for tasks such as document clustering, recommendation systems, or information retrieval.\n\nHere's an example of how to use the `HNSWLib` class:\n\n```javascript\nconst embeddings = new Embeddings(/* ... */);\nconst args = { space: 'cosine' };\nconst hnswLib = new HNSWLib(embeddings, args);\n\n// Add documents to the index\nawait hnswLib.addDocuments(documents);\n\n// Perform a similarity search\nconst queryVector = /* ... */;\nconst k = 10;\nconst results = await hnswLib.similaritySearchVectorWithScore(queryVector, k);\n```\n\nThis code snippet demonstrates how to create an `HNSWLib` instance, add documents to the index, and perform a similarity search. The results can then be used for various purposes, such as finding related documents or generating recommendations based on content similarity.","metadata":{"source":".autodoc\\docs\\markdown\\src\\langchain\\summary.md"}}],["28",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src)\n\nThe `.autodoc\\docs\\json\\src` folder contains the core components of the autodoc project, which is designed to automatically generate documentation for a given code repository using OpenAI's language models (LLMs). The folder consists of three main files: `const.ts`, `index.ts`, and `types.ts`, as well as two subfolders: `cli` and `langchain`.\n\n`const.ts` defines the name and file path of the user configuration file for the autodoc project. This file stores user-specific settings in JSON format. Other parts of the project can easily access and use these constants to read or write user-specific settings. For example:\n\n```javascript\nimport { userConfigFilePath } from './path/to/this/file';\n\n// Read user configuration from the file\nconst userConfig = JSON.parse(fs.readFileSync(userConfigFilePath, 'utf-8'));\n\n// Apply user settings\napplyUserSettings(userConfig);\n```\n\n`index.ts` serves as the main entry point for the Autodoc CLI tool, providing a set of commands for developers to generate and manage documentation for their codebase. The available commands include `init`, `estimate`, `index`, `user`, and `q`. The CLI tool uses the `commander` library for command handling and `inquirer` for interactive prompts.\n\n`types.ts` defines the types and interfaces for the autodoc project, such as `AutodocUserConfig`, `AutodocRepoConfig`, `FileSummary`, `FolderSummary`, and more. These types are used to configure and run the autodoc tool, allowing users to generate documentation for their code repositories using OpenAI's LLMs.\n\nThe `cli` subfolder contains the `spinner.ts` file, which manages a spinner for visual feedback during background processes. It exports functions like `updateSpinnerText`, `stopSpinner`, `spinnerError`, `spinnerSuccess`, and `spinnerInfo` for easy interaction with the spinner.\n\nThe `langchain` subfolder contains the `hnswlib.ts` file, which provides the `HNSWLib` class for efficient similarity search using the Hierarchical Navigable Small World (HNSW) algorithm. This class is used to store and search for documents based on their embeddings, which are high-dimensional vectors representing the documents' content. Example usage:\n\n```javascript\nconst embeddings = new Embeddings(/* ... */);\nconst args = { space: 'cosine' };\nconst hnswLib = new HNSWLib(embeddings, args);\n\n// Add documents to the index\nawait hnswLib.addDocuments(documents);\n\n// Perform a similarity search\nconst queryVector = /* ... */;\nconst k = 10;\nconst results = await hnswLib.similaritySearchVectorWithScore(queryVector, k);\n```\n\nIn summary, the code in this folder is responsible for the core functionality of the autodoc project, including user configuration management, CLI tool commands, type definitions, spinner management, and efficient similarity search using the HNSW algorithm.","metadata":{"source":".autodoc\\docs\\markdown\\src\\summary.md"}}],["29",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/src\\types.ts)\n\nThis code defines the types and interfaces for the `autodoc` project, which aims to automatically generate documentation for a given code repository. The project uses OpenAI's language models (LLMs) to process and generate summaries, questions, and other relevant information for files and folders in the repository.\n\nThe `AutodocUserConfig` and `AutodocRepoConfig` types define the configuration options for the user and repository, respectively. These include settings such as the LLM models to use, repository URL, output directory, and content type.\n\n`FileSummary` and `FolderSummary` types represent the generated summaries for files and folders, including their paths, URLs, and checksums. The `ProcessFileParams` and `ProcessFolderParams` types define the parameters required for processing files and folders, such as the file or folder name, path, and content type.\n\n`ProcessFile` and `ProcessFolder` are function types that take the respective parameters and return a promise. These functions are responsible for processing the files and folders, generating summaries, and updating the documentation.\n\n`TraverseFileSystemParams` type defines the parameters for traversing the file system, including the input path, project name, and optional `processFile` and `processFolder` functions. It also includes settings for ignoring certain files or folders and content type preferences.\n\nThe `LLMModels` enum lists the available language models, such as GPT-3.5 Turbo, GPT-4, and GPT-4 32k. The `LLMModelDetails` type provides information about each model, including the cost per 1K tokens, maximum length, and success/failure statistics.\n\nIn the larger project, these types and interfaces would be used to configure and run the `autodoc` tool, allowing users to automatically generate documentation for their code repositories using OpenAI's language models. For example, a user could provide an `AutodocRepoConfig` object to configure the tool, and then use the `TraverseFileSystem` function to process the repository and generate the documentation.\n## Questions: \n 1. **What is the purpose of the `AutodocUserConfig` and `AutodocRepoConfig` types?**\n\n   The `AutodocUserConfig` type is used to define the user configuration for the autodoc project, which includes an array of LLMModels. The `AutodocRepoConfig` type is used to define the repository configuration for the autodoc project, which includes various properties such as name, repository URL, root, output, LLMModels, and more.\n\n2. **What are the different LLMModels available in the `LLMModels` enum?**\n\n   The `LLMModels` enum lists the available language models for the autodoc project. Currently, there are three models: GPT3 (gpt-3.5-turbo), GPT4 (gpt-4), and GPT432k (gpt-4-32k).\n\n3. **What is the purpose of the `ProcessFile` and `ProcessFolder` types?**\n\n   The `ProcessFile` type is a function type that takes a `ProcessFileParams` object as input and returns a Promise. It is used to process a single file in the autodoc project. The `ProcessFolder` type is a function type that takes a `ProcessFolderParams` object as input and returns a Promise. It is used to process a folder in the autodoc project.","metadata":{"source":".autodoc\\docs\\markdown\\src\\types.md"}}],["30",{"pageContent":"[View code on GitHub](https://github.com/context-labs/autodoc/tsconfig.json)\n\nThe code provided is a configuration file for the TypeScript compiler in a project. It specifies various options that control how the TypeScript compiler should process the source code and generate the output JavaScript files. This configuration file is typically named `tsconfig.json` and is placed at the root of a TypeScript project.\n\nThe `compilerOptions` object contains several key-value pairs that define the behavior of the TypeScript compiler:\n\n- `rootDir`: Specifies the root directory of the source files. In this case, it is set to \"src\", meaning that the source files are located in the \"src\" folder.\n- `outDir`: Specifies the output directory for the compiled JavaScript files. In this case, it is set to \"dist\", meaning that the compiled files will be placed in the \"dist\" folder.\n- `strict`: Enables strict type checking, which helps catch potential issues in the code.\n- `target`: Specifies the ECMAScript target version for the output JavaScript files. In this case, it is set to \"es2020\", meaning that the output files will be compatible with ECMAScript 2020 features.\n- `module`: Specifies the module system to be used. In this case, it is set to \"ES2020\", meaning that the output files will use the ECMAScript 2020 module system.\n- `sourceMap`: Generates source map files, which help in debugging the compiled code by mapping it back to the original TypeScript source files.\n- `esModuleInterop`: Enables compatibility with ECMAScript modules for importing CommonJS modules.\n- `moduleResolution`: Specifies the module resolution strategy. In this case, it is set to \"node\", meaning that the Node.js module resolution algorithm will be used.\n- `allowSyntheticDefaultImports`: Allows default imports from modules with no default export.\n- `declaration`: Generates TypeScript declaration files (`.d.ts`) alongside the compiled JavaScript files, which can be useful for other projects that depend on this one.\n- `skipLibCheck`: Skips type checking of declaration files, which can speed up the compilation process.\n\nOverall, this configuration file helps ensure that the TypeScript compiler processes the source code according to the specified options, resulting in compiled JavaScript files that are compatible with the desired ECMAScript version and module system, while also providing useful features like source maps and strict type checking.\n## Questions: \n 1. **What is the purpose of the `rootDir` and `outDir` options in the configuration?**\n\n   The `rootDir` option specifies the root directory of the input files, while the `outDir` option specifies the output directory for the compiled files.\n\n2. **What does the `strict` option do in the configuration?**\n\n   The `strict` option enables a wide range of type checking behavior that results in stronger guarantees of program correctness.\n\n3. **What is the significance of the `target` and `module` options in the configuration?**\n\n   The `target` option specifies the ECMAScript target version for the output code, and the `module` option specifies the module system used in the output code. In this case, both are set to \"es2020\", which means the output code will be compatible with ECMAScript 2020 features and module system.","metadata":{"source":".autodoc\\docs\\markdown\\tsconfig.md"}}]]