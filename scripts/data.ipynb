{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41bb7e5c-6f9d-4ab1-9b43-40c756b66ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed autodoc.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, quote\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def parse_markdown_to_csv(md_content, csv_file_path):\n",
    "    heading_pattern = re.compile(r'^(#+)\\s*(.*)', re.MULTILINE)\n",
    "    headings_contents = []\n",
    "    current_heading = None\n",
    "    current_content = []\n",
    "    \n",
    "    for line in md_content.split('\\n'):\n",
    "        match = heading_pattern.match(line)\n",
    "        if match:\n",
    "            if current_heading is not None:\n",
    "                headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "            current_heading = match.group(2).strip()\n",
    "            current_content = []\n",
    "        else:\n",
    "            if line.strip():\n",
    "                current_content.append(line.strip())\n",
    "    \n",
    "    if current_heading is not None:\n",
    "        headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "    \n",
    "    df = pd.DataFrame(headings_contents, columns=['Title', 'Content'])\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "def fetch_and_convert_readme_to_csv(repo_urls, output_dir):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # GitHub API endpoint for fetching the contents of the README file\n",
    "    for url in repo_urls:\n",
    "        parsed_url = urlparse(url)\n",
    "        parts = parsed_url.path.strip('/').split('/')\n",
    "        repo_user, repo_name = parts[0], parts[1]\n",
    "        api_url = f\"https://api.github.com/repos/{repo_user}/{repo_name}/readme\"\n",
    "        \n",
    "        # Set up appropriate headers for GitHub API including the token for authorization\n",
    "        headers = {\n",
    "            'Accept': 'application/vnd.github.v3.raw',\n",
    "            'Authorization': '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'  # Replace 'YOUR_GITHUB_TOKEN' with your actual GitHub token\n",
    "        }\n",
    "        \n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            readme_content = response.text\n",
    "            csv_file_path = os.path.join(output_dir, f\"{repo_name}.csv\")\n",
    "            parse_markdown_to_csv(readme_content, csv_file_path)\n",
    "            print(f\"Processed {repo_name}.csv\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch README for {repo_name}: {response.status_code}\")\n",
    "\n",
    "# Example usage:\n",
    "repo_urls = [\n",
    "    'https://github.com/context-labs/autodoc'\n",
    "]\n",
    "\n",
    "fetch_and_convert_readme_to_csv(repo_urls, 'output_csv_files')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d005ac-8225-4d78-8650-09b98ff4ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved autodoc_context.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import base64\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def fetch_and_concatenate_source_code(repo_urls, output_dir, token):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'token {token}',\n",
    "        'Accept': 'application/vnd.github.v3.raw'  # Requests raw content directly\n",
    "    }\n",
    "\n",
    "    for url in repo_urls:\n",
    "        parsed_url = urlparse(url)\n",
    "        parts = parsed_url.path.strip('/').split('/')\n",
    "        repo_user, repo_name = parts[0], parts[1]\n",
    "\n",
    "        # Fetch the default branch\n",
    "        repo_info_url = f'https://api.github.com/repos/{repo_user}/{repo_name}'\n",
    "        repo_info_response = requests.get(repo_info_url, headers=headers)\n",
    "        if repo_info_response.status_code == 200:\n",
    "            default_branch = repo_info_response.json()['default_branch']\n",
    "        else:\n",
    "            print(f'Failed to fetch repo info for {repo_name}: {repo_info_response.status_code}')\n",
    "            continue\n",
    "\n",
    "        api_url = f'https://api.github.com/repos/{repo_user}/{repo_name}/git/trees/{default_branch}?recursive=true'\n",
    "        response = requests.get(api_url, headers={'Authorization': f'token {token}', 'Accept': 'application/vnd.github.v3+json'})\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            all_files_content = []\n",
    "\n",
    "            for file in data['tree']:\n",
    "                if file['type'] == 'blob' and file['path'].endswith(('.py', '.c', '.cpp', '.java', '.js', '.ts', '.go')):\n",
    "                    file_url = f\"https://api.github.com/repos/{repo_user}/{repo_name}/contents/{file['path']}?ref={default_branch}\"\n",
    "                    file_response = requests.get(file_url, headers=headers)\n",
    "                    if file_response.status_code == 200:\n",
    "                        file_content = file_response.text\n",
    "                        all_files_content.append(file_content)\n",
    "\n",
    "            concatenated_content = \"\\n\".join(all_files_content)\n",
    "            df = pd.DataFrame([concatenated_content], columns=['SourceCode'])\n",
    "            df.to_csv(os.path.join(output_dir, f'{repo_name}_context.csv'), index=False)\n",
    "            print(f'Saved {repo_name}_context.csv')\n",
    "        else:\n",
    "            print(f'Failed to fetch repository data for {repo_name}: {response.status_code}')\n",
    "\n",
    "# Example usage:\n",
    "repo_urls = [\n",
    "    \"https://github.com/context-labs/autodoc\"\n",
    "]\n",
    "output_directory = 'output_csv_files'\n",
    "github_token = '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'  # Replace with your GitHub access token\n",
    "\n",
    "fetch_and_concatenate_source_code(repo_urls, output_directory, github_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e78d0b1-b921-468b-8376-d3651bd2d6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed system-design-primer.csv\n",
      "Processed awesome-python.csv\n",
      "Processed Python.csv\n",
      "Processed stable-diffusion-webui.csv\n",
      "Processed youtube-dl.csv\n",
      "Processed transformers.csv\n",
      "Processed HelloGitHub.csv\n",
      "Processed langchain.csv\n",
      "Processed thefuck.csv\n",
      "Processed pytorch.csv\n",
      "Processed django.csv\n",
      "Processed models.csv\n",
      "Processed yt-dlp.csv\n",
      "Processed core.csv\n",
      "Processed flask.csv\n",
      "Processed funNLP.csv\n",
      "Processed ansible.csv\n",
      "Processed keras.csv\n",
      "Processed whisper.csv\n",
      "Processed scikit-learn.csv\n",
      "Processed d2l-zh.csv\n",
      "Processed llama.csv\n",
      "Processed private-gpt.csv\n",
      "Processed requests.csv\n",
      "Processed gpt-engineer.csv\n",
      "Processed screenshot-to-code.csv\n",
      "Processed faceswap.csv\n",
      "Processed you-get.csv\n",
      "Processed openpilot.csv\n",
      "Processed rich.csv\n",
      "Processed big-list-of-naughty-strings.csv\n",
      "Processed pandas.csv\n",
      "Processed CppCoreGuidelines.csv\n",
      "Processed MetaGPT.csv\n",
      "Processed python-patterns.csv\n",
      "Processed PaddleOCR.csv\n",
      "Processed ailearning.csv\n",
      "Processed black.csv\n",
      "Processed Deep-Learning-Papers-Reading-Roadmap.csv\n",
      "Processed sentry.csv\n",
      "Processed text-generation-webui.csv\n",
      "Processed Open-Assistant.csv\n",
      "Processed stablediffusion.csv\n",
      "Processed python-cheatsheet.csv\n",
      "Processed wtfpython.csv\n",
      "Processed diagrams.csv\n",
      "Processed GFPGAN.csv\n",
      "Processed airflow.csv\n",
      "Processed TaskMatrix.csv\n",
      "Processed FastChat.csv\n",
      "Processed ComfyUI.csv\n",
      "Processed 12306.csv\n",
      "Processed shadowsocks.csv\n",
      "Processed DeepSpeed.csv\n",
      "Processed jieba.csv\n",
      "Processed 30-Days-Of-Python.csv\n",
      "Processed nanoGPT.csv\n",
      "Processed streamlit.csv\n",
      "Processed ccxt.csv\n",
      "Processed certbot.csv\n",
      "Processed sqlmap.csv\n",
      "Processed Python.csv\n",
      "Processed pytorch-image-models.csv\n",
      "Processed linux-insides.csv\n",
      "Processed fairseq.csv\n",
      "Processed gradio.csv\n",
      "Processed stanford_alpaca.csv\n",
      "Processed spaCy.csv\n",
      "Processed jax.csv\n",
      "Processed ControlNet.csv\n",
      "Processed mmdetection.csv\n",
      "Processed Langchain-Chatchat.csv\n",
      "Processed django-rest-framework.csv\n",
      "Processed CheatSheetSeries.csv\n",
      "Processed data-science-ipython-notebooks.csv\n",
      "Processed numpy.csv\n",
      "Processed python-fire.csv\n",
      "Processed OpenBBTerminal.csv\n",
      "Processed Detectron.csv\n",
      "Processed freqtrade.csv\n",
      "Processed YouCompleteMe.csv\n",
      "Processed glances.csv\n",
      "Processed roop.csv\n",
      "Processed spleeter.csv\n",
      "Processed MiniGPT-4.csv\n",
      "Processed python-telegram-bot.csv\n",
      "Processed pipenv.csv\n",
      "Processed OpenVoice.csv\n",
      "Processed OpenDevin.csv\n",
      "Processed cascadia-code.csv\n",
      "Processed Mask_RCNN.csv\n",
      "Processed tinygrad.csv\n",
      "Processed so-vits-svc.csv\n",
      "Processed GPT-SoVITS.csv\n",
      "Processed jumpserver.csv\n",
      "Processed locust.csv\n",
      "Processed wttr.in.csv\n",
      "Processed textual.csv\n",
      "Processed celery.csv\n",
      "Processed algorithms.csv\n",
      "Processed wttr.in.csv\n",
      "Processed textual.csv\n",
      "Processed algorithms.csv\n",
      "Processed vnpy.csv\n",
      "Processed DeepFaceLive.csv\n",
      "Processed ultralytics.csv\n",
      "Processed JARVIS.csv\n",
      "Processed diffusers.csv\n",
      "Processed generative-models.csv\n",
      "Processed NLP-progress.csv\n",
      "Processed EasyOCR.csv\n",
      "Processed kitty.csv\n",
      "Processed pytorch-CycleGAN-and-pix2pix.csv\n",
      "Processed labelImg.csv\n",
      "Processed d2l-en.csv\n",
      "Processed PythonRobotics.csv\n",
      "Processed cookiecutter.csv\n",
      "Processed tornado.csv\n",
      "Processed insightface.csv\n",
      "Processed Awesome-Linux-Software.csv\n",
      "Processed deep-learning-for-image-processing.csv\n",
      "Processed macOS-Security-and-Privacy-Guide.csv\n",
      "Processed Gooey.csv\n",
      "Processed pytorch_geometric.csv\n",
      "Processed zulip.csv\n",
      "Processed jina.csv\n",
      "Processed GitHub520.csv\n",
      "Processed ArchiveBox.csv\n",
      "Processed audiocraft.csv\n",
      "Processed llama3.csv\n",
      "Processed Retrieval-based-Voice-Conversion-WebUI.csv\n",
      "Processed matplotlib.csv\n",
      "Processed ddia.csv\n",
      "Processed localGPT.csv\n",
      "Processed vllm.csv\n",
      "Processed manim.csv\n",
      "Processed ungoogled-chromium.csv\n",
      "Processed minGPT.csv\n",
      "Processed magenta.csv\n",
      "Processed datasets.csv\n",
      "Processed unilm.csv\n",
      "Processed mkdocs.csv\n",
      "Processed magic-wormhole.csv\n",
      "Processed vit-pytorch.csv\n",
      "Processed prophet.csv\n",
      "Processed python-spider.csv\n",
      "Processed Chinese-LLaMA-Alpaca.csv\n",
      "Processed pyscript.csv\n",
      "Processed posthog.csv\n",
      "Processed mlflow.csv\n",
      "Processed wagtail.csv\n",
      "Processed game-programmer.csv\n",
      "Processed Ciphey.csv\n",
      "Processed zipline.csv\n",
      "Processed paperless-ngx.csv\n",
      "Processed erpnext.csv\n",
      "Processed devika.csv\n",
      "Processed inter.csv\n",
      "Processed kivy.csv\n",
      "Processed reflex.csv\n",
      "Processed onnx.csv\n",
      "Processed reddit.csv\n",
      "Processed Open-Sora.csv\n",
      "Processed GPT_API_free.csv\n",
      "Processed InstaPy.csv\n",
      "Processed awesome-free-chatgpt.csv\n",
      "Processed ml-stable-diffusion.csv\n",
      "Processed awesome-quant.csv\n",
      "Processed avatarify-python.csv\n",
      "Processed sd-webui-controlnet.csv\n",
      "Processed autojump.csv\n",
      "Processed PyTorch-GAN.csv\n",
      "Processed awesome-python-login-model.csv\n",
      "Processed twint.csv\n",
      "Processed ChatGLM2-6B.csv\n",
      "Processed neural-networks-and-deep-learning.csv\n",
      "Processed vision.csv\n",
      "Processed Shadowrocket-ADBlock-Rules.csv\n",
      "Processed baselines.csv\n",
      "Processed plotly.py.csv\n",
      "Processed gensim.csv\n",
      "Processed codellama.csv\n",
      "Processed click.csv\n",
      "Processed spotify-downloader.csv\n",
      "Processed changedetection.io.csv\n",
      "Processed ultimatevocalremovergui.csv\n",
      "Processed netbox.csv\n",
      "Processed GHunt.csv\n",
      "Processed ranger.csv\n",
      "Processed aws-cli.csv\n",
      "Processed frigate.csv\n",
      "Processed voice-changer.csv\n",
      "Processed rembg.csv\n",
      "Processed dalle-mini.csv\n",
      "Processed fabric.csv\n",
      "Processed aiohttp.csv\n",
      "Processed SuperAGI.csv\n",
      "Processed Bringing-Old-Photos-Back-to-Life.csv\n",
      "Processed typer.csv\n",
      "Processed discord.py.csv\n",
      "Processed mlc-llm.csv\n",
      "Processed zipline.csv\n",
      "Processed erpnext.csv\n",
      "Processed devika.csv\n",
      "Processed inter.csv\n",
      "Processed kivy.csv\n",
      "Processed reddit.csv\n",
      "Processed GPT_API_free.csv\n",
      "Processed InstaPy.csv\n",
      "Processed awesome-free-chatgpt.csv\n",
      "Processed PySnooper.csv\n",
      "Processed ml-stable-diffusion.csv\n",
      "Processed ipython.csv\n",
      "Processed awesome-quant.csv\n",
      "Processed avatarify-python.csv\n",
      "Processed sd-webui-controlnet.csv\n",
      "Processed autojump.csv\n",
      "Processed learn-python.csv\n",
      "Processed PyTorch-GAN.csv\n",
      "Processed awesome-python-login-model.csv\n",
      "Processed twint.csv\n",
      "Processed ChatGLM2-6B.csv\n",
      "Processed learn_python3_spider.csv\n",
      "Processed neural-networks-and-deep-learning.csv\n",
      "Processed vision.csv\n",
      "Processed Shadowrocket-ADBlock-Rules.csv\n",
      "Processed SMSBoom.csv\n",
      "Processed baselines.csv\n",
      "Processed gensim.csv\n",
      "Processed awesome-oss-alternatives.csv\n",
      "Processed codellama.csv\n",
      "Processed click.csv\n",
      "Processed spotify-downloader.csv\n",
      "Processed changedetection.io.csv\n",
      "Processed ultimatevocalremovergui.csv\n",
      "Processed netbox.csv\n",
      "Processed GHunt.csv\n",
      "Processed ranger.csv\n",
      "Processed aws-cli.csv\n",
      "Processed frigate.csv\n",
      "Processed voice-changer.csv\n",
      "Processed ChuanhuChatGPT.csv\n",
      "Processed prefect.csv\n",
      "Processed jupyter.csv\n",
      "Processed facefusion.csv\n",
      "Processed rembg.csv\n",
      "Processed dalle-mini.csv\n",
      "Processed fabric.csv\n",
      "Processed aiohttp.csv\n",
      "Processed numpy-ml.csv\n",
      "Processed SuperAGI.csv\n",
      "Processed Bringing-Old-Photos-Back-to-Life.csv\n",
      "Processed pyecharts.csv\n",
      "Processed discord.py.csv\n",
      "Processed fauxpilot.csv\n",
      "Processed mackup.csv\n",
      "Processed DeDRM_tools.csv\n",
      "Processed qlib.csv\n",
      "Processed networkx.csv\n",
      "Processed powerline.csv\n",
      "Processed DocsGPT.csv\n",
      "Processed python-mini-projects.csv\n",
      "Processed airbyte.csv\n",
      "Processed imgaug.csv\n",
      "Processed supervision.csv\n",
      "Processed py12306.csv\n",
      "Processed the-gan-zoo.csv\n",
      "Processed ivy.csv\n",
      "Processed peft.csv\n",
      "Processed stylegan.csv\n",
      "Processed YYeTsBot.csv\n",
      "Processed ChatterBot.csv\n",
      "Processed sentence-transformers.csv\n",
      "Processed wechat_jump_game.csv\n",
      "Processed nni.csv\n",
      "Processed haystack.csv\n",
      "Processed newspaper.csv\n",
      "Processed crewAI.csv\n",
      "Processed yapf.csv\n",
      "Processed requests-html.csv\n",
      "Processed flair.csv\n",
      "Processed CodeFormer.csv\n",
      "Processed examples-of-web-crawlers.csv\n",
      "Processed facenet.csv\n",
      "Processed MediaCrawler.csv\n",
      "Processed awx.csv\n",
      "Processed albumentations.csv\n",
      "Processed mailinabox.csv\n",
      "Processed speedtest-cli.csv\n",
      "Processed searx.csv\n",
      "Processed reinforcement-learning-an-introduction.csv\n",
      "Processed dvc.csv\n",
      "Processed PySimpleGUI.csv\n",
      "Processed backtrader.csv\n",
      "Processed Swin-Transformer.csv\n",
      "Processed transferlearning.csv\n",
      "Processed detr.csv\n",
      "Processed explainshell.csv\n",
      "Processed XSStrike.csv\n",
      "Processed impacket.csv\n",
      "Processed mihomo.csv\n",
      "Processed wifiphisher.csv\n",
      "Processed AutoEq.csv\n",
      "Processed tushare.csv\n",
      "Processed memray.csv\n",
      "Processed EIPs.csv\n",
      "Processed PaddleHub.csv\n",
      "Processed sympy.csv\n",
      "Processed beets.csv\n",
      "Processed labelme.csv\n",
      "Processed httpx.csv\n",
      "Processed redis-py.csv\n",
      "Processed pelican.csv\n",
      "Processed ChatGLM3.csv\n",
      "Processed awesome-aws.csv\n",
      "Processed pyright.csv\n",
      "Processed pre-commit.csv\n",
      "Processed PaddleDetection.csv\n",
      "Processed OCRmyPDF.csv\n",
      "Processed chatgpt-mirai-qq-bot.csv\n",
      "Processed ydata-profiling.csv\n",
      "Processed dask.csv\n",
      "Processed yfinance.csv\n",
      "Processed pix2code.csv\n",
      "Processed routersploit.csv\n",
      "Processed neural-enhance.csv\n",
      "Processed moviepy.csv\n",
      "Processed walle-web.csv\n",
      "Processed MOSS.csv\n",
      "Processed spiderfoot.csv\n",
      "Processed alphafold.csv\n",
      "Processed pgcli.csv\n",
      "Processed Pillow.csv\n",
      "Processed RWKV-LM.csv\n",
      "Processed allennlp.csv\n",
      "Processed Llama-Chinese.csv\n",
      "Processed developer.csv\n",
      "Processed calibre-web.csv\n",
      "Processed cookiecutter-django.csv\n",
      "Processed fashion-mnist.csv\n",
      "Processed gaussian-splatting.csv\n",
      "Processed pwntools.csv\n",
      "Processed schedule.csv\n",
      "Processed PaddleNLP.csv\n",
      "Processed tutorials.csv\n",
      "Processed pytest.csv\n",
      "Processed MHDDoS.csv\n",
      "Processed shap-e.csv\n",
      "Processed pyinstaller.csv\n",
      "Processed owasp-mastg.csv\n",
      "Processed abu.csv\n",
      "Processed dirsearch.csv\n",
      "Processed tvm.csv\n",
      "Processed deep_learning_object_detection.csv\n",
      "Processed Qwen.csv\n",
      "Processed scalene.csv\n",
      "Processed dspy.csv\n",
      "Processed sshuttle.csv\n",
      "Processed DB-GPT.csv\n",
      "Processed FastPhotoStyle.csv\n",
      "Processed pandas-ai.csv\n",
      "Processed OpenCore-Legacy-Patcher.csv\n",
      "Processed LaTeX-OCR.csv\n",
      "Processed activitywatch.csv\n",
      "Processed External-Attention-pytorch.csv\n",
      "Processed chia-blockchain.csv\n",
      "Processed DALLE2-pytorch.csv\n",
      "Processed peewee.csv\n",
      "Processed dolly.csv\n",
      "Processed playwright-python.csv\n",
      "Processed theZoo.csv\n",
      "Processed DALL-E.csv\n",
      "Processed chinese-xinhua.csv\n",
      "Processed gdb-dashboard.csv\n",
      "Processed Meshroom.csv\n",
      "Processed borg.csv\n",
      "Processed apprise.csv\n",
      "Processed SadTalker.csv\n",
      "Processed Photon.csv\n",
      "Processed h2ogpt.csv\n",
      "Processed howdoi.csv\n",
      "Processed urh.csv\n",
      "Processed theHarvester.csv\n",
      "Processed python-mastery.csv\n",
      "Processed pytube.csv\n",
      "Processed CustomTkinter.csv\n",
      "Processed chalice.csv\n",
      "Processed dagster.csv\n",
      "Processed stanford-tensorflow-tutorials.csv\n",
      "Processed social-engineer-toolkit.csv\n",
      "Processed gallery-dl.csv\n",
      "Processed github-trends.csv\n",
      "Processed Open-Sora-Plan.csv\n",
      "Processed amazing-qr.csv\n",
      "Processed AnimatedDrawings.csv\n",
      "Processed PaddleSpeech.csv\n",
      "Processed binwalk.csv\n",
      "Processed gorilla.csv\n",
      "Processed deepface.csv\n",
      "Processed NeMo.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Fetch a list of 200 best GitHub repositories written in Python\n",
    "def fetch_python_repo_urls():\n",
    "    repo_urls = []\n",
    "    headers = {\n",
    "        'Authorization': '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',  # Replace with your GitHub token\n",
    "        'Accept': 'application/vnd.github.v3+json'\n",
    "    }\n",
    "    query = 'language:python'\n",
    "    per_page = 100\n",
    "    total_repos = 300\n",
    "\n",
    "    for page in range(1, (total_repos // per_page) + 2):\n",
    "        search_url = f'https://api.github.com/search/repositories?q={query}&sort=stars&order=desc&per_page={per_page}&page={page}'\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            repo_data = response.json()['items']\n",
    "            repo_urls.extend([repo['html_url'] for repo in repo_data])\n",
    "        else:\n",
    "            print(f\"Failed to fetch repositories: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    return repo_urls\n",
    "\n",
    "# Parse the README.md content into a CSV\n",
    "def parse_markdown_to_csv(md_content, csv_file_path):\n",
    "    heading_pattern = re.compile(r'^(#+)\\s*(.*)', re.MULTILINE)\n",
    "    headings_contents = []\n",
    "    current_heading = None\n",
    "    current_content = []\n",
    "\n",
    "    for line in md_content.split('\\n'):\n",
    "        match = heading_pattern.match(line)\n",
    "        if match:\n",
    "            if current_heading is not None:\n",
    "                headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "            current_heading = match.group(2).strip()\n",
    "            current_content = []\n",
    "        else:\n",
    "            if line.strip():\n",
    "                current_content.append(line.strip())\n",
    "\n",
    "    if current_heading is not None:\n",
    "        headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "\n",
    "    df = pd.DataFrame(headings_contents, columns=['Title', 'Content'])\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Fetch and convert README files to CSV\n",
    "def fetch_and_convert_readme_to_csv(repo_urls, output_dir, github_token):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    headers = {\n",
    "        'Accept': 'application/vnd.github.v3.raw',\n",
    "        'Authorization': f'token {github_token}'\n",
    "    }\n",
    "\n",
    "    for url in repo_urls:\n",
    "        parsed_url = urlparse(url)\n",
    "        parts = parsed_url.path.strip('/').split('/')\n",
    "        repo_user, repo_name = parts[0], parts[1]\n",
    "        api_url = f'https://api.github.com/repos/{repo_user}/{repo_name}/readme'\n",
    "\n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            readme_content = response.text\n",
    "            csv_file_path = os.path.join(output_dir, f\"{repo_name}.csv\")\n",
    "            parse_markdown_to_csv(readme_content, csv_file_path)\n",
    "            print(f\"Processed {repo_name}.csv\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch README for {repo_name}: {response.status_code}\")\n",
    "\n",
    "# Usage\n",
    "github_token = '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'  # Replace with your actual GitHub Personal Access Token\n",
    "repo_urls = fetch_python_repo_urls()\n",
    "output_directory = 'output_csv_files'\n",
    "fetch_and_convert_readme_to_csv(repo_urls, output_directory, github_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f96d8c-589b-446b-bceb-63f4fa4b9498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining API requests: 4319\n",
      "Error processing public-apis: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing system-design-primer: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing awesome-python: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing Python: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing Python-100-Days: Extra data: line 1 column 3 (char 2)\n",
      "Error processing stable-diffusion-webui: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing youtube-dl: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing transformers: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing HelloGitHub: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing langchain: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing thefuck: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing pytorch: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing django: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing models: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing yt-dlp: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing fastapi: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing core: Extra data: line 1 column 3 (char 2)\n",
      "Error processing flask: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing funNLP: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing devops-exercises: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 131>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Process the repositories and save source code to CSV\u001b[39;00m\n\u001b[1;32m    130\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_csv_files\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 131\u001b[0m \u001b[43mfetch_and_concatenate_source_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_urls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgithub_token\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mfetch_and_concatenate_source_code\u001b[0;34m(repo_urls, output_dir, token)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Fetch the default branch\u001b[39;00m\n\u001b[1;32m     73\u001b[0m repo_info_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://api.github.com/repos/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_user\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 74\u001b[0m repo_info_response \u001b[38;5;241m=\u001b[39m \u001b[43mretry_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_info_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_info_response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     76\u001b[0m     default_branch \u001b[38;5;241m=\u001b[39m repo_info_response\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_branch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(url, headers, retries, delay)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretry_request\u001b[39m(url, headers, retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, delay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[0;32m---> 46\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[1;32m   1045\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1046\u001b[0m         (\n\u001b[1;32m   1047\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1053\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/connection.py:414\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_certs\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_cert_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(context, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_default_certs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    411\u001b[0m ):\n\u001b[1;32m    412\u001b[0m     context\u001b[38;5;241m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# for the host.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    431\u001b[0m     default_ssl_context\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39mversion() \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTLSv1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    435\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/urllib3/util/ssl_.py:402\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ca_certs \u001b[38;5;129;01mor\u001b[39;00m ca_cert_dir \u001b[38;5;129;01mor\u001b[39;00m ca_cert_data:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIOError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import base64\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Fetch 200 best Python GitHub repo URLs\n",
    "def fetch_python_repo_urls(token, total_repos=300):\n",
    "    repo_urls = []\n",
    "    headers = {\n",
    "        'Authorization': f'token {token}',\n",
    "        'Accept': 'application/vnd.github.v3+json'\n",
    "    }\n",
    "    query = 'language:python'\n",
    "    per_page = 100\n",
    "\n",
    "    for page in range(1, (total_repos // per_page) + 2):\n",
    "        search_url = f'https://api.github.com/search/repositories?q={query}&sort=stars&order=desc&per_page={per_page}&page={page}'\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            repo_data = response.json()['items']\n",
    "            repo_urls.extend([repo['html_url'] for repo in repo_data])\n",
    "        else:\n",
    "            print(f\"Failed to fetch repositories: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "\n",
    "    return repo_urls\n",
    "\n",
    "# Function to check API rate limit\n",
    "def check_rate_limit(headers):\n",
    "    rate_limit_url = \"https://api.github.com/rate_limit\"\n",
    "    response = requests.get(rate_limit_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        rate_limit_data = response.json()\n",
    "        remaining = rate_limit_data[\"resources\"][\"core\"][\"remaining\"]\n",
    "        print(f\"Remaining API requests: {remaining}\")\n",
    "        return remaining\n",
    "    else:\n",
    "        print(f\"Failed to check rate limit: {response.status_code}, {response.text}\")\n",
    "        return 0\n",
    "\n",
    "# Retry logic wrapper\n",
    "def retry_request(url, headers, retries=3, delay=2):\n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        print(f\"Attempt {attempt + 1} failed: {response.status_code}, {response.text}. Retrying in {delay} seconds...\")\n",
    "        time.sleep(delay)\n",
    "    return response\n",
    "\n",
    "# Function to fetch and concatenate source code, and save to CSV\n",
    "def fetch_and_concatenate_source_code(repo_urls, output_dir, token):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': f'token {token}',\n",
    "        'Accept': 'application/vnd.github.v3.raw'  # Requests raw content directly\n",
    "    }\n",
    "\n",
    "    if check_rate_limit(headers) == 0:\n",
    "        print(\"API rate limit exceeded. Try again later.\")\n",
    "        return\n",
    "\n",
    "    for url in repo_urls:\n",
    "        parsed_url = urlparse(url)\n",
    "        parts = parsed_url.path.strip('/').split('/')\n",
    "        repo_user, repo_name = parts[0], parts[1]\n",
    "\n",
    "        # Fetch the default branch\n",
    "        repo_info_url = f'https://api.github.com/repos/{repo_user}/{repo_name}'\n",
    "        repo_info_response = retry_request(repo_info_url, headers)\n",
    "        if repo_info_response.status_code == 200:\n",
    "            default_branch = repo_info_response.json().get('default_branch', 'main')\n",
    "        else:\n",
    "            print(f'Failed to fetch repo info for {repo_name}: {repo_info_response.status_code}, {repo_info_response.text}')\n",
    "            continue\n",
    "\n",
    "        api_url = f'https://api.github.com/repos/{repo_user}/{repo_name}/git/trees/{default_branch}?recursive=true'\n",
    "        response = retry_request(api_url, headers={'Authorization': f'token {token}', 'Accept': 'application/vnd.github.v3+json'})\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                data = response.json()\n",
    "                all_files_content = []\n",
    "\n",
    "                for file in data['tree']:\n",
    "                    if file['type'] == 'blob' and file['path'].endswith(('.py', '.c', '.cpp', '.java', '.js', '.ts', '.go')):\n",
    "                        file_url = f\"https://api.github.com/repos/{repo_user}/{repo_name}/contents/{file['path']}?ref={default_branch}\"\n",
    "                        file_response = retry_request(file_url, headers)\n",
    "                        if file_response.status_code == 200:\n",
    "                            file_data = file_response.json()\n",
    "                            if 'content' in file_data:\n",
    "                                try:\n",
    "                                    file_content = base64.b64decode(file_data['content'])\n",
    "                                    try:\n",
    "                                        file_content = file_content.decode('utf-8')\n",
    "                                        if file_content:\n",
    "                                            all_files_content.append(file_content)\n",
    "                                        else:\n",
    "                                            print(f\"File {file['path']} from {repo_name} is empty. Skipping...\")\n",
    "                                    except UnicodeDecodeError:\n",
    "                                        print(f\"Failed to decode file {file['path']} from {repo_name} as UTF-8. Skipping...\")\n",
    "                                except (ValueError, UnicodeDecodeError) as e:\n",
    "                                    print(f\"Failed to decode file {file['path']} from {repo_name}: {e}. Skipping...\")\n",
    "                            else:\n",
    "                                print(f\"File {file['path']} from {repo_name} does not have content. Skipping...\")\n",
    "                        else:\n",
    "                            print(f\"Failed to fetch file {file['path']} from {repo_name}: {file_response.status_code}, {file_response.text}\")\n",
    "\n",
    "                if all_files_content:\n",
    "                    concatenated_content = \"\\n\".join(all_files_content)\n",
    "                    df = pd.DataFrame([concatenated_content], columns=['SourceCode'])\n",
    "                    df.to_csv(os.path.join(output_dir, f'{repo_name}_context.csv'), index=False)\n",
    "                    print(f'Saved {repo_name}_context.csv')\n",
    "                else:\n",
    "                    print(f\"No files could be processed for {repo_name}. Skipping...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {repo_name}: {e}\")\n",
    "        else:\n",
    "            print(f'Failed to fetch repository data for {repo_name}: {response.status_code}, {response.text}')\n",
    "\n",
    "# Fetch 200 best Python repositories\n",
    "github_token = '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'  # Replace with your GitHub Personal Access Token\n",
    "repo_urls = fetch_python_repo_urls(github_token, total_repos=300)\n",
    "\n",
    "# Process the repositories and save source code to CSV\n",
    "output_directory = 'context_csv_files'\n",
    "fetch_and_concatenate_source_code(repo_urls, output_directory, github_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba939428-e9b5-4555-9e8d-f517b0da0cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://github.com/public-apis/public-apis', 'https://github.com/donnemartin/system-design-primer', 'https://github.com/vinta/awesome-python', 'https://github.com/TheAlgorithms/Python', 'https://github.com/jackfrued/Python-100-Days', 'https://github.com/AUTOMATIC1111/stable-diffusion-webui', 'https://github.com/ytdl-org/youtube-dl', 'https://github.com/huggingface/transformers', 'https://github.com/521xueweihan/HelloGitHub', 'https://github.com/langchain-ai/langchain', 'https://github.com/nvbn/thefuck', 'https://github.com/pytorch/pytorch', 'https://github.com/django/django', 'https://github.com/tensorflow/models', 'https://github.com/yt-dlp/yt-dlp', 'https://github.com/tiangolo/fastapi', 'https://github.com/home-assistant/core', 'https://github.com/pallets/flask', 'https://github.com/fighting41love/funNLP', 'https://github.com/bregman-arie/devops-exercises', 'https://github.com/josephmisiti/awesome-machine-learning', 'https://github.com/ansible/ansible', 'https://github.com/keras-team/keras', 'https://github.com/openai/whisper', 'https://github.com/python/cpython', 'https://github.com/3b1b/manim', 'https://github.com/scikit-learn/scikit-learn', 'https://github.com/xtekky/gpt4free', 'https://github.com/binary-husky/gpt_academic', 'https://github.com/d2l-ai/d2l-zh', 'https://github.com/swisskyrepo/PayloadsAllTheThings', 'https://github.com/meta-llama/llama', 'https://github.com/localstack/localstack', 'https://github.com/zylon-ai/private-gpt', 'https://github.com/ageitgey/face_recognition', 'https://github.com/sherlock-project/sherlock', 'https://github.com/psf/requests', 'https://github.com/scrapy/scrapy', 'https://github.com/CorentinJ/Real-Time-Voice-Cloning', 'https://github.com/gpt-engineer-org/gpt-engineer', 'https://github.com/abi/screenshot-to-code', 'https://github.com/deepfakes/faceswap', 'https://github.com/soimort/you-get', 'https://github.com/OpenInterpreter/open-interpreter', 'https://github.com/xai-org/grok-1', 'https://github.com/commaai/openpilot', 'https://github.com/Textualize/rich', 'https://github.com/ultralytics/yolov5', 'https://github.com/minimaxir/big-list-of-naughty-strings', 'https://github.com/iperov/DeepFaceLab', 'https://github.com/charlax/professional-programming', 'https://github.com/Z4nzu/hackingtool', 'https://github.com/pandas-dev/pandas', 'https://github.com/isocpp/CppCoreGuidelines', 'https://github.com/geekan/MetaGPT', 'https://github.com/faif/python-patterns', 'https://github.com/THUDM/ChatGLM-6B', 'https://github.com/PaddlePaddle/PaddleOCR', 'https://github.com/apachecn/ailearning', 'https://github.com/hpcaitech/ColossalAI', 'https://github.com/chubin/cheat.sh', 'https://github.com/psf/black', 'https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap', 'https://github.com/google-research/bert', 'https://github.com/getsentry/sentry', 'https://github.com/oobabooga/text-generation-webui', 'https://github.com/LAION-AI/Open-Assistant', 'https://github.com/Stability-AI/stablediffusion', 'https://github.com/0voice/interview_internal_reference', 'https://github.com/gto76/python-cheatsheet', 'https://github.com/lllyasviel/Fooocus', 'https://github.com/XingangPan/DragGAN', 'https://github.com/satwikkansal/wtfpython', 'https://github.com/mingrammer/diagrams', 'https://github.com/odoo/odoo', 'https://github.com/TencentARC/GFPGAN', 'https://github.com/apache/airflow', 'https://github.com/chenfei-wu/TaskMatrix', 'https://github.com/mitmproxy/mitmproxy', 'https://github.com/lm-sys/FastChat', 'https://github.com/comfyanonymous/ComfyUI', 'https://github.com/babysor/MockingBird', 'https://github.com/openai/gym', 'https://github.com/testerSunshine/12306', 'https://github.com/shadowsocks/shadowsocks', 'https://github.com/microsoft/DeepSpeed', 'https://github.com/XX-net/XX-Net', 'https://github.com/fxsjy/jieba', 'https://github.com/hankcs/HanLP', 'https://github.com/Asabeneh/30-Days-Of-Python', 'https://github.com/karpathy/nanoGPT', 'https://github.com/httpie/cli', 'https://github.com/streamlit/streamlit', 'https://github.com/ccxt/ccxt', 'https://github.com/run-llama/llama_index', 'https://github.com/ray-project/ray', 'https://github.com/certbot/certbot', 'https://github.com/sqlmapproject/sqlmap', 'https://github.com/geekcomputers/Python', 'https://github.com/huggingface/pytorch-image-models', 'https://github.com/coqui-ai/TTS', 'https://github.com/python-poetry/poetry', 'https://github.com/0xAX/linux-insides', 'https://github.com/facebookresearch/fairseq', 'https://github.com/gradio-app/gradio', 'https://github.com/yunjey/pytorch-tutorial', 'https://github.com/tatsu-lab/stanford_alpaca', 'https://github.com/explosion/spaCy', 'https://github.com/donnemartin/interactive-coding-challenges', 'https://github.com/facebookresearch/detectron2', 'https://github.com/Pythagora-io/gpt-pilot', 'https://github.com/google/jax', 'https://github.com/lllyasviel/ControlNet', 'https://github.com/acheong08/ChatGPT', 'https://github.com/open-mmlab/mmdetection', 'https://github.com/chatchat-space/Langchain-Chatchat', 'https://github.com/encode/django-rest-framework', 'https://github.com/tqdm/tqdm', 'https://github.com/Lightning-AI/pytorch-lightning', 'https://github.com/LC044/WeChatMsg', 'https://github.com/OWASP/CheatSheetSeries', 'https://github.com/donnemartin/data-science-ipython-notebooks', 'https://github.com/numpy/numpy', 'https://github.com/google/python-fire', 'https://github.com/xinntao/Real-ESRGAN', 'https://github.com/OpenBB-finance/OpenBBTerminal', 'https://github.com/facebookresearch/Detectron', 'https://github.com/freqtrade/freqtrade', 'https://github.com/StevenBlack/hosts', 'https://github.com/ycm-core/YouCompleteMe', 'https://github.com/spipm/Depix', 'https://github.com/zhayujie/chatgpt-on-wechat', 'https://github.com/littlecodersh/ItChat', 'https://github.com/nicolargo/glances', 'https://github.com/s0md3v/roop', 'https://github.com/getredash/redash', 'https://github.com/deezer/spleeter', 'https://github.com/Vision-CAIR/MiniGPT-4', 'https://github.com/python-telegram-bot/python-telegram-bot', 'https://github.com/pypa/pipenv', 'https://github.com/myshell-ai/OpenVoice', 'https://github.com/OpenDevin/OpenDevin', 'https://github.com/microsoft/cascadia-code', 'https://github.com/matterport/Mask_RCNN', 'https://github.com/tinygrad/tinygrad', 'https://github.com/svc-develop-team/so-vits-svc', 'https://github.com/RVC-Boss/GPT-SoVITS', 'https://github.com/jumpserver/jumpserver', 'https://github.com/locustio/locust', 'https://github.com/chubin/wttr.in', 'https://github.com/Textualize/textual', 'https://github.com/celery/celery', 'https://github.com/keon/algorithms', 'https://github.com/vnpy/vnpy', 'https://github.com/iperov/DeepFaceLive', 'https://github.com/ultralytics/ultralytics', 'https://github.com/eriklindernoren/ML-From-Scratch', 'https://github.com/microsoft/JARVIS', 'https://github.com/huggingface/diffusers', 'https://github.com/wangzheng0822/algo', 'https://github.com/mouredev/Hello-Python', 'https://github.com/Stability-AI/generative-models', 'https://github.com/sebastianruder/NLP-progress', 'https://github.com/JaidedAI/EasyOCR', 'https://github.com/kovidgoyal/kitty', 'https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix', 'https://github.com/HumanSignal/labelImg', 'https://github.com/d2l-ai/d2l-en', 'https://github.com/AtsushiSakai/PythonRobotics', 'https://github.com/pytorch/examples', 'https://github.com/cookiecutter/cookiecutter', 'https://github.com/tornadoweb/tornado', 'https://github.com/hiyouga/LLaMA-Factory', 'https://github.com/mindsdb/mindsdb', 'https://github.com/deepinsight/insightface', 'https://github.com/openai/gpt-2', 'https://github.com/luong-komorebi/Awesome-Linux-Software', 'https://github.com/WZMIAOMIAO/deep-learning-for-image-processing', 'https://github.com/drduh/macOS-Security-and-Privacy-Guide', 'https://github.com/openai/chatgpt-retrieval-plugin', 'https://github.com/plotly/dash', 'https://github.com/chriskiehl/Gooey', 'https://github.com/jhao104/proxy_pool', 'https://github.com/pyg-team/pytorch_geometric', 'https://github.com/saleor/saleor', 'https://github.com/zulip/zulip', 'https://github.com/jina-ai/jina', 'https://github.com/openai/openai-python', 'https://github.com/KurtBestor/Hitomi-Downloader', 'https://github.com/521xueweihan/GitHub520', 'https://github.com/ArchiveBox/ArchiveBox', 'https://github.com/facebookresearch/audiocraft', 'https://github.com/meta-llama/llama3', 'https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI', 'https://github.com/matplotlib/matplotlib', 'https://github.com/yoheinakajima/babyagi', 'https://github.com/Vonng/ddia', 'https://github.com/PromtEngineer/localGPT', 'https://github.com/vllm-project/vllm', 'https://github.com/ManimCommunity/manim', 'https://github.com/ungoogled-software/ungoogled-chromium', 'https://github.com/karpathy/minGPT', 'https://github.com/magenta/magenta', 'https://github.com/bokeh/bokeh', 'https://github.com/pydantic/pydantic', 'https://github.com/huggingface/datasets', 'https://github.com/microsoft/unilm', 'https://github.com/kholia/OSX-KVM', 'https://github.com/kovidgoyal/calibre', 'https://github.com/mkdocs/mkdocs', 'https://github.com/magic-wormhole/magic-wormhole', 'https://github.com/Delgan/loguru', 'https://github.com/lucidrains/vit-pytorch', 'https://github.com/nginx-proxy/nginx-proxy', 'https://github.com/recommenders-team/recommenders', 'https://github.com/RasaHQ/rasa', 'https://github.com/facebook/prophet', 'https://github.com/sanic-org/sanic', 'https://github.com/kaixindelele/ChatPaper', 'https://github.com/Jack-Cherish/python-spider', 'https://github.com/jantic/DeOldify', 'https://github.com/python/mypy', 'https://github.com/ymcui/Chinese-LLaMA-Alpaca', 'https://github.com/pyscript/pyscript', 'https://github.com/PostHog/posthog', 'https://github.com/mlflow/mlflow', 'https://github.com/spotify/luigi', 'https://github.com/wagtail/wagtail', 'https://github.com/Sanster/IOPaint', 'https://github.com/miloyip/game-programmer', 'https://github.com/joke2k/faker', 'https://github.com/mlc-ai/mlc-llm', 'https://github.com/Ciphey/Ciphey', 'https://github.com/quantopian/zipline', 'https://github.com/paperless-ngx/paperless-ngx', 'https://github.com/frappe/erpnext', 'https://github.com/stitionai/devika', 'https://github.com/rsms/inter', 'https://github.com/kivy/kivy', 'https://github.com/reflex-dev/reflex', 'https://github.com/onnx/onnx', 'https://github.com/reddit-archive/reddit', 'https://github.com/hpcaitech/Open-Sora', 'https://github.com/haotian-liu/LLaVA', 'https://github.com/chatanywhere/GPT_API_free', 'https://github.com/InstaPy/InstaPy', 'https://github.com/binux/pyspider', 'https://github.com/LiLittleCat/awesome-free-chatgpt', 'https://github.com/cool-RR/PySnooper', 'https://github.com/apple/ml-stable-diffusion', 'https://github.com/ipython/ipython', 'https://github.com/wilsonfreitas/awesome-quant', 'https://github.com/alievk/avatarify-python', 'https://github.com/Mikubill/sd-webui-controlnet', 'https://github.com/wting/autojump', 'https://github.com/trekhleb/learn-python', 'https://github.com/eriklindernoren/PyTorch-GAN', 'https://github.com/Kr1s77/awesome-python-login-model', 'https://github.com/twintproject/twint', 'https://github.com/THUDM/ChatGLM2-6B', 'https://github.com/wistbean/learn_python3_spider', 'https://github.com/mnielsen/neural-networks-and-deep-learning', 'https://github.com/pytorch/vision', 'https://github.com/h2y/Shadowrocket-ADBlock-Rules', 'https://github.com/OpenEthan/SMSBoom', 'https://github.com/openai/baselines', 'https://github.com/plotly/plotly.py', 'https://github.com/piskvorky/gensim', 'https://github.com/RunaCapital/awesome-oss-alternatives', 'https://github.com/meta-llama/codellama', 'https://github.com/pallets/click', 'https://github.com/spotDL/spotify-downloader', 'https://github.com/dgtlmoon/changedetection.io', 'https://github.com/Anjok07/ultimatevocalremovergui', 'https://github.com/netbox-community/netbox', 'https://github.com/mxrch/GHunt', 'https://github.com/ranger/ranger', 'https://github.com/tensorflow/tensor2tensor', 'https://github.com/aws/aws-cli', 'https://github.com/blakeblackshear/frigate', 'https://github.com/w-okada/voice-changer', 'https://github.com/GaiZhenbiao/ChuanhuChatGPT', 'https://github.com/PrefectHQ/prefect', 'https://github.com/jupyter/jupyter', 'https://github.com/facefusion/facefusion', 'https://github.com/danielgatis/rembg', 'https://github.com/borisdayma/dalle-mini', 'https://github.com/fabric/fabric', 'https://github.com/aio-libs/aiohttp', 'https://github.com/ddbourgin/numpy-ml', 'https://github.com/TransformerOptimus/SuperAGI', 'https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life', 'https://github.com/pyecharts/pyecharts', 'https://github.com/tiangolo/typer', 'https://github.com/Rapptz/discord.py', 'https://github.com/fauxpilot/fauxpilot', 'https://github.com/lra/mackup', 'https://github.com/apprenticeharper/DeDRM_tools', 'https://github.com/microsoft/qlib', 'https://github.com/networkx/networkx', 'https://github.com/powerline/powerline', 'https://github.com/arc53/DocsGPT', 'https://github.com/Python-World/python-mini-projects', 'https://github.com/airbytehq/airbyte', 'https://github.com/aleju/imgaug', 'https://github.com/roboflow/supervision', 'https://github.com/pjialin/py12306', 'https://github.com/hindupuravinash/the-gan-zoo', 'https://github.com/unifyai/ivy', 'https://github.com/openai/evals', 'https://github.com/horovod/horovod', 'https://github.com/huggingface/peft', 'https://github.com/NVlabs/stylegan', 'https://github.com/tgbot-collection/YYeTsBot', 'https://github.com/gunthercox/ChatterBot', 'https://github.com/UKPLab/sentence-transformers', 'https://github.com/saltstack/salt', 'https://github.com/wangshub/wechat_jump_game', 'https://github.com/youfou/wxpy', 'https://github.com/microsoft/nni', 'https://github.com/deepset-ai/haystack', 'https://github.com/codelucas/newspaper', 'https://github.com/joaomdmoura/crewAI', 'https://github.com/google/yapf', 'https://github.com/psf/requests-html', 'https://github.com/flairNLP/flair', 'https://github.com/sczhou/CodeFormer', 'https://github.com/shengqiangzhang/examples-of-web-crawlers', 'https://github.com/davidsandberg/facenet', 'https://github.com/NanmiCoder/MediaCrawler', 'https://github.com/ansible/awx', 'https://github.com/albumentations-team/albumentations', 'https://github.com/programthink/zhao', 'https://github.com/mail-in-a-box/mailinabox', 'https://github.com/sivel/speedtest-cli', 'https://github.com/searx/searx', 'https://github.com/ShangtongZhang/reinforcement-learning-an-introduction', 'https://github.com/iterative/dvc', 'https://github.com/PySimpleGUI/PySimpleGUI', 'https://github.com/mementum/backtrader', 'https://github.com/tiangolo/sqlmodel', 'https://github.com/nltk/nltk', 'https://github.com/dmlc/dgl', 'https://github.com/microsoft/Swin-Transformer', 'https://github.com/jindongwang/transferlearning', 'https://github.com/facebookresearch/detr', 'https://github.com/idank/explainshell', 'https://github.com/s0md3v/XSStrike', 'https://github.com/fortra/impacket', 'https://github.com/MetaCubeX/mihomo', 'https://github.com/wifiphisher/wifiphisher', 'https://github.com/jaakkopasanen/AutoEq', 'https://github.com/waditu/tushare', 'https://github.com/edgedb/edgedb', 'https://github.com/bloomberg/memray', 'https://github.com/ethereum/EIPs', 'https://github.com/PaddlePaddle/PaddleHub', 'https://github.com/scipy/scipy', 'https://github.com/chroma-core/chroma', 'https://github.com/sympy/sympy', 'https://github.com/beetbox/beets', 'https://github.com/postmanlabs/httpbin', 'https://github.com/labelmeai/labelme', 'https://github.com/SFTtech/openage', 'https://github.com/encode/httpx', 'https://github.com/redis/redis-py', 'https://github.com/getpelican/pelican', 'https://github.com/THUDM/ChatGLM3', 'https://github.com/jina-ai/clip-as-service', 'https://github.com/donnemartin/awesome-aws', 'https://github.com/microsoft/pyright', 'https://github.com/pre-commit/pre-commit', 'https://github.com/PaddlePaddle/PaddleDetection', 'https://github.com/ocrmypdf/OCRmyPDF', 'https://github.com/lss233/chatgpt-mirai-qq-bot', 'https://github.com/ydataai/ydata-profiling', 'https://github.com/dask/dask', 'https://github.com/mwaskom/seaborn', 'https://github.com/ranaroussi/yfinance', 'https://github.com/tonybeltramelli/pix2code', 'https://github.com/threat9/routersploit', 'https://github.com/Miserlou/Zappa', 'https://github.com/alexjc/neural-enhance', 'https://github.com/Zulko/moviepy', 'https://github.com/meolu/walle-web', 'https://github.com/OpenMOSS/MOSS', 'https://github.com/smicallef/spiderfoot', 'https://github.com/matrix-org/synapse', 'https://github.com/google-deepmind/alphafold', 'https://github.com/dbcli/pgcli', 'https://github.com/python-pillow/Pillow', 'https://github.com/BlinkDL/RWKV-LM', 'https://github.com/allenai/allennlp', 'https://github.com/LlamaFamily/Llama-Chinese', 'https://github.com/smol-ai/developer', 'https://github.com/janeczku/calibre-web', 'https://github.com/Embedding/Chinese-Word-Vectors', 'https://github.com/cookiecutter/cookiecutter-django', 'https://github.com/rougier/numpy-100', 'https://github.com/zalandoresearch/fashion-mnist']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "# Save the list to a pickle file\n",
    "with open('repo_urls.pickle', 'wb') as file:\n",
    "    pickle.dump(repo_urls, file)\n",
    "print(repo_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f62fd8-1e33-4250-8c9a-f14d5d9cdbb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
