{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4fd5fa2",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41bb7e5c-6f9d-4ab1-9b43-40c756b66ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed autodoc.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, quote\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def parse_markdown_to_csv(md_content, csv_file_path):\n",
    "    heading_pattern = re.compile(r'^(#+)\\s*(.*)', re.MULTILINE)\n",
    "    headings_contents = []\n",
    "    current_heading = None\n",
    "    current_content = []\n",
    "    \n",
    "    for line in md_content.split('\\n'):\n",
    "        match = heading_pattern.match(line)\n",
    "        if match:\n",
    "            if current_heading is not None:\n",
    "                headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "            current_heading = match.group(2).strip()\n",
    "            current_content = []\n",
    "        else:\n",
    "            if line.strip():\n",
    "                current_content.append(line.strip())\n",
    "    \n",
    "    if current_heading is not None:\n",
    "        headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "    \n",
    "    df = pd.DataFrame(headings_contents, columns=['Title', 'Content'])\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "def fetch_and_convert_readme_to_csv(repo_urls, output_dir):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # GitHub API endpoint for fetching the contents of the README file\n",
    "    for url in repo_urls:\n",
    "        parsed_url = urlparse(url)\n",
    "        parts = parsed_url.path.strip('/').split('/')\n",
    "        repo_user, repo_name = parts[0], parts[1]\n",
    "        api_url = f\"https://api.github.com/repos/{repo_user}/{repo_name}/readme\"\n",
    "        \n",
    "        # Set up appropriate headers for GitHub API including the token for authorization\n",
    "        headers = {\n",
    "            'Accept': 'application/vnd.github.v3.raw',\n",
    "            'Authorization': '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'  # Replace 'YOUR_GITHUB_TOKEN' with your actual GitHub token\n",
    "        }\n",
    "        \n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            readme_content = response.text\n",
    "            csv_file_path = os.path.join(output_dir, f\"{repo_name}.csv\")\n",
    "            parse_markdown_to_csv(readme_content, csv_file_path)\n",
    "            print(f\"Processed {repo_name}.csv\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch README for {repo_name}: {response.status_code}\")\n",
    "\n",
    "# Example usage:\n",
    "repo_urls = [\n",
    "    'https://github.com/context-labs/autodoc'\n",
    "]\n",
    "\n",
    "fetch_and_convert_readme_to_csv(repo_urls, 'output_csv_files')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d005ac-8225-4d78-8650-09b98ff4ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved autodoc_context.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import base64\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def fetch_and_concatenate_source_code(repo_urls, output_dir, token):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'token {token}',\n",
    "        'Accept': 'application/vnd.github.v3.raw'  # Requests raw content directly\n",
    "    }\n",
    "\n",
    "    for url in repo_urls:\n",
    "        parsed_url = urlparse(url)\n",
    "        parts = parsed_url.path.strip('/').split('/')\n",
    "        repo_user, repo_name = parts[0], parts[1]\n",
    "\n",
    "        # Fetch the default branch\n",
    "        repo_info_url = f'https://api.github.com/repos/{repo_user}/{repo_name}'\n",
    "        repo_info_response = requests.get(repo_info_url, headers=headers)\n",
    "        if repo_info_response.status_code == 200:\n",
    "            default_branch = repo_info_response.json()['default_branch']\n",
    "        else:\n",
    "            print(f'Failed to fetch repo info for {repo_name}: {repo_info_response.status_code}')\n",
    "            continue\n",
    "\n",
    "        api_url = f'https://api.github.com/repos/{repo_user}/{repo_name}/git/trees/{default_branch}?recursive=true'\n",
    "        response = requests.get(api_url, headers={'Authorization': f'token {token}', 'Accept': 'application/vnd.github.v3+json'})\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            all_files_content = []\n",
    "\n",
    "            for file in data['tree']:\n",
    "                if file['type'] == 'blob' and file['path'].endswith(('.py', '.c', '.cpp', '.java', '.js', '.ts', '.go')):\n",
    "                    file_url = f\"https://api.github.com/repos/{repo_user}/{repo_name}/contents/{file['path']}?ref={default_branch}\"\n",
    "                    file_response = requests.get(file_url, headers=headers)\n",
    "                    if file_response.status_code == 200:\n",
    "                        file_content = file_response.text\n",
    "                        all_files_content.append(file_content)\n",
    "\n",
    "            concatenated_content = \"\\n\".join(all_files_content)\n",
    "            df = pd.DataFrame([concatenated_content], columns=['SourceCode'])\n",
    "            df.to_csv(os.path.join(output_dir, f'{repo_name}_context.csv'), index=False)\n",
    "            print(f'Saved {repo_name}_context.csv')\n",
    "        else:\n",
    "            print(f'Failed to fetch repository data for {repo_name}: {response.status_code}')\n",
    "\n",
    "# Example usage:\n",
    "repo_urls = [\n",
    "    \"https://github.com/context-labs/autodoc\"\n",
    "]\n",
    "output_directory = 'output_csv_files'\n",
    "github_token = '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'  # Replace with your GitHub access token\n",
    "\n",
    "fetch_and_concatenate_source_code(repo_urls, output_directory, github_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e78d0b1-b921-468b-8376-d3651bd2d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, quote\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "# Clone repository to a local path\n",
    "def git_clone(repo_url, clone_path):\n",
    "    if os.path.exists(clone_path):\n",
    "        subprocess.run(['rm', '-rf', clone_path], check=True)\n",
    "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)\n",
    "\n",
    "# Parse the README.md content into a CSV\n",
    "def parse_markdown_to_csv(md_file_path, csv_file_path):\n",
    "    with open(md_file_path, 'r', encoding='utf-8') as file:\n",
    "        md_content = file.read()\n",
    "\n",
    "    heading_pattern = re.compile(r'^(#+)\\s*(.*)', re.MULTILINE)\n",
    "    headings_contents = []\n",
    "    current_heading = None\n",
    "    current_content = []\n",
    "\n",
    "    for line in md_content.split('\\n'):\n",
    "        match = heading_pattern.match(line)\n",
    "        if match:\n",
    "            if current_heading is not None:\n",
    "                headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "            current_heading = match.group(2).strip()\n",
    "            current_content = []\n",
    "        else:\n",
    "            if line.strip():\n",
    "                current_content.append(line.strip())\n",
    "\n",
    "    if current_heading is not None:\n",
    "        headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "\n",
    "    df = pd.DataFrame(headings_contents, columns=['Title', 'Content'])\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Process a list of GitHub repository URLs\n",
    "def process_repos(repo_urls, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for url in repo_urls:\n",
    "        parsed_url = urlparse(url)\n",
    "        parts = parsed_url.path.strip('/').split('/')\n",
    "        repo_user, repo_name = parts[0], parts[1]\n",
    "        clone_path = f\"/tmp/{repo_name}\"  # Temporary path for cloning\n",
    "        git_clone(url, clone_path)\n",
    "\n",
    "        readme_path = os.path.join(clone_path, 'README.md')\n",
    "        csv_file_path = os.path.join(output_dir, f\"{repo_name}.csv\")\n",
    "        if os.path.exists(readme_path):\n",
    "            parse_markdown_to_csv(readme_path, csv_file_path)\n",
    "            print(f\"Processed {repo_name}.csv\")\n",
    "        else:\n",
    "            print(f\"README.md not found for {repo_name}\")\n",
    "\n",
    "        # Remove the repository directory to clean up\n",
    "        subprocess.run(['rm', '-rf', clone_path], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f50bec-95dd-4f3d-85da-6d60768cea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this list with your own list of 300 URLs\n",
    "#repo_urls = ['https://github.com/public-apis/public-apis', 'https://github.com/donnemartin/system-design-primer', 'https://github.com/vinta/awesome-python', 'https://github.com/TheAlgorithms/Python', 'https://github.com/jackfrued/Python-100-Days', 'https://github.com/AUTOMATIC1111/stable-diffusion-webui', 'https://github.com/ytdl-org/youtube-dl', 'https://github.com/huggingface/transformers', 'https://github.com/521xueweihan/HelloGitHub', 'https://github.com/langchain-ai/langchain', 'https://github.com/nvbn/thefuck', 'https://github.com/pytorch/pytorch', 'https://github.com/django/django', 'https://github.com/tensorflow/models', 'https://github.com/yt-dlp/yt-dlp', 'https://github.com/tiangolo/fastapi', 'https://github.com/home-assistant/core', 'https://github.com/pallets/flask', 'https://github.com/fighting41love/funNLP', 'https://github.com/bregman-arie/devops-exercises', 'https://github.com/josephmisiti/awesome-machine-learning', 'https://github.com/ansible/ansible', 'https://github.com/keras-team/keras', 'https://github.com/openai/whisper', 'https://github.com/python/cpython', 'https://github.com/3b1b/manim', 'https://github.com/scikit-learn/scikit-learn', 'https://github.com/xtekky/gpt4free', 'https://github.com/binary-husky/gpt_academic', 'https://github.com/d2l-ai/d2l-zh', 'https://github.com/swisskyrepo/PayloadsAllTheThings', 'https://github.com/meta-llama/llama', 'https://github.com/localstack/localstack', 'https://github.com/zylon-ai/private-gpt', 'https://github.com/ageitgey/face_recognition', 'https://github.com/sherlock-project/sherlock', 'https://github.com/psf/requests', 'https://github.com/scrapy/scrapy', 'https://github.com/CorentinJ/Real-Time-Voice-Cloning', 'https://github.com/gpt-engineer-org/gpt-engineer', 'https://github.com/abi/screenshot-to-code', 'https://github.com/deepfakes/faceswap', 'https://github.com/soimort/you-get', 'https://github.com/OpenInterpreter/open-interpreter', 'https://github.com/xai-org/grok-1', 'https://github.com/commaai/openpilot', 'https://github.com/Textualize/rich', 'https://github.com/ultralytics/yolov5', 'https://github.com/minimaxir/big-list-of-naughty-strings', 'https://github.com/iperov/DeepFaceLab', 'https://github.com/charlax/professional-programming', 'https://github.com/Z4nzu/hackingtool', 'https://github.com/pandas-dev/pandas', 'https://github.com/isocpp/CppCoreGuidelines', 'https://github.com/geekan/MetaGPT', 'https://github.com/faif/python-patterns', 'https://github.com/THUDM/ChatGLM-6B', 'https://github.com/PaddlePaddle/PaddleOCR', 'https://github.com/apachecn/ailearning', 'https://github.com/hpcaitech/ColossalAI', 'https://github.com/chubin/cheat.sh', 'https://github.com/psf/black', 'https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap', 'https://github.com/google-research/bert', 'https://github.com/getsentry/sentry', 'https://github.com/oobabooga/text-generation-webui', 'https://github.com/LAION-AI/Open-Assistant', 'https://github.com/Stability-AI/stablediffusion', 'https://github.com/0voice/interview_internal_reference', 'https://github.com/gto76/python-cheatsheet', 'https://github.com/lllyasviel/Fooocus', 'https://github.com/XingangPan/DragGAN', 'https://github.com/satwikkansal/wtfpython', 'https://github.com/mingrammer/diagrams', 'https://github.com/odoo/odoo', 'https://github.com/TencentARC/GFPGAN', 'https://github.com/apache/airflow', 'https://github.com/chenfei-wu/TaskMatrix', 'https://github.com/mitmproxy/mitmproxy', 'https://github.com/lm-sys/FastChat', 'https://github.com/comfyanonymous/ComfyUI', 'https://github.com/babysor/MockingBird', 'https://github.com/openai/gym', 'https://github.com/testerSunshine/12306', 'https://github.com/shadowsocks/shadowsocks', 'https://github.com/microsoft/DeepSpeed', 'https://github.com/XX-net/XX-Net', 'https://github.com/fxsjy/jieba', 'https://github.com/hankcs/HanLP', 'https://github.com/Asabeneh/30-Days-Of-Python', 'https://github.com/karpathy/nanoGPT', 'https://github.com/httpie/cli', 'https://github.com/streamlit/streamlit', 'https://github.com/ccxt/ccxt', 'https://github.com/run-llama/llama_index', 'https://github.com/ray-project/ray', 'https://github.com/certbot/certbot', 'https://github.com/sqlmapproject/sqlmap', 'https://github.com/geekcomputers/Python', 'https://github.com/huggingface/pytorch-image-models', 'https://github.com/coqui-ai/TTS', 'https://github.com/python-poetry/poetry', 'https://github.com/0xAX/linux-insides', 'https://github.com/facebookresearch/fairseq', 'https://github.com/gradio-app/gradio', 'https://github.com/yunjey/pytorch-tutorial', 'https://github.com/tatsu-lab/stanford_alpaca', 'https://github.com/explosion/spaCy', 'https://github.com/donnemartin/interactive-coding-challenges', 'https://github.com/facebookresearch/detectron2', 'https://github.com/Pythagora-io/gpt-pilot', 'https://github.com/google/jax', 'https://github.com/lllyasviel/ControlNet', 'https://github.com/acheong08/ChatGPT', 'https://github.com/open-mmlab/mmdetection', 'https://github.com/chatchat-space/Langchain-Chatchat', 'https://github.com/encode/django-rest-framework', 'https://github.com/tqdm/tqdm', 'https://github.com/Lightning-AI/pytorch-lightning', 'https://github.com/LC044/WeChatMsg', 'https://github.com/OWASP/CheatSheetSeries', 'https://github.com/donnemartin/data-science-ipython-notebooks', 'https://github.com/numpy/numpy', 'https://github.com/google/python-fire', 'https://github.com/xinntao/Real-ESRGAN', 'https://github.com/OpenBB-finance/OpenBBTerminal', 'https://github.com/facebookresearch/Detectron', 'https://github.com/freqtrade/freqtrade', 'https://github.com/StevenBlack/hosts', 'https://github.com/ycm-core/YouCompleteMe', 'https://github.com/spipm/Depix', 'https://github.com/zhayujie/chatgpt-on-wechat', 'https://github.com/littlecodersh/ItChat', 'https://github.com/nicolargo/glances', 'https://github.com/s0md3v/roop', 'https://github.com/getredash/redash', 'https://github.com/deezer/spleeter', 'https://github.com/Vision-CAIR/MiniGPT-4', 'https://github.com/python-telegram-bot/python-telegram-bot', 'https://github.com/pypa/pipenv', 'https://github.com/myshell-ai/OpenVoice', 'https://github.com/OpenDevin/OpenDevin', 'https://github.com/microsoft/cascadia-code', 'https://github.com/matterport/Mask_RCNN', 'https://github.com/tinygrad/tinygrad', 'https://github.com/svc-develop-team/so-vits-svc', 'https://github.com/RVC-Boss/GPT-SoVITS', 'https://github.com/jumpserver/jumpserver', 'https://github.com/locustio/locust', 'https://github.com/chubin/wttr.in', 'https://github.com/Textualize/textual', 'https://github.com/celery/celery', 'https://github.com/keon/algorithms', 'https://github.com/vnpy/vnpy', 'https://github.com/iperov/DeepFaceLive', 'https://github.com/ultralytics/ultralytics', 'https://github.com/eriklindernoren/ML-From-Scratch', 'https://github.com/microsoft/JARVIS', 'https://github.com/huggingface/diffusers', 'https://github.com/wangzheng0822/algo', 'https://github.com/mouredev/Hello-Python', 'https://github.com/Stability-AI/generative-models', 'https://github.com/sebastianruder/NLP-progress', 'https://github.com/JaidedAI/EasyOCR', 'https://github.com/kovidgoyal/kitty', 'https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix', 'https://github.com/HumanSignal/labelImg', 'https://github.com/d2l-ai/d2l-en', 'https://github.com/AtsushiSakai/PythonRobotics', 'https://github.com/pytorch/examples', 'https://github.com/cookiecutter/cookiecutter', 'https://github.com/tornadoweb/tornado', 'https://github.com/hiyouga/LLaMA-Factory', 'https://github.com/mindsdb/mindsdb', 'https://github.com/deepinsight/insightface', 'https://github.com/openai/gpt-2', 'https://github.com/luong-komorebi/Awesome-Linux-Software', 'https://github.com/WZMIAOMIAO/deep-learning-for-image-processing', 'https://github.com/drduh/macOS-Security-and-Privacy-Guide', 'https://github.com/openai/chatgpt-retrieval-plugin', 'https://github.com/plotly/dash', 'https://github.com/chriskiehl/Gooey', 'https://github.com/jhao104/proxy_pool', 'https://github.com/pyg-team/pytorch_geometric', 'https://github.com/saleor/saleor', 'https://github.com/zulip/zulip', 'https://github.com/jina-ai/jina', 'https://github.com/openai/openai-python', 'https://github.com/KurtBestor/Hitomi-Downloader', 'https://github.com/521xueweihan/GitHub520', 'https://github.com/ArchiveBox/ArchiveBox', 'https://github.com/facebookresearch/audiocraft', 'https://github.com/meta-llama/llama3', 'https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI', 'https://github.com/matplotlib/matplotlib', 'https://github.com/yoheinakajima/babyagi', 'https://github.com/Vonng/ddia', 'https://github.com/PromtEngineer/localGPT', 'https://github.com/vllm-project/vllm', 'https://github.com/ManimCommunity/manim', 'https://github.com/ungoogled-software/ungoogled-chromium', 'https://github.com/karpathy/minGPT', 'https://github.com/magenta/magenta', 'https://github.com/bokeh/bokeh', 'https://github.com/pydantic/pydantic', 'https://github.com/huggingface/datasets', 'https://github.com/microsoft/unilm', 'https://github.com/kholia/OSX-KVM', 'https://github.com/kovidgoyal/calibre', 'https://github.com/mkdocs/mkdocs', 'https://github.com/magic-wormhole/magic-wormhole', 'https://github.com/Delgan/loguru', 'https://github.com/lucidrains/vit-pytorch', 'https://github.com/nginx-proxy/nginx-proxy', 'https://github.com/recommenders-team/recommenders', 'https://github.com/RasaHQ/rasa', 'https://github.com/facebook/prophet', 'https://github.com/sanic-org/sanic', 'https://github.com/kaixindelele/ChatPaper', 'https://github.com/Jack-Cherish/python-spider', 'https://github.com/jantic/DeOldify', 'https://github.com/python/mypy', 'https://github.com/ymcui/Chinese-LLaMA-Alpaca', 'https://github.com/pyscript/pyscript', 'https://github.com/PostHog/posthog', 'https://github.com/mlflow/mlflow', 'https://github.com/spotify/luigi', 'https://github.com/wagtail/wagtail', 'https://github.com/Sanster/IOPaint', 'https://github.com/miloyip/game-programmer', 'https://github.com/joke2k/faker', 'https://github.com/mlc-ai/mlc-llm', 'https://github.com/Ciphey/Ciphey', 'https://github.com/quantopian/zipline', 'https://github.com/paperless-ngx/paperless-ngx', 'https://github.com/frappe/erpnext', 'https://github.com/stitionai/devika', 'https://github.com/rsms/inter', 'https://github.com/kivy/kivy', 'https://github.com/reflex-dev/reflex', 'https://github.com/onnx/onnx', 'https://github.com/reddit-archive/reddit', 'https://github.com/hpcaitech/Open-Sora', 'https://github.com/haotian-liu/LLaVA', 'https://github.com/chatanywhere/GPT_API_free', 'https://github.com/InstaPy/InstaPy', 'https://github.com/binux/pyspider', 'https://github.com/LiLittleCat/awesome-free-chatgpt', 'https://github.com/cool-RR/PySnooper', 'https://github.com/apple/ml-stable-diffusion', 'https://github.com/ipython/ipython', 'https://github.com/wilsonfreitas/awesome-quant', 'https://github.com/alievk/avatarify-python', 'https://github.com/Mikubill/sd-webui-controlnet', 'https://github.com/wting/autojump', 'https://github.com/trekhleb/learn-python', 'https://github.com/eriklindernoren/PyTorch-GAN', 'https://github.com/Kr1s77/awesome-python-login-model', 'https://github.com/twintproject/twint', 'https://github.com/THUDM/ChatGLM2-6B', 'https://github.com/wistbean/learn_python3_spider', 'https://github.com/mnielsen/neural-networks-and-deep-learning', 'https://github.com/pytorch/vision', 'https://github.com/h2y/Shadowrocket-ADBlock-Rules', 'https://github.com/OpenEthan/SMSBoom', 'https://github.com/openai/baselines', 'https://github.com/plotly/plotly.py', 'https://github.com/piskvorky/gensim', 'https://github.com/RunaCapital/awesome-oss-alternatives', 'https://github.com/meta-llama/codellama', 'https://github.com/pallets/click', 'https://github.com/spotDL/spotify-downloader', 'https://github.com/dgtlmoon/changedetection.io', 'https://github.com/Anjok07/ultimatevocalremovergui', 'https://github.com/netbox-community/netbox', 'https://github.com/mxrch/GHunt', 'https://github.com/ranger/ranger', 'https://github.com/tensorflow/tensor2tensor', 'https://github.com/aws/aws-cli', 'https://github.com/blakeblackshear/frigate', 'https://github.com/w-okada/voice-changer', 'https://github.com/GaiZhenbiao/ChuanhuChatGPT', 'https://github.com/PrefectHQ/prefect', 'https://github.com/jupyter/jupyter', 'https://github.com/facefusion/facefusion', 'https://github.com/danielgatis/rembg', 'https://github.com/borisdayma/dalle-mini', 'https://github.com/fabric/fabric', 'https://github.com/aio-libs/aiohttp', 'https://github.com/ddbourgin/numpy-ml', 'https://github.com/TransformerOptimus/SuperAGI', 'https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life', 'https://github.com/pyecharts/pyecharts', 'https://github.com/tiangolo/typer', 'https://github.com/Rapptz/discord.py', 'https://github.com/fauxpilot/fauxpilot', 'https://github.com/lra/mackup', 'https://github.com/apprenticeharper/DeDRM_tools', 'https://github.com/microsoft/qlib', 'https://github.com/networkx/networkx', 'https://github.com/powerline/powerline', 'https://github.com/arc53/DocsGPT', 'https://github.com/Python-World/python-mini-projects', 'https://github.com/airbytehq/airbyte', 'https://github.com/aleju/imgaug', 'https://github.com/roboflow/supervision', 'https://github.com/pjialin/py12306', 'https://github.com/hindupuravinash/the-gan-zoo', 'https://github.com/unifyai/ivy', 'https://github.com/openai/evals', 'https://github.com/horovod/horovod', 'https://github.com/huggingface/peft', 'https://github.com/NVlabs/stylegan', 'https://github.com/tgbot-collection/YYeTsBot', 'https://github.com/gunthercox/ChatterBot', 'https://github.com/UKPLab/sentence-transformers', 'https://github.com/saltstack/salt', 'https://github.com/wangshub/wechat_jump_game', 'https://github.com/youfou/wxpy', 'https://github.com/microsoft/nni', 'https://github.com/deepset-ai/haystack', 'https://github.com/codelucas/newspaper', 'https://github.com/joaomdmoura/crewAI', 'https://github.com/google/yapf', 'https://github.com/psf/requests-html', 'https://github.com/flairNLP/flair', 'https://github.com/sczhou/CodeFormer', 'https://github.com/shengqiangzhang/examples-of-web-crawlers', 'https://github.com/davidsandberg/facenet', 'https://github.com/NanmiCoder/MediaCrawler', 'https://github.com/ansible/awx', 'https://github.com/albumentations-team/albumentations', 'https://github.com/programthink/zhao', 'https://github.com/mail-in-a-box/mailinabox', 'https://github.com/sivel/speedtest-cli', 'https://github.com/searx/searx', 'https://github.com/ShangtongZhang/reinforcement-learning-an-introduction', 'https://github.com/iterative/dvc', 'https://github.com/PySimpleGUI/PySimpleGUI', 'https://github.com/mementum/backtrader', 'https://github.com/tiangolo/sqlmodel', 'https://github.com/nltk/nltk', 'https://github.com/dmlc/dgl', 'https://github.com/microsoft/Swin-Transformer', 'https://github.com/jindongwang/transferlearning', 'https://github.com/facebookresearch/detr', 'https://github.com/idank/explainshell', 'https://github.com/s0md3v/XSStrike', 'https://github.com/fortra/impacket', 'https://github.com/MetaCubeX/mihomo', 'https://github.com/wifiphisher/wifiphisher', 'https://github.com/jaakkopasanen/AutoEq', 'https://github.com/waditu/tushare', 'https://github.com/edgedb/edgedb', 'https://github.com/bloomberg/memray', 'https://github.com/ethereum/EIPs', 'https://github.com/PaddlePaddle/PaddleHub', 'https://github.com/scipy/scipy', 'https://github.com/chroma-core/chroma', 'https://github.com/sympy/sympy', 'https://github.com/beetbox/beets', 'https://github.com/postmanlabs/httpbin', 'https://github.com/labelmeai/labelme', 'https://github.com/SFTtech/openage', 'https://github.com/encode/httpx', 'https://github.com/redis/redis-py', 'https://github.com/getpelican/pelican', 'https://github.com/THUDM/ChatGLM3', 'https://github.com/jina-ai/clip-as-service', 'https://github.com/donnemartin/awesome-aws', 'https://github.com/microsoft/pyright', 'https://github.com/pre-commit/pre-commit', 'https://github.com/PaddlePaddle/PaddleDetection', 'https://github.com/ocrmypdf/OCRmyPDF', 'https://github.com/lss233/chatgpt-mirai-qq-bot', 'https://github.com/ydataai/ydata-profiling', 'https://github.com/dask/dask', 'https://github.com/mwaskom/seaborn', 'https://github.com/ranaroussi/yfinance', 'https://github.com/tonybeltramelli/pix2code', 'https://github.com/threat9/routersploit', 'https://github.com/Miserlou/Zappa', 'https://github.com/alexjc/neural-enhance', 'https://github.com/Zulko/moviepy', 'https://github.com/meolu/walle-web', 'https://github.com/OpenMOSS/MOSS', 'https://github.com/smicallef/spiderfoot', 'https://github.com/matrix-org/synapse', 'https://github.com/google-deepmind/alphafold', 'https://github.com/dbcli/pgcli', 'https://github.com/python-pillow/Pillow', 'https://github.com/BlinkDL/RWKV-LM', 'https://github.com/allenai/allennlp', 'https://github.com/LlamaFamily/Llama-Chinese', 'https://github.com/smol-ai/developer', 'https://github.com/janeczku/calibre-web', 'https://github.com/Embedding/Chinese-Word-Vectors', 'https://github.com/cookiecutter/cookiecutter-django', 'https://github.com/rougier/numpy-100', 'https://github.com/zalandoresearch/fashion-mnist']\n",
    "repo_urls = ['https://github.com/pyscript/pyscript', 'https://github.com/PostHog/posthog', 'https://github.com/mlflow/mlflow', 'https://github.com/spotify/luigi', 'https://github.com/wagtail/wagtail', 'https://github.com/Sanster/IOPaint', 'https://github.com/miloyip/game-programmer', 'https://github.com/joke2k/faker', 'https://github.com/mlc-ai/mlc-llm', 'https://github.com/Ciphey/Ciphey', 'https://github.com/quantopian/zipline', 'https://github.com/paperless-ngx/paperless-ngx', 'https://github.com/frappe/erpnext', 'https://github.com/stitionai/devika', 'https://github.com/rsms/inter', 'https://github.com/kivy/kivy', 'https://github.com/reflex-dev/reflex', 'https://github.com/onnx/onnx', 'https://github.com/reddit-archive/reddit', 'https://github.com/hpcaitech/Open-Sora', 'https://github.com/haotian-liu/LLaVA', 'https://github.com/chatanywhere/GPT_API_free', 'https://github.com/InstaPy/InstaPy', 'https://github.com/binux/pyspider', 'https://github.com/LiLittleCat/awesome-free-chatgpt', 'https://github.com/cool-RR/PySnooper', 'https://github.com/apple/ml-stable-diffusion', 'https://github.com/ipython/ipython', 'https://github.com/wilsonfreitas/awesome-quant', 'https://github.com/alievk/avatarify-python', 'https://github.com/Mikubill/sd-webui-controlnet', 'https://github.com/wting/autojump', 'https://github.com/trekhleb/learn-python', 'https://github.com/eriklindernoren/PyTorch-GAN', 'https://github.com/Kr1s77/awesome-python-login-model', 'https://github.com/twintproject/twint', 'https://github.com/THUDM/ChatGLM2-6B', 'https://github.com/wistbean/learn_python3_spider', 'https://github.com/mnielsen/neural-networks-and-deep-learning', 'https://github.com/pytorch/vision', 'https://github.com/h2y/Shadowrocket-ADBlock-Rules', 'https://github.com/OpenEthan/SMSBoom', 'https://github.com/openai/baselines', 'https://github.com/plotly/plotly.py', 'https://github.com/piskvorky/gensim', 'https://github.com/RunaCapital/awesome-oss-alternatives', 'https://github.com/meta-llama/codellama', 'https://github.com/pallets/click', 'https://github.com/spotDL/spotify-downloader', 'https://github.com/dgtlmoon/changedetection.io', 'https://github.com/Anjok07/ultimatevocalremovergui', 'https://github.com/netbox-community/netbox', 'https://github.com/mxrch/GHunt', 'https://github.com/ranger/ranger', 'https://github.com/tensorflow/tensor2tensor', 'https://github.com/aws/aws-cli', 'https://github.com/blakeblackshear/frigate', 'https://github.com/w-okada/voice-changer', 'https://github.com/GaiZhenbiao/ChuanhuChatGPT', 'https://github.com/PrefectHQ/prefect', 'https://github.com/jupyter/jupyter', 'https://github.com/facefusion/facefusion', 'https://github.com/danielgatis/rembg', 'https://github.com/borisdayma/dalle-mini', 'https://github.com/fabric/fabric', 'https://github.com/aio-libs/aiohttp', 'https://github.com/ddbourgin/numpy-ml', 'https://github.com/TransformerOptimus/SuperAGI', 'https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life', 'https://github.com/pyecharts/pyecharts', 'https://github.com/tiangolo/typer', 'https://github.com/Rapptz/discord.py', 'https://github.com/fauxpilot/fauxpilot', 'https://github.com/lra/mackup', 'https://github.com/apprenticeharper/DeDRM_tools', 'https://github.com/microsoft/qlib', 'https://github.com/networkx/networkx', 'https://github.com/powerline/powerline', 'https://github.com/arc53/DocsGPT', 'https://github.com/Python-World/python-mini-projects', 'https://github.com/airbytehq/airbyte', 'https://github.com/aleju/imgaug', 'https://github.com/roboflow/supervision', 'https://github.com/pjialin/py12306', 'https://github.com/hindupuravinash/the-gan-zoo', 'https://github.com/unifyai/ivy', 'https://github.com/openai/evals', 'https://github.com/horovod/horovod', 'https://github.com/huggingface/peft', 'https://github.com/NVlabs/stylegan', 'https://github.com/tgbot-collection/YYeTsBot', 'https://github.com/gunthercox/ChatterBot', 'https://github.com/UKPLab/sentence-transformers', 'https://github.com/saltstack/salt', 'https://github.com/wangshub/wechat_jump_game', 'https://github.com/youfou/wxpy', 'https://github.com/microsoft/nni', 'https://github.com/deepset-ai/haystack', 'https://github.com/codelucas/newspaper', 'https://github.com/joaomdmoura/crewAI', 'https://github.com/google/yapf', 'https://github.com/psf/requests-html', 'https://github.com/flairNLP/flair', 'https://github.com/sczhou/CodeFormer', 'https://github.com/shengqiangzhang/examples-of-web-crawlers', 'https://github.com/davidsandberg/facenet', 'https://github.com/NanmiCoder/MediaCrawler', 'https://github.com/ansible/awx', 'https://github.com/albumentations-team/albumentations', 'https://github.com/programthink/zhao', 'https://github.com/mail-in-a-box/mailinabox', 'https://github.com/sivel/speedtest-cli', 'https://github.com/searx/searx', 'https://github.com/ShangtongZhang/reinforcement-learning-an-introduction', 'https://github.com/iterative/dvc', 'https://github.com/PySimpleGUI/PySimpleGUI', 'https://github.com/mementum/backtrader', 'https://github.com/tiangolo/sqlmodel', 'https://github.com/nltk/nltk', 'https://github.com/dmlc/dgl', 'https://github.com/microsoft/Swin-Transformer', 'https://github.com/jindongwang/transferlearning', 'https://github.com/facebookresearch/detr', 'https://github.com/idank/explainshell', 'https://github.com/s0md3v/XSStrike', 'https://github.com/fortra/impacket', 'https://github.com/MetaCubeX/mihomo', 'https://github.com/wifiphisher/wifiphisher', 'https://github.com/jaakkopasanen/AutoEq', 'https://github.com/waditu/tushare', 'https://github.com/edgedb/edgedb', 'https://github.com/bloomberg/memray', 'https://github.com/ethereum/EIPs', 'https://github.com/PaddlePaddle/PaddleHub', 'https://github.com/scipy/scipy', 'https://github.com/chroma-core/chroma', 'https://github.com/sympy/sympy', 'https://github.com/beetbox/beets', 'https://github.com/postmanlabs/httpbin', 'https://github.com/labelmeai/labelme', 'https://github.com/SFTtech/openage', 'https://github.com/encode/httpx', 'https://github.com/redis/redis-py', 'https://github.com/getpelican/pelican', 'https://github.com/THUDM/ChatGLM3', 'https://github.com/jina-ai/clip-as-service', 'https://github.com/donnemartin/awesome-aws', 'https://github.com/microsoft/pyright', 'https://github.com/pre-commit/pre-commit', 'https://github.com/PaddlePaddle/PaddleDetection', 'https://github.com/ocrmypdf/OCRmyPDF', 'https://github.com/lss233/chatgpt-mirai-qq-bot', 'https://github.com/ydataai/ydata-profiling', 'https://github.com/dask/dask', 'https://github.com/mwaskom/seaborn', 'https://github.com/ranaroussi/yfinance', 'https://github.com/tonybeltramelli/pix2code', 'https://github.com/threat9/routersploit', 'https://github.com/Miserlou/Zappa', 'https://github.com/alexjc/neural-enhance', 'https://github.com/Zulko/moviepy', 'https://github.com/meolu/walle-web', 'https://github.com/OpenMOSS/MOSS', 'https://github.com/smicallef/spiderfoot', 'https://github.com/matrix-org/synapse', 'https://github.com/google-deepmind/alphafold', 'https://github.com/dbcli/pgcli', 'https://github.com/python-pillow/Pillow', 'https://github.com/BlinkDL/RWKV-LM', 'https://github.com/allenai/allennlp', 'https://github.com/LlamaFamily/Llama-Chinese', 'https://github.com/smol-ai/developer', 'https://github.com/janeczku/calibre-web', 'https://github.com/Embedding/Chinese-Word-Vectors', 'https://github.com/cookiecutter/cookiecutter-django', 'https://github.com/rougier/numpy-100', 'https://github.com/zalandoresearch/fashion-mnist']\n",
    "output_directory = 'output_csv_files'\n",
    "process_repos(repo_urls, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f96d8c-589b-446b-bceb-63f4fa4b9498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f62fd8-1e33-4250-8c9a-f14d5d9cdbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clone a GitHub repository and collect all source code into a single string\n",
    "def collect_source_code(repo_url):\n",
    "    # Extract the repo name from the URL\n",
    "    repo_name = repo_url.rstrip('/').split('/')[-1]\n",
    "    subprocess.run(['git', 'clone', repo_url], check=True)\n",
    "    \n",
    "    # Collect all source code files into a single string\n",
    "    source_code = []\n",
    "    for root, dirs, files in os.walk(repo_name):\n",
    "        for file in files:\n",
    "            # Filter for source code files only (adjust filters as needed)\n",
    "            if file.endswith(('.py', '.js', '.java', '.cpp', '.c', '.h', '.html', '.css', '.ts', '.go', '.rb', '.php')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', errors='ignore') as f:\n",
    "                    source_code.append(f.read())\n",
    "                    \n",
    "    # Join all source code files as one big string\n",
    "    concatenated_code = \"\\n\".join(source_code)\n",
    "    \n",
    "    # Delete the repo after extraction\n",
    "    shutil.rmtree(repo_name)\n",
    "    \n",
    "    return repo_name, concatenated_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38ebfa87-d349-4d5b-a298-936ee1ff116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this list with your own list of 300 URLs\n",
    "#github_urls = ['https://github.com/public-apis/public-apis', 'https://github.com/donnemartin/system-design-primer', 'https://github.com/vinta/awesome-python', 'https://github.com/TheAlgorithms/Python', 'https://github.com/jackfrued/Python-100-Days', 'https://github.com/AUTOMATIC1111/stable-diffusion-webui', 'https://github.com/ytdl-org/youtube-dl', 'https://github.com/huggingface/transformers', 'https://github.com/521xueweihan/HelloGitHub', 'https://github.com/langchain-ai/langchain', 'https://github.com/nvbn/thefuck', 'https://github.com/pytorch/pytorch', 'https://github.com/django/django', 'https://github.com/tensorflow/models', 'https://github.com/yt-dlp/yt-dlp', 'https://github.com/tiangolo/fastapi', 'https://github.com/home-assistant/core', 'https://github.com/pallets/flask', 'https://github.com/fighting41love/funNLP', 'https://github.com/bregman-arie/devops-exercises', 'https://github.com/josephmisiti/awesome-machine-learning', 'https://github.com/ansible/ansible', 'https://github.com/keras-team/keras', 'https://github.com/openai/whisper', 'https://github.com/python/cpython', 'https://github.com/3b1b/manim', 'https://github.com/scikit-learn/scikit-learn', 'https://github.com/xtekky/gpt4free', 'https://github.com/binary-husky/gpt_academic', 'https://github.com/d2l-ai/d2l-zh', 'https://github.com/swisskyrepo/PayloadsAllTheThings', 'https://github.com/meta-llama/llama', 'https://github.com/localstack/localstack', 'https://github.com/zylon-ai/private-gpt', 'https://github.com/ageitgey/face_recognition', 'https://github.com/sherlock-project/sherlock', 'https://github.com/psf/requests', 'https://github.com/scrapy/scrapy', 'https://github.com/CorentinJ/Real-Time-Voice-Cloning', 'https://github.com/gpt-engineer-org/gpt-engineer', 'https://github.com/abi/screenshot-to-code', 'https://github.com/deepfakes/faceswap', 'https://github.com/soimort/you-get', 'https://github.com/OpenInterpreter/open-interpreter', 'https://github.com/xai-org/grok-1', 'https://github.com/commaai/openpilot', 'https://github.com/Textualize/rich', 'https://github.com/ultralytics/yolov5', 'https://github.com/minimaxir/big-list-of-naughty-strings', 'https://github.com/iperov/DeepFaceLab', 'https://github.com/charlax/professional-programming', 'https://github.com/Z4nzu/hackingtool', 'https://github.com/pandas-dev/pandas', 'https://github.com/isocpp/CppCoreGuidelines', 'https://github.com/geekan/MetaGPT', 'https://github.com/faif/python-patterns', 'https://github.com/THUDM/ChatGLM-6B', 'https://github.com/PaddlePaddle/PaddleOCR', 'https://github.com/apachecn/ailearning', 'https://github.com/hpcaitech/ColossalAI', 'https://github.com/chubin/cheat.sh', 'https://github.com/psf/black', 'https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap', 'https://github.com/google-research/bert', 'https://github.com/getsentry/sentry', 'https://github.com/oobabooga/text-generation-webui', 'https://github.com/LAION-AI/Open-Assistant', 'https://github.com/Stability-AI/stablediffusion', 'https://github.com/0voice/interview_internal_reference', 'https://github.com/gto76/python-cheatsheet', 'https://github.com/lllyasviel/Fooocus', 'https://github.com/XingangPan/DragGAN', 'https://github.com/satwikkansal/wtfpython', 'https://github.com/mingrammer/diagrams', 'https://github.com/odoo/odoo', 'https://github.com/TencentARC/GFPGAN', 'https://github.com/apache/airflow', 'https://github.com/chenfei-wu/TaskMatrix', 'https://github.com/mitmproxy/mitmproxy', 'https://github.com/lm-sys/FastChat', 'https://github.com/comfyanonymous/ComfyUI', 'https://github.com/babysor/MockingBird', 'https://github.com/openai/gym', 'https://github.com/testerSunshine/12306', 'https://github.com/shadowsocks/shadowsocks', 'https://github.com/microsoft/DeepSpeed', 'https://github.com/XX-net/XX-Net', 'https://github.com/fxsjy/jieba', 'https://github.com/hankcs/HanLP', 'https://github.com/Asabeneh/30-Days-Of-Python', 'https://github.com/karpathy/nanoGPT', 'https://github.com/httpie/cli', 'https://github.com/streamlit/streamlit', 'https://github.com/ccxt/ccxt', 'https://github.com/run-llama/llama_index', 'https://github.com/ray-project/ray', 'https://github.com/certbot/certbot', 'https://github.com/sqlmapproject/sqlmap', 'https://github.com/geekcomputers/Python', 'https://github.com/huggingface/pytorch-image-models', 'https://github.com/coqui-ai/TTS', 'https://github.com/python-poetry/poetry', 'https://github.com/0xAX/linux-insides', 'https://github.com/facebookresearch/fairseq', 'https://github.com/gradio-app/gradio', 'https://github.com/yunjey/pytorch-tutorial', 'https://github.com/tatsu-lab/stanford_alpaca', 'https://github.com/explosion/spaCy', 'https://github.com/donnemartin/interactive-coding-challenges', 'https://github.com/facebookresearch/detectron2', 'https://github.com/Pythagora-io/gpt-pilot', 'https://github.com/google/jax', 'https://github.com/lllyasviel/ControlNet', 'https://github.com/acheong08/ChatGPT', 'https://github.com/open-mmlab/mmdetection', 'https://github.com/chatchat-space/Langchain-Chatchat', 'https://github.com/encode/django-rest-framework', 'https://github.com/tqdm/tqdm', 'https://github.com/Lightning-AI/pytorch-lightning', 'https://github.com/LC044/WeChatMsg', 'https://github.com/OWASP/CheatSheetSeries', 'https://github.com/donnemartin/data-science-ipython-notebooks', 'https://github.com/numpy/numpy', 'https://github.com/google/python-fire', 'https://github.com/xinntao/Real-ESRGAN', 'https://github.com/OpenBB-finance/OpenBBTerminal', 'https://github.com/facebookresearch/Detectron', 'https://github.com/freqtrade/freqtrade', 'https://github.com/StevenBlack/hosts', 'https://github.com/ycm-core/YouCompleteMe', 'https://github.com/spipm/Depix', 'https://github.com/zhayujie/chatgpt-on-wechat', 'https://github.com/littlecodersh/ItChat', 'https://github.com/nicolargo/glances', 'https://github.com/s0md3v/roop', 'https://github.com/getredash/redash', 'https://github.com/deezer/spleeter', 'https://github.com/Vision-CAIR/MiniGPT-4', 'https://github.com/python-telegram-bot/python-telegram-bot', 'https://github.com/pypa/pipenv', 'https://github.com/myshell-ai/OpenVoice', 'https://github.com/OpenDevin/OpenDevin', 'https://github.com/microsoft/cascadia-code', 'https://github.com/matterport/Mask_RCNN', 'https://github.com/tinygrad/tinygrad', 'https://github.com/svc-develop-team/so-vits-svc', 'https://github.com/RVC-Boss/GPT-SoVITS', 'https://github.com/jumpserver/jumpserver', 'https://github.com/locustio/locust', 'https://github.com/chubin/wttr.in', 'https://github.com/Textualize/textual', 'https://github.com/celery/celery', 'https://github.com/keon/algorithms', 'https://github.com/vnpy/vnpy', 'https://github.com/iperov/DeepFaceLive', 'https://github.com/ultralytics/ultralytics', 'https://github.com/eriklindernoren/ML-From-Scratch', 'https://github.com/microsoft/JARVIS', 'https://github.com/huggingface/diffusers', 'https://github.com/wangzheng0822/algo', 'https://github.com/mouredev/Hello-Python', 'https://github.com/Stability-AI/generative-models', 'https://github.com/sebastianruder/NLP-progress', 'https://github.com/JaidedAI/EasyOCR', 'https://github.com/kovidgoyal/kitty', 'https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix', 'https://github.com/HumanSignal/labelImg', 'https://github.com/d2l-ai/d2l-en', 'https://github.com/AtsushiSakai/PythonRobotics', 'https://github.com/pytorch/examples', 'https://github.com/cookiecutter/cookiecutter', 'https://github.com/tornadoweb/tornado', 'https://github.com/hiyouga/LLaMA-Factory', 'https://github.com/mindsdb/mindsdb', 'https://github.com/deepinsight/insightface', 'https://github.com/openai/gpt-2', 'https://github.com/luong-komorebi/Awesome-Linux-Software', 'https://github.com/WZMIAOMIAO/deep-learning-for-image-processing', 'https://github.com/drduh/macOS-Security-and-Privacy-Guide', 'https://github.com/openai/chatgpt-retrieval-plugin', 'https://github.com/plotly/dash', 'https://github.com/chriskiehl/Gooey', 'https://github.com/jhao104/proxy_pool', 'https://github.com/pyg-team/pytorch_geometric', 'https://github.com/saleor/saleor', 'https://github.com/zulip/zulip', 'https://github.com/jina-ai/jina', 'https://github.com/openai/openai-python', 'https://github.com/KurtBestor/Hitomi-Downloader', 'https://github.com/521xueweihan/GitHub520', 'https://github.com/ArchiveBox/ArchiveBox', 'https://github.com/facebookresearch/audiocraft', 'https://github.com/meta-llama/llama3', 'https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI', 'https://github.com/matplotlib/matplotlib', 'https://github.com/yoheinakajima/babyagi', 'https://github.com/Vonng/ddia', 'https://github.com/PromtEngineer/localGPT', 'https://github.com/vllm-project/vllm', 'https://github.com/ManimCommunity/manim', 'https://github.com/ungoogled-software/ungoogled-chromium', 'https://github.com/karpathy/minGPT', 'https://github.com/magenta/magenta', 'https://github.com/bokeh/bokeh', 'https://github.com/pydantic/pydantic', 'https://github.com/huggingface/datasets', 'https://github.com/microsoft/unilm', 'https://github.com/kholia/OSX-KVM', 'https://github.com/kovidgoyal/calibre', 'https://github.com/mkdocs/mkdocs', 'https://github.com/magic-wormhole/magic-wormhole', 'https://github.com/Delgan/loguru', 'https://github.com/lucidrains/vit-pytorch', 'https://github.com/nginx-proxy/nginx-proxy', 'https://github.com/recommenders-team/recommenders', 'https://github.com/RasaHQ/rasa', 'https://github.com/facebook/prophet', 'https://github.com/sanic-org/sanic', 'https://github.com/kaixindelele/ChatPaper', 'https://github.com/Jack-Cherish/python-spider', 'https://github.com/jantic/DeOldify', 'https://github.com/python/mypy', 'https://github.com/ymcui/Chinese-LLaMA-Alpaca', 'https://github.com/pyscript/pyscript', 'https://github.com/PostHog/posthog', 'https://github.com/mlflow/mlflow', 'https://github.com/spotify/luigi', 'https://github.com/wagtail/wagtail', 'https://github.com/Sanster/IOPaint', 'https://github.com/miloyip/game-programmer', 'https://github.com/joke2k/faker', 'https://github.com/mlc-ai/mlc-llm', 'https://github.com/Ciphey/Ciphey', 'https://github.com/quantopian/zipline', 'https://github.com/paperless-ngx/paperless-ngx', 'https://github.com/frappe/erpnext', 'https://github.com/stitionai/devika', 'https://github.com/rsms/inter', 'https://github.com/kivy/kivy', 'https://github.com/reflex-dev/reflex', 'https://github.com/onnx/onnx', 'https://github.com/reddit-archive/reddit', 'https://github.com/hpcaitech/Open-Sora', 'https://github.com/haotian-liu/LLaVA', 'https://github.com/chatanywhere/GPT_API_free', 'https://github.com/InstaPy/InstaPy', 'https://github.com/binux/pyspider', 'https://github.com/LiLittleCat/awesome-free-chatgpt', 'https://github.com/cool-RR/PySnooper', 'https://github.com/apple/ml-stable-diffusion', 'https://github.com/ipython/ipython', 'https://github.com/wilsonfreitas/awesome-quant', 'https://github.com/alievk/avatarify-python', 'https://github.com/Mikubill/sd-webui-controlnet', 'https://github.com/wting/autojump', 'https://github.com/trekhleb/learn-python', 'https://github.com/eriklindernoren/PyTorch-GAN', 'https://github.com/Kr1s77/awesome-python-login-model', 'https://github.com/twintproject/twint', 'https://github.com/THUDM/ChatGLM2-6B', 'https://github.com/wistbean/learn_python3_spider', 'https://github.com/mnielsen/neural-networks-and-deep-learning', 'https://github.com/pytorch/vision', 'https://github.com/h2y/Shadowrocket-ADBlock-Rules', 'https://github.com/OpenEthan/SMSBoom', 'https://github.com/openai/baselines', 'https://github.com/plotly/plotly.py', 'https://github.com/piskvorky/gensim', 'https://github.com/RunaCapital/awesome-oss-alternatives', 'https://github.com/meta-llama/codellama', 'https://github.com/pallets/click', 'https://github.com/spotDL/spotify-downloader', 'https://github.com/dgtlmoon/changedetection.io', 'https://github.com/Anjok07/ultimatevocalremovergui', 'https://github.com/netbox-community/netbox', 'https://github.com/mxrch/GHunt', 'https://github.com/ranger/ranger', 'https://github.com/tensorflow/tensor2tensor', 'https://github.com/aws/aws-cli', 'https://github.com/blakeblackshear/frigate', 'https://github.com/w-okada/voice-changer', 'https://github.com/GaiZhenbiao/ChuanhuChatGPT', 'https://github.com/PrefectHQ/prefect', 'https://github.com/jupyter/jupyter', 'https://github.com/facefusion/facefusion', 'https://github.com/danielgatis/rembg', 'https://github.com/borisdayma/dalle-mini', 'https://github.com/fabric/fabric', 'https://github.com/aio-libs/aiohttp', 'https://github.com/ddbourgin/numpy-ml', 'https://github.com/TransformerOptimus/SuperAGI', 'https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life', 'https://github.com/pyecharts/pyecharts', 'https://github.com/tiangolo/typer', 'https://github.com/Rapptz/discord.py', 'https://github.com/fauxpilot/fauxpilot', 'https://github.com/lra/mackup', 'https://github.com/apprenticeharper/DeDRM_tools', 'https://github.com/microsoft/qlib', 'https://github.com/networkx/networkx', 'https://github.com/powerline/powerline', 'https://github.com/arc53/DocsGPT', 'https://github.com/Python-World/python-mini-projects', 'https://github.com/airbytehq/airbyte', 'https://github.com/aleju/imgaug', 'https://github.com/roboflow/supervision', 'https://github.com/pjialin/py12306', 'https://github.com/hindupuravinash/the-gan-zoo', 'https://github.com/unifyai/ivy', 'https://github.com/openai/evals', 'https://github.com/horovod/horovod', 'https://github.com/huggingface/peft', 'https://github.com/NVlabs/stylegan', 'https://github.com/tgbot-collection/YYeTsBot', 'https://github.com/gunthercox/ChatterBot', 'https://github.com/UKPLab/sentence-transformers', 'https://github.com/saltstack/salt', 'https://github.com/wangshub/wechat_jump_game', 'https://github.com/youfou/wxpy', 'https://github.com/microsoft/nni', 'https://github.com/deepset-ai/haystack', 'https://github.com/codelucas/newspaper', 'https://github.com/joaomdmoura/crewAI', 'https://github.com/google/yapf', 'https://github.com/psf/requests-html', 'https://github.com/flairNLP/flair', 'https://github.com/sczhou/CodeFormer', 'https://github.com/shengqiangzhang/examples-of-web-crawlers', 'https://github.com/davidsandberg/facenet', 'https://github.com/NanmiCoder/MediaCrawler', 'https://github.com/ansible/awx', 'https://github.com/albumentations-team/albumentations', 'https://github.com/programthink/zhao', 'https://github.com/mail-in-a-box/mailinabox', 'https://github.com/sivel/speedtest-cli', 'https://github.com/searx/searx', 'https://github.com/ShangtongZhang/reinforcement-learning-an-introduction', 'https://github.com/iterative/dvc', 'https://github.com/PySimpleGUI/PySimpleGUI', 'https://github.com/mementum/backtrader', 'https://github.com/tiangolo/sqlmodel', 'https://github.com/nltk/nltk', 'https://github.com/dmlc/dgl', 'https://github.com/microsoft/Swin-Transformer', 'https://github.com/jindongwang/transferlearning', 'https://github.com/facebookresearch/detr', 'https://github.com/idank/explainshell', 'https://github.com/s0md3v/XSStrike', 'https://github.com/fortra/impacket', 'https://github.com/MetaCubeX/mihomo', 'https://github.com/wifiphisher/wifiphisher', 'https://github.com/jaakkopasanen/AutoEq', 'https://github.com/waditu/tushare', 'https://github.com/edgedb/edgedb', 'https://github.com/bloomberg/memray', 'https://github.com/ethereum/EIPs', 'https://github.com/PaddlePaddle/PaddleHub', 'https://github.com/scipy/scipy', 'https://github.com/chroma-core/chroma', 'https://github.com/sympy/sympy', 'https://github.com/beetbox/beets', 'https://github.com/postmanlabs/httpbin', 'https://github.com/labelmeai/labelme', 'https://github.com/SFTtech/openage', 'https://github.com/encode/httpx', 'https://github.com/redis/redis-py', 'https://github.com/getpelican/pelican', 'https://github.com/THUDM/ChatGLM3', 'https://github.com/jina-ai/clip-as-service', 'https://github.com/donnemartin/awesome-aws', 'https://github.com/microsoft/pyright', 'https://github.com/pre-commit/pre-commit', 'https://github.com/PaddlePaddle/PaddleDetection', 'https://github.com/ocrmypdf/OCRmyPDF', 'https://github.com/lss233/chatgpt-mirai-qq-bot', 'https://github.com/ydataai/ydata-profiling', 'https://github.com/dask/dask', 'https://github.com/mwaskom/seaborn', 'https://github.com/ranaroussi/yfinance', 'https://github.com/tonybeltramelli/pix2code', 'https://github.com/threat9/routersploit', 'https://github.com/Miserlou/Zappa', 'https://github.com/alexjc/neural-enhance', 'https://github.com/Zulko/moviepy', 'https://github.com/meolu/walle-web', 'https://github.com/OpenMOSS/MOSS', 'https://github.com/smicallef/spiderfoot', 'https://github.com/matrix-org/synapse', 'https://github.com/google-deepmind/alphafold', 'https://github.com/dbcli/pgcli', 'https://github.com/python-pillow/Pillow', 'https://github.com/BlinkDL/RWKV-LM', 'https://github.com/allenai/allennlp', 'https://github.com/LlamaFamily/Llama-Chinese', 'https://github.com/smol-ai/developer', 'https://github.com/janeczku/calibre-web', 'https://github.com/Embedding/Chinese-Word-Vectors', 'https://github.com/cookiecutter/cookiecutter-django', 'https://github.com/rougier/numpy-100', 'https://github.com/zalandoresearch/fashion-mnist']\n",
    "#github_urls = ['https://github.com/TencentARC/GFPGAN', 'https://github.com/apache/airflow', 'https://github.com/chenfei-wu/TaskMatrix', 'https://github.com/mitmproxy/mitmproxy', 'https://github.com/lm-sys/FastChat', 'https://github.com/comfyanonymous/ComfyUI', 'https://github.com/babysor/MockingBird', 'https://github.com/openai/gym', 'https://github.com/testerSunshine/12306', 'https://github.com/shadowsocks/shadowsocks', 'https://github.com/microsoft/DeepSpeed', 'https://github.com/XX-net/XX-Net', 'https://github.com/fxsjy/jieba', 'https://github.com/hankcs/HanLP', 'https://github.com/Asabeneh/30-Days-Of-Python', 'https://github.com/karpathy/nanoGPT', 'https://github.com/httpie/cli', 'https://github.com/streamlit/streamlit', 'https://github.com/ccxt/ccxt', 'https://github.com/run-llama/llama_index', 'https://github.com/ray-project/ray', 'https://github.com/certbot/certbot', 'https://github.com/sqlmapproject/sqlmap', 'https://github.com/geekcomputers/Python', 'https://github.com/huggingface/pytorch-image-models', 'https://github.com/coqui-ai/TTS', 'https://github.com/python-poetry/poetry', 'https://github.com/0xAX/linux-insides', 'https://github.com/facebookresearch/fairseq', 'https://github.com/gradio-app/gradio', 'https://github.com/yunjey/pytorch-tutorial', 'https://github.com/tatsu-lab/stanford_alpaca', 'https://github.com/explosion/spaCy', 'https://github.com/donnemartin/interactive-coding-challenges', 'https://github.com/facebookresearch/detectron2', 'https://github.com/Pythagora-io/gpt-pilot', 'https://github.com/google/jax', 'https://github.com/lllyasviel/ControlNet', 'https://github.com/acheong08/ChatGPT', 'https://github.com/open-mmlab/mmdetection', 'https://github.com/chatchat-space/Langchain-Chatchat', 'https://github.com/encode/django-rest-framework', 'https://github.com/tqdm/tqdm', 'https://github.com/Lightning-AI/pytorch-lightning', 'https://github.com/LC044/WeChatMsg', 'https://github.com/OWASP/CheatSheetSeries', 'https://github.com/donnemartin/data-science-ipython-notebooks', 'https://github.com/numpy/numpy', 'https://github.com/google/python-fire', 'https://github.com/xinntao/Real-ESRGAN', 'https://github.com/OpenBB-finance/OpenBBTerminal', 'https://github.com/facebookresearch/Detectron', 'https://github.com/freqtrade/freqtrade', 'https://github.com/StevenBlack/hosts', 'https://github.com/ycm-core/YouCompleteMe', 'https://github.com/spipm/Depix', 'https://github.com/zhayujie/chatgpt-on-wechat', 'https://github.com/littlecodersh/ItChat', 'https://github.com/nicolargo/glances', 'https://github.com/s0md3v/roop', 'https://github.com/getredash/redash', 'https://github.com/deezer/spleeter', 'https://github.com/Vision-CAIR/MiniGPT-4', 'https://github.com/python-telegram-bot/python-telegram-bot', 'https://github.com/pypa/pipenv', 'https://github.com/myshell-ai/OpenVoice', 'https://github.com/OpenDevin/OpenDevin', 'https://github.com/microsoft/cascadia-code', 'https://github.com/matterport/Mask_RCNN', 'https://github.com/tinygrad/tinygrad', 'https://github.com/svc-develop-team/so-vits-svc', 'https://github.com/RVC-Boss/GPT-SoVITS', 'https://github.com/jumpserver/jumpserver', 'https://github.com/locustio/locust', 'https://github.com/chubin/wttr.in', 'https://github.com/Textualize/textual', 'https://github.com/celery/celery', 'https://github.com/keon/algorithms', 'https://github.com/vnpy/vnpy', 'https://github.com/iperov/DeepFaceLive', 'https://github.com/ultralytics/ultralytics', 'https://github.com/eriklindernoren/ML-From-Scratch', 'https://github.com/microsoft/JARVIS', 'https://github.com/huggingface/diffusers', 'https://github.com/wangzheng0822/algo', 'https://github.com/mouredev/Hello-Python', 'https://github.com/Stability-AI/generative-models', 'https://github.com/sebastianruder/NLP-progress', 'https://github.com/JaidedAI/EasyOCR', 'https://github.com/kovidgoyal/kitty', 'https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix', 'https://github.com/HumanSignal/labelImg', 'https://github.com/d2l-ai/d2l-en', 'https://github.com/AtsushiSakai/PythonRobotics', 'https://github.com/pytorch/examples', 'https://github.com/cookiecutter/cookiecutter', 'https://github.com/tornadoweb/tornado', 'https://github.com/hiyouga/LLaMA-Factory', 'https://github.com/mindsdb/mindsdb', 'https://github.com/deepinsight/insightface', 'https://github.com/openai/gpt-2', 'https://github.com/luong-komorebi/Awesome-Linux-Software', 'https://github.com/WZMIAOMIAO/deep-learning-for-image-processing', 'https://github.com/drduh/macOS-Security-and-Privacy-Guide', 'https://github.com/openai/chatgpt-retrieval-plugin', 'https://github.com/plotly/dash', 'https://github.com/chriskiehl/Gooey', 'https://github.com/jhao104/proxy_pool', 'https://github.com/pyg-team/pytorch_geometric', 'https://github.com/saleor/saleor', 'https://github.com/zulip/zulip', 'https://github.com/jina-ai/jina', 'https://github.com/openai/openai-python', 'https://github.com/KurtBestor/Hitomi-Downloader', 'https://github.com/521xueweihan/GitHub520', 'https://github.com/ArchiveBox/ArchiveBox', 'https://github.com/facebookresearch/audiocraft', 'https://github.com/meta-llama/llama3', 'https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI', 'https://github.com/matplotlib/matplotlib', 'https://github.com/yoheinakajima/babyagi', 'https://github.com/Vonng/ddia', 'https://github.com/PromtEngineer/localGPT', 'https://github.com/vllm-project/vllm', 'https://github.com/ManimCommunity/manim', 'https://github.com/ungoogled-software/ungoogled-chromium', 'https://github.com/karpathy/minGPT', 'https://github.com/magenta/magenta', 'https://github.com/bokeh/bokeh', 'https://github.com/pydantic/pydantic', 'https://github.com/huggingface/datasets', 'https://github.com/microsoft/unilm', 'https://github.com/kholia/OSX-KVM', 'https://github.com/kovidgoyal/calibre', 'https://github.com/mkdocs/mkdocs', 'https://github.com/magic-wormhole/magic-wormhole', 'https://github.com/Delgan/loguru', 'https://github.com/lucidrains/vit-pytorch', 'https://github.com/nginx-proxy/nginx-proxy', 'https://github.com/recommenders-team/recommenders', 'https://github.com/RasaHQ/rasa', 'https://github.com/facebook/prophet', 'https://github.com/sanic-org/sanic', 'https://github.com/kaixindelele/ChatPaper', 'https://github.com/Jack-Cherish/python-spider', 'https://github.com/jantic/DeOldify', 'https://github.com/python/mypy', 'https://github.com/ymcui/Chinese-LLaMA-Alpaca', 'https://github.com/pyscript/pyscript', 'https://github.com/PostHog/posthog', 'https://github.com/mlflow/mlflow', 'https://github.com/spotify/luigi', 'https://github.com/wagtail/wagtail', 'https://github.com/Sanster/IOPaint', 'https://github.com/miloyip/game-programmer', 'https://github.com/joke2k/faker', 'https://github.com/mlc-ai/mlc-llm', 'https://github.com/Ciphey/Ciphey', 'https://github.com/quantopian/zipline', 'https://github.com/paperless-ngx/paperless-ngx', 'https://github.com/frappe/erpnext', 'https://github.com/stitionai/devika', 'https://github.com/rsms/inter', 'https://github.com/kivy/kivy', 'https://github.com/reflex-dev/reflex', 'https://github.com/onnx/onnx', 'https://github.com/reddit-archive/reddit', 'https://github.com/hpcaitech/Open-Sora', 'https://github.com/haotian-liu/LLaVA', 'https://github.com/chatanywhere/GPT_API_free', 'https://github.com/InstaPy/InstaPy', 'https://github.com/binux/pyspider', 'https://github.com/LiLittleCat/awesome-free-chatgpt', 'https://github.com/cool-RR/PySnooper', 'https://github.com/apple/ml-stable-diffusion', 'https://github.com/ipython/ipython', 'https://github.com/wilsonfreitas/awesome-quant', 'https://github.com/alievk/avatarify-python', 'https://github.com/Mikubill/sd-webui-controlnet', 'https://github.com/wting/autojump', 'https://github.com/trekhleb/learn-python', 'https://github.com/eriklindernoren/PyTorch-GAN', 'https://github.com/Kr1s77/awesome-python-login-model', 'https://github.com/twintproject/twint', 'https://github.com/THUDM/ChatGLM2-6B', 'https://github.com/wistbean/learn_python3_spider', 'https://github.com/mnielsen/neural-networks-and-deep-learning', 'https://github.com/pytorch/vision', 'https://github.com/h2y/Shadowrocket-ADBlock-Rules', 'https://github.com/OpenEthan/SMSBoom', 'https://github.com/openai/baselines', 'https://github.com/plotly/plotly.py', 'https://github.com/piskvorky/gensim', 'https://github.com/RunaCapital/awesome-oss-alternatives', 'https://github.com/meta-llama/codellama', 'https://github.com/pallets/click', 'https://github.com/spotDL/spotify-downloader', 'https://github.com/dgtlmoon/changedetection.io', 'https://github.com/Anjok07/ultimatevocalremovergui', 'https://github.com/netbox-community/netbox', 'https://github.com/mxrch/GHunt', 'https://github.com/ranger/ranger', 'https://github.com/tensorflow/tensor2tensor', 'https://github.com/aws/aws-cli', 'https://github.com/blakeblackshear/frigate', 'https://github.com/w-okada/voice-changer', 'https://github.com/GaiZhenbiao/ChuanhuChatGPT', 'https://github.com/PrefectHQ/prefect', 'https://github.com/jupyter/jupyter', 'https://github.com/facefusion/facefusion', 'https://github.com/danielgatis/rembg', 'https://github.com/borisdayma/dalle-mini', 'https://github.com/fabric/fabric', 'https://github.com/aio-libs/aiohttp', 'https://github.com/ddbourgin/numpy-ml', 'https://github.com/TransformerOptimus/SuperAGI', 'https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life', 'https://github.com/pyecharts/pyecharts', 'https://github.com/tiangolo/typer', 'https://github.com/Rapptz/discord.py', 'https://github.com/fauxpilot/fauxpilot', 'https://github.com/lra/mackup', 'https://github.com/apprenticeharper/DeDRM_tools', 'https://github.com/microsoft/qlib', 'https://github.com/networkx/networkx', 'https://github.com/powerline/powerline', 'https://github.com/arc53/DocsGPT', 'https://github.com/Python-World/python-mini-projects', 'https://github.com/airbytehq/airbyte', 'https://github.com/aleju/imgaug', 'https://github.com/roboflow/supervision', 'https://github.com/pjialin/py12306', 'https://github.com/hindupuravinash/the-gan-zoo', 'https://github.com/unifyai/ivy', 'https://github.com/openai/evals', 'https://github.com/horovod/horovod', 'https://github.com/huggingface/peft', 'https://github.com/NVlabs/stylegan', 'https://github.com/tgbot-collection/YYeTsBot', 'https://github.com/gunthercox/ChatterBot', 'https://github.com/UKPLab/sentence-transformers', 'https://github.com/saltstack/salt', 'https://github.com/wangshub/wechat_jump_game', 'https://github.com/youfou/wxpy', 'https://github.com/microsoft/nni', 'https://github.com/deepset-ai/haystack', 'https://github.com/codelucas/newspaper', 'https://github.com/joaomdmoura/crewAI', 'https://github.com/google/yapf', 'https://github.com/psf/requests-html', 'https://github.com/flairNLP/flair', 'https://github.com/sczhou/CodeFormer', 'https://github.com/shengqiangzhang/examples-of-web-crawlers', 'https://github.com/davidsandberg/facenet', 'https://github.com/NanmiCoder/MediaCrawler', 'https://github.com/ansible/awx', 'https://github.com/albumentations-team/albumentations', 'https://github.com/programthink/zhao', 'https://github.com/mail-in-a-box/mailinabox', 'https://github.com/sivel/speedtest-cli', 'https://github.com/searx/searx', 'https://github.com/ShangtongZhang/reinforcement-learning-an-introduction', 'https://github.com/iterative/dvc', 'https://github.com/PySimpleGUI/PySimpleGUI', 'https://github.com/mementum/backtrader', 'https://github.com/tiangolo/sqlmodel', 'https://github.com/nltk/nltk', 'https://github.com/dmlc/dgl', 'https://github.com/microsoft/Swin-Transformer', 'https://github.com/jindongwang/transferlearning', 'https://github.com/facebookresearch/detr', 'https://github.com/idank/explainshell', 'https://github.com/s0md3v/XSStrike', 'https://github.com/fortra/impacket', 'https://github.com/MetaCubeX/mihomo', 'https://github.com/wifiphisher/wifiphisher', 'https://github.com/jaakkopasanen/AutoEq', 'https://github.com/waditu/tushare', 'https://github.com/edgedb/edgedb', 'https://github.com/bloomberg/memray', 'https://github.com/ethereum/EIPs', 'https://github.com/PaddlePaddle/PaddleHub', 'https://github.com/scipy/scipy', 'https://github.com/chroma-core/chroma', 'https://github.com/sympy/sympy', 'https://github.com/beetbox/beets', 'https://github.com/postmanlabs/httpbin', 'https://github.com/labelmeai/labelme', 'https://github.com/SFTtech/openage', 'https://github.com/encode/httpx', 'https://github.com/redis/redis-py', 'https://github.com/getpelican/pelican', 'https://github.com/THUDM/ChatGLM3', 'https://github.com/jina-ai/clip-as-service', 'https://github.com/donnemartin/awesome-aws', 'https://github.com/microsoft/pyright', 'https://github.com/pre-commit/pre-commit', 'https://github.com/PaddlePaddle/PaddleDetection', 'https://github.com/ocrmypdf/OCRmyPDF', 'https://github.com/lss233/chatgpt-mirai-qq-bot', 'https://github.com/ydataai/ydata-profiling', 'https://github.com/dask/dask', 'https://github.com/mwaskom/seaborn', 'https://github.com/ranaroussi/yfinance', 'https://github.com/tonybeltramelli/pix2code', 'https://github.com/threat9/routersploit', 'https://github.com/Miserlou/Zappa', 'https://github.com/alexjc/neural-enhance', 'https://github.com/Zulko/moviepy', 'https://github.com/meolu/walle-web', 'https://github.com/OpenMOSS/MOSS', 'https://github.com/smicallef/spiderfoot', 'https://github.com/matrix-org/synapse', 'https://github.com/google-deepmind/alphafold', 'https://github.com/dbcli/pgcli', 'https://github.com/python-pillow/Pillow', 'https://github.com/BlinkDL/RWKV-LM', 'https://github.com/allenai/allennlp', 'https://github.com/LlamaFamily/Llama-Chinese', 'https://github.com/smol-ai/developer', 'https://github.com/janeczku/calibre-web', 'https://github.com/Embedding/Chinese-Word-Vectors', 'https://github.com/cookiecutter/cookiecutter-django', 'https://github.com/rougier/numpy-100', 'https://github.com/zalandoresearch/fashion-mnist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bca9c11-5104-4102-bfd4-607705f3aae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'GFPGAN'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/TencentARC/GFPGAN to GFPGAN.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'airflow'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/apache/airflow to airflow.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'TaskMatrix'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/chenfei-wu/TaskMatrix to TaskMatrix.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'mitmproxy'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/mitmproxy/mitmproxy to mitmproxy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'FastChat'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/lm-sys/FastChat to FastChat.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ComfyUI'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/comfyanonymous/ComfyUI to ComfyUI.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'MockingBird'...\n",
      "Updating files: 100% (205/205), done.\n",
      "Cloning into 'gym'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/babysor/MockingBird to MockingBird.csv\n",
      "Successfully processed and saved https://github.com/openai/gym to gym.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '12306'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/testerSunshine/12306 to 12306.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'shadowsocks'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/shadowsocks/shadowsocks to shadowsocks.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepSpeed'...\n",
      "Updating files: 100% (1554/1554), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/microsoft/DeepSpeed to DeepSpeed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'XX-Net'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/XX-net/XX-Net to XX-Net.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'jieba'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/fxsjy/jieba to jieba.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'HanLP'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/hankcs/HanLP to HanLP.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '30-Days-Of-Python'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Asabeneh/30-Days-Of-Python to 30-Days-Of-Python.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'nanoGPT'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/karpathy/nanoGPT to nanoGPT.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'cli'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/httpie/cli to cli.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'streamlit'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/streamlit/streamlit to streamlit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ccxt'...\n",
      "Updating files: 100% (5987/5987), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ccxt/ccxt to ccxt.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama_index'...\n",
      "Updating files: 100% (9994/9994), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/run-llama/llama_index to llama_index.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ray'...\n",
      "Updating files: 100% (7575/7575), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ray-project/ray to ray.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'certbot'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/certbot/certbot to certbot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'sqlmap'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/sqlmapproject/sqlmap to sqlmap.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Python'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/geekcomputers/Python to Python.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-image-models'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/huggingface/pytorch-image-models to pytorch-image-models.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'TTS'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/coqui-ai/TTS to TTS.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'poetry'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/python-poetry/poetry to poetry.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'linux-insides'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/0xAX/linux-insides to linux-insides.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'fairseq'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/facebookresearch/fairseq to fairseq.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gradio'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/gradio-app/gradio to gradio.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-tutorial'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/yunjey/pytorch-tutorial to pytorch-tutorial.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'stanford_alpaca'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/tatsu-lab/stanford_alpaca to stanford_alpaca.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'spaCy'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/explosion/spaCy to spaCy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'interactive-coding-challenges'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/donnemartin/interactive-coding-challenges to interactive-coding-challenges.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'detectron2'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/facebookresearch/detectron2 to detectron2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gpt-pilot'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Pythagora-io/gpt-pilot to gpt-pilot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'jax'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/google/jax to jax.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ControlNet'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/lllyasviel/ControlNet to ControlNet.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatGPT'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/acheong08/ChatGPT to ChatGPT.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'mmdetection'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/open-mmlab/mmdetection to mmdetection.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Langchain-Chatchat'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/chatchat-space/Langchain-Chatchat to Langchain-Chatchat.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'django-rest-framework'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/encode/django-rest-framework to django-rest-framework.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'tqdm'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/tqdm/tqdm to tqdm.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-lightning'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Lightning-AI/pytorch-lightning to pytorch-lightning.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'WeChatMsg'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/LC044/WeChatMsg to WeChatMsg.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'CheatSheetSeries'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/OWASP/CheatSheetSeries to CheatSheetSeries.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'data-science-ipython-notebooks'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/donnemartin/data-science-ipython-notebooks to data-science-ipython-notebooks.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'numpy'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/numpy/numpy to numpy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'python-fire'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/google/python-fire to python-fire.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Real-ESRGAN'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/xinntao/Real-ESRGAN to Real-ESRGAN.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'OpenBBTerminal'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/OpenBB-finance/OpenBBTerminal to OpenBBTerminal.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Detectron'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/facebookresearch/Detectron to Detectron.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'freqtrade'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/freqtrade/freqtrade to freqtrade.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'hosts'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/StevenBlack/hosts to hosts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'YouCompleteMe'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ycm-core/YouCompleteMe to YouCompleteMe.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Depix'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/spipm/Depix to Depix.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'chatgpt-on-wechat'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/zhayujie/chatgpt-on-wechat to chatgpt-on-wechat.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ItChat'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/littlecodersh/ItChat to ItChat.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'glances'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/nicolargo/glances to glances.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'roop'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/s0md3v/roop to roop.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'redash'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/getredash/redash to redash.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'spleeter'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/deezer/spleeter to spleeter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'MiniGPT-4'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Vision-CAIR/MiniGPT-4 to MiniGPT-4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'python-telegram-bot'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/python-telegram-bot/python-telegram-bot to python-telegram-bot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pipenv'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/pypa/pipenv to pipenv.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'OpenVoice'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/myshell-ai/OpenVoice to OpenVoice.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'OpenDevin'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/OpenDevin/OpenDevin to OpenDevin.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'cascadia-code'...\n",
      "Updating files: 100% (41191/41191), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/microsoft/cascadia-code to cascadia-code.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Mask_RCNN'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/matterport/Mask_RCNN to Mask_RCNN.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'tinygrad'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/tinygrad/tinygrad to tinygrad.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'so-vits-svc'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/svc-develop-team/so-vits-svc to so-vits-svc.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'GPT-SoVITS'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/RVC-Boss/GPT-SoVITS to GPT-SoVITS.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'jumpserver'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/jumpserver/jumpserver to jumpserver.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'locust'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/locustio/locust to locust.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'wttr.in'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/chubin/wttr.in to wttr.in.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'textual'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Textualize/textual to textual.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'celery'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/celery/celery to celery.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'algorithms'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/keon/algorithms to algorithms.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'vnpy'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/vnpy/vnpy to vnpy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepFaceLive'...\n",
      "Updating files: 100% (696/696), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/iperov/DeepFaceLive to DeepFaceLive.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ultralytics'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ultralytics/ultralytics to ultralytics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ML-From-Scratch'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/eriklindernoren/ML-From-Scratch to ML-From-Scratch.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'JARVIS'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/microsoft/JARVIS to JARVIS.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'diffusers'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/huggingface/diffusers to diffusers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'algo'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/wangzheng0822/algo to algo.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Hello-Python'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/mouredev/Hello-Python to Hello-Python.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'generative-models'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Stability-AI/generative-models to generative-models.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'NLP-progress'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/sebastianruder/NLP-progress to NLP-progress.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'EasyOCR'...\n",
      "Updating files: 100% (313/313), done.\n",
      "Cloning into 'kitty'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/JaidedAI/EasyOCR to EasyOCR.csv\n",
      "Successfully processed and saved https://github.com/kovidgoyal/kitty to kitty.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix to pytorch-CycleGAN-and-pix2pix.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'labelImg'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/HumanSignal/labelImg to labelImg.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'd2l-en'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/d2l-ai/d2l-en to d2l-en.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'PythonRobotics'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/AtsushiSakai/PythonRobotics to PythonRobotics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'examples'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/pytorch/examples to examples.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'cookiecutter'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/cookiecutter/cookiecutter to cookiecutter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'tornado'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/tornadoweb/tornado to tornado.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "Updating files: 100% (214/214), done.\n",
      "Cloning into 'mindsdb'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/hiyouga/LLaMA-Factory to LLaMA-Factory.csv\n",
      "Successfully processed and saved https://github.com/mindsdb/mindsdb to mindsdb.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'insightface'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/deepinsight/insightface to insightface.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gpt-2'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/openai/gpt-2 to gpt-2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Awesome-Linux-Software'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/luong-komorebi/Awesome-Linux-Software to Awesome-Linux-Software.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'deep-learning-for-image-processing'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/WZMIAOMIAO/deep-learning-for-image-processing to deep-learning-for-image-processing.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'macOS-Security-and-Privacy-Guide'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/drduh/macOS-Security-and-Privacy-Guide to macOS-Security-and-Privacy-Guide.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'chatgpt-retrieval-plugin'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/openai/chatgpt-retrieval-plugin to chatgpt-retrieval-plugin.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'dash'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/plotly/dash to dash.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Gooey'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/chriskiehl/Gooey to Gooey.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'proxy_pool'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/jhao104/proxy_pool to proxy_pool.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch_geometric'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/pyg-team/pytorch_geometric to pytorch_geometric.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'saleor'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/saleor/saleor to saleor.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'zulip'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/zulip/zulip to zulip.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'jina'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/jina-ai/jina to jina.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'openai-python'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/openai/openai-python to openai-python.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Hitomi-Downloader'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/KurtBestor/Hitomi-Downloader to Hitomi-Downloader.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'GitHub520'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/521xueweihan/GitHub520 to GitHub520.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ArchiveBox'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing https://github.com/ArchiveBox/ArchiveBox: [Errno 2] No such file or directory: 'ArchiveBox/archivebox/vendor/atomicwrites.py'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'audiocraft'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/facebookresearch/audiocraft to audiocraft.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama3'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/meta-llama/llama3 to llama3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Retrieval-based-Voice-Conversion-WebUI'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI to Retrieval-based-Voice-Conversion-WebUI.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'matplotlib'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/matplotlib/matplotlib to matplotlib.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'babyagi'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/yoheinakajima/babyagi to babyagi.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ddia'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Vonng/ddia to ddia.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'localGPT'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/PromtEngineer/localGPT to localGPT.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'vllm'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/vllm-project/vllm to vllm.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'manim'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ManimCommunity/manim to manim.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ungoogled-chromium'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ungoogled-software/ungoogled-chromium to ungoogled-chromium.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'minGPT'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/karpathy/minGPT to minGPT.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'magenta'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/magenta/magenta to magenta.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'bokeh'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/bokeh/bokeh to bokeh.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pydantic'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/pydantic/pydantic to pydantic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'datasets'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/huggingface/datasets to datasets.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'unilm'...\n",
      "Updating files: 100% (5228/5228), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/microsoft/unilm to unilm.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'OSX-KVM'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/kholia/OSX-KVM to OSX-KVM.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'calibre'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/kovidgoyal/calibre to calibre.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'mkdocs'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/mkdocs/mkdocs to mkdocs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'magic-wormhole'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/magic-wormhole/magic-wormhole to magic-wormhole.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'loguru'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Delgan/loguru to loguru.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'vit-pytorch'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/lucidrains/vit-pytorch to vit-pytorch.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'nginx-proxy'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/nginx-proxy/nginx-proxy to nginx-proxy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'recommenders'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/recommenders-team/recommenders to recommenders.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'rasa'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/RasaHQ/rasa to rasa.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'prophet'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/facebook/prophet to prophet.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'sanic'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/sanic-org/sanic to sanic.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatPaper'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/kaixindelele/ChatPaper to ChatPaper.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'python-spider'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Jack-Cherish/python-spider to python-spider.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeOldify'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/jantic/DeOldify to DeOldify.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'mypy'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/python/mypy to mypy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Chinese-LLaMA-Alpaca'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ymcui/Chinese-LLaMA-Alpaca to Chinese-LLaMA-Alpaca.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pyscript'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/pyscript/pyscript to pyscript.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'posthog'...\n",
      "Updating files: 100% (6383/6383), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/PostHog/posthog to posthog.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'mlflow'...\n",
      "Updating files: 100% (4116/4116), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/mlflow/mlflow to mlflow.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'luigi'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/spotify/luigi to luigi.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'wagtail'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/wagtail/wagtail to wagtail.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'IOPaint'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Sanster/IOPaint to IOPaint.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'game-programmer'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/miloyip/game-programmer to game-programmer.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'faker'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/joke2k/faker to faker.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'mlc-llm'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/mlc-ai/mlc-llm to mlc-llm.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Ciphey'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Ciphey/Ciphey to Ciphey.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'zipline'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/quantopian/zipline to zipline.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'paperless-ngx'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/paperless-ngx/paperless-ngx to paperless-ngx.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'erpnext'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/frappe/erpnext to erpnext.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'devika'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/stitionai/devika to devika.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'inter'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/rsms/inter to inter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'kivy'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/kivy/kivy to kivy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'reflex'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/reflex-dev/reflex to reflex.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'onnx'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/onnx/onnx to onnx.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'reddit'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/reddit-archive/reddit to reddit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Open-Sora'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/hpcaitech/Open-Sora to Open-Sora.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaVA'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/haotian-liu/LLaVA to LLaVA.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'GPT_API_free'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/chatanywhere/GPT_API_free to GPT_API_free.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'InstaPy'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/InstaPy/InstaPy to InstaPy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pyspider'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/binux/pyspider to pyspider.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'awesome-free-chatgpt'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/LiLittleCat/awesome-free-chatgpt to awesome-free-chatgpt.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'PySnooper'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/cool-RR/PySnooper to PySnooper.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ml-stable-diffusion'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/apple/ml-stable-diffusion to ml-stable-diffusion.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ipython'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ipython/ipython to ipython.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'awesome-quant'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/wilsonfreitas/awesome-quant to awesome-quant.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'avatarify-python'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/alievk/avatarify-python to avatarify-python.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'sd-webui-controlnet'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Mikubill/sd-webui-controlnet to sd-webui-controlnet.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'autojump'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/wting/autojump to autojump.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'learn-python'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/trekhleb/learn-python to learn-python.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'PyTorch-GAN'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/eriklindernoren/PyTorch-GAN to PyTorch-GAN.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'awesome-python-login-model'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Kr1s77/awesome-python-login-model to awesome-python-login-model.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'twint'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/twintproject/twint to twint.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatGLM2-6B'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/THUDM/ChatGLM2-6B to ChatGLM2-6B.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'learn_python3_spider'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/wistbean/learn_python3_spider to learn_python3_spider.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'neural-networks-and-deep-learning'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/mnielsen/neural-networks-and-deep-learning to neural-networks-and-deep-learning.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'vision'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/pytorch/vision to vision.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Shadowrocket-ADBlock-Rules'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/h2y/Shadowrocket-ADBlock-Rules to Shadowrocket-ADBlock-Rules.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'SMSBoom'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/OpenEthan/SMSBoom to SMSBoom.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'baselines'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/openai/baselines to baselines.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'plotly.py'...\n",
      "Updating files: 100% (14054/14054), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/plotly/plotly.py to plotly.py.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gensim'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/piskvorky/gensim to gensim.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'awesome-oss-alternatives'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/RunaCapital/awesome-oss-alternatives to awesome-oss-alternatives.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'codellama'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/meta-llama/codellama to codellama.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'click'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/pallets/click to click.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'spotify-downloader'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/spotDL/spotify-downloader to spotify-downloader.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'changedetection.io'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/dgtlmoon/changedetection.io to changedetection.io.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ultimatevocalremovergui'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Anjok07/ultimatevocalremovergui to ultimatevocalremovergui.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'netbox'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/netbox-community/netbox to netbox.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'GHunt'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/mxrch/GHunt to GHunt.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ranger'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ranger/ranger to ranger.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'tensor2tensor'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/tensorflow/tensor2tensor to tensor2tensor.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'aws-cli'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/aws/aws-cli to aws-cli.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'frigate'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/blakeblackshear/frigate to frigate.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'voice-changer'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/w-okada/voice-changer to voice-changer.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChuanhuChatGPT'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/GaiZhenbiao/ChuanhuChatGPT to ChuanhuChatGPT.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'prefect'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/PrefectHQ/prefect to prefect.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'jupyter'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/jupyter/jupyter to jupyter.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'facefusion'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/facefusion/facefusion to facefusion.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'rembg'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/danielgatis/rembg to rembg.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'dalle-mini'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/borisdayma/dalle-mini to dalle-mini.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'fabric'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/fabric/fabric to fabric.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'aiohttp'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/aio-libs/aiohttp to aiohttp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'numpy-ml'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ddbourgin/numpy-ml to numpy-ml.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'SuperAGI'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/TransformerOptimus/SuperAGI to SuperAGI.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Bringing-Old-Photos-Back-to-Life'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life to Bringing-Old-Photos-Back-to-Life.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pyecharts'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/pyecharts/pyecharts to pyecharts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'typer'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/tiangolo/typer to typer.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'discord.py'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Rapptz/discord.py to discord.py.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'fauxpilot'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/fauxpilot/fauxpilot to fauxpilot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'mackup'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/lra/mackup to mackup.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeDRM_tools'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/apprenticeharper/DeDRM_tools to DeDRM_tools.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'qlib'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/microsoft/qlib to qlib.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'networkx'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/networkx/networkx to networkx.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'powerline'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/powerline/powerline to powerline.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'DocsGPT'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/arc53/DocsGPT to DocsGPT.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'python-mini-projects'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Python-World/python-mini-projects to python-mini-projects.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'airbyte'...\n",
      "Updating files: 100% (15906/15906), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/airbytehq/airbyte to airbyte.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'imgaug'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/aleju/imgaug to imgaug.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'supervision'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/roboflow/supervision to supervision.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'py12306'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/pjialin/py12306 to py12306.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'the-gan-zoo'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/hindupuravinash/the-gan-zoo to the-gan-zoo.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ivy'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/unifyai/ivy to ivy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'evals'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/openai/evals to evals.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'horovod'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/horovod/horovod to horovod.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'peft'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/huggingface/peft to peft.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'stylegan'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/NVlabs/stylegan to stylegan.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'YYeTsBot'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/tgbot-collection/YYeTsBot to YYeTsBot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatterBot'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/gunthercox/ChatterBot to ChatterBot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'sentence-transformers'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/UKPLab/sentence-transformers to sentence-transformers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'salt'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/saltstack/salt to salt.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'wechat_jump_game'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/wangshub/wechat_jump_game to wechat_jump_game.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'wxpy'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/youfou/wxpy to wxpy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'nni'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/microsoft/nni to nni.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'haystack'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/deepset-ai/haystack to haystack.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'newspaper'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/codelucas/newspaper to newspaper.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'crewAI'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/joaomdmoura/crewAI to crewAI.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yapf'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/google/yapf to yapf.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'requests-html'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/psf/requests-html to requests-html.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'flair'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/flairNLP/flair to flair.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'CodeFormer'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/sczhou/CodeFormer to CodeFormer.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'examples-of-web-crawlers'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/shengqiangzhang/examples-of-web-crawlers to examples-of-web-crawlers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'facenet'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/davidsandberg/facenet to facenet.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'MediaCrawler'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/NanmiCoder/MediaCrawler to MediaCrawler.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'awx'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ansible/awx to awx.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'albumentations'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/albumentations-team/albumentations to albumentations.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'zhao'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/programthink/zhao to zhao.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'mailinabox'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/mail-in-a-box/mailinabox to mailinabox.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'speedtest-cli'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/sivel/speedtest-cli to speedtest-cli.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'searx'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/searx/searx to searx.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'reinforcement-learning-an-introduction'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ShangtongZhang/reinforcement-learning-an-introduction to reinforcement-learning-an-introduction.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'dvc'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/iterative/dvc to dvc.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'PySimpleGUI'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/PySimpleGUI/PySimpleGUI to PySimpleGUI.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'backtrader'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/mementum/backtrader to backtrader.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'sqlmodel'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/tiangolo/sqlmodel to sqlmodel.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'nltk'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/nltk/nltk to nltk.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'dgl'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/dmlc/dgl to dgl.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Swin-Transformer'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/microsoft/Swin-Transformer to Swin-Transformer.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'transferlearning'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/jindongwang/transferlearning to transferlearning.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'detr'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/facebookresearch/detr to detr.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'explainshell'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/idank/explainshell to explainshell.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'XSStrike'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/s0md3v/XSStrike to XSStrike.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'impacket'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/fortra/impacket to impacket.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'mihomo'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/MetaCubeX/mihomo to mihomo.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'wifiphisher'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/wifiphisher/wifiphisher to wifiphisher.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'AutoEq'...\n",
      "Updating files: 100% (63620/63620), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/jaakkopasanen/AutoEq to AutoEq.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'tushare'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/waditu/tushare to tushare.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'edgedb'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/edgedb/edgedb to edgedb.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'memray'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/bloomberg/memray to memray.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'EIPs'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ethereum/EIPs to EIPs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'PaddleHub'...\n",
      "Updating files: 100% (2956/2956), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/PaddlePaddle/PaddleHub to PaddleHub.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'scipy'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/scipy/scipy to scipy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'chroma'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/chroma-core/chroma to chroma.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'sympy'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/sympy/sympy to sympy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'beets'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/beetbox/beets to beets.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'httpbin'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/postmanlabs/httpbin to httpbin.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'labelme'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/labelmeai/labelme to labelme.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'openage'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/SFTtech/openage to openage.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'httpx'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/encode/httpx to httpx.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'redis-py'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/redis/redis-py to redis-py.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pelican'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/getpelican/pelican to pelican.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatGLM3'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/THUDM/ChatGLM3 to ChatGLM3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'clip-as-service'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/jina-ai/clip-as-service to clip-as-service.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'awesome-aws'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/donnemartin/awesome-aws to awesome-aws.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pyright'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/microsoft/pyright to pyright.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pre-commit'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/pre-commit/pre-commit to pre-commit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'PaddleDetection'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/PaddlePaddle/PaddleDetection to PaddleDetection.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'OCRmyPDF'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ocrmypdf/OCRmyPDF to OCRmyPDF.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'chatgpt-mirai-qq-bot'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/lss233/chatgpt-mirai-qq-bot to chatgpt-mirai-qq-bot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ydata-profiling'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ydataai/ydata-profiling to ydata-profiling.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'dask'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/dask/dask to dask.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'seaborn'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/mwaskom/seaborn to seaborn.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yfinance'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/ranaroussi/yfinance to yfinance.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pix2code'...\n",
      "Updating files: 100% (43/43), done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/tonybeltramelli/pix2code to pix2code.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'routersploit'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/threat9/routersploit to routersploit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Zappa'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Miserlou/Zappa to Zappa.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'neural-enhance'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/alexjc/neural-enhance to neural-enhance.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'moviepy'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Zulko/moviepy to moviepy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'walle-web'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/meolu/walle-web to walle-web.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'MOSS'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/OpenMOSS/MOSS to MOSS.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'spiderfoot'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/smicallef/spiderfoot to spiderfoot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'synapse'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/matrix-org/synapse to synapse.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'alphafold'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/google-deepmind/alphafold to alphafold.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pgcli'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/dbcli/pgcli to pgcli.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Pillow'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/python-pillow/Pillow to Pillow.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'RWKV-LM'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/BlinkDL/RWKV-LM to RWKV-LM.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'allennlp'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/allenai/allennlp to allennlp.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Llama-Chinese'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/LlamaFamily/Llama-Chinese to Llama-Chinese.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'developer'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/smol-ai/developer to developer.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'calibre-web'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/janeczku/calibre-web to calibre-web.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Chinese-Word-Vectors'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/Embedding/Chinese-Word-Vectors to Chinese-Word-Vectors.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'cookiecutter-django'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/cookiecutter/cookiecutter-django to cookiecutter-django.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'numpy-100'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/rougier/numpy-100 to numpy-100.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'fashion-mnist'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved https://github.com/zalandoresearch/fashion-mnist to fashion-mnist.csv\n",
      "All repositories processed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Directory to store CSV files\n",
    "output_dir = \"github_repo_source_code\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create a CSV file per GitHub repo\n",
    "for url in github_urls:\n",
    "    try:\n",
    "        repo_name, concatenated_code = collect_source_code(url)\n",
    "        csv_file_name = f\"{repo_name}.csv\"\n",
    "        csv_file_path = os.path.join(output_dir, csv_file_name)\n",
    "        with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([concatenated_code])\n",
    "        print(f\"Successfully processed and saved {url} to {csv_file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "print(\"All repositories processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164f097f-1bb8-43ec-9d45-ce799b9dc254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, quote\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to clone a GitHub repository and collect all source code into a single string\n",
    "def collect_source_code(repo_url):\n",
    "    # Extract the repo name from the URL\n",
    "    repo_name = repo_url.rstrip('/').split('/')[-1]\n",
    "    subprocess.run(['git', 'clone', repo_url], check=True)\n",
    "    \n",
    "    # Collect all source code files into a single string\n",
    "    source_code = []\n",
    "    for root, dirs, files in os.walk(repo_name):\n",
    "        for file in files:\n",
    "            # Filter for source code files only (adjust filters as needed)\n",
    "            if file.endswith(('.py', '.js', '.java', '.cpp', '.c', '.h', '.html', '.css', '.ts', '.go', '.rb', '.php')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', errors='ignore') as f:\n",
    "                    source_code.append(f.read())\n",
    "                    \n",
    "    # Join all source code files as one big string\n",
    "    concatenated_code = \"\\n\".join(source_code)\n",
    "    print(type(concatenated_code))\n",
    "    \n",
    "    # Delete the repo after extraction\n",
    "    shutil.rmtree(repo_name)\n",
    "    \n",
    "    return repo_name, concatenated_code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b6b2733-8a26-4376-bfdf-85a8e5f27c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this list with your own list of 300 URLs\n",
    "#github_urls = ['https://github.com/public-apis/public-apis', 'https://github.com/donnemartin/system-design-primer', 'https://github.com/vinta/awesome-python', 'https://github.com/TheAlgorithms/Python', 'https://github.com/jackfrued/Python-100-Days', 'https://github.com/AUTOMATIC1111/stable-diffusion-webui', 'https://github.com/ytdl-org/youtube-dl', 'https://github.com/huggingface/transformers', 'https://github.com/521xueweihan/HelloGitHub', 'https://github.com/langchain-ai/langchain', 'https://github.com/nvbn/thefuck', 'https://github.com/pytorch/pytorch', 'https://github.com/django/django', 'https://github.com/tensorflow/models', 'https://github.com/yt-dlp/yt-dlp', 'https://github.com/tiangolo/fastapi', 'https://github.com/home-assistant/core', 'https://github.com/pallets/flask', 'https://github.com/fighting41love/funNLP', 'https://github.com/bregman-arie/devops-exercises', 'https://github.com/josephmisiti/awesome-machine-learning', 'https://github.com/ansible/ansible', 'https://github.com/keras-team/keras', 'https://github.com/openai/whisper', 'https://github.com/python/cpython', 'https://github.com/3b1b/manim', 'https://github.com/scikit-learn/scikit-learn', 'https://github.com/xtekky/gpt4free', 'https://github.com/binary-husky/gpt_academic', 'https://github.com/d2l-ai/d2l-zh', 'https://github.com/swisskyrepo/PayloadsAllTheThings', 'https://github.com/meta-llama/llama', 'https://github.com/localstack/localstack', 'https://github.com/zylon-ai/private-gpt', 'https://github.com/ageitgey/face_recognition', 'https://github.com/sherlock-project/sherlock', 'https://github.com/psf/requests', 'https://github.com/scrapy/scrapy', 'https://github.com/CorentinJ/Real-Time-Voice-Cloning', 'https://github.com/gpt-engineer-org/gpt-engineer', 'https://github.com/abi/screenshot-to-code', 'https://github.com/deepfakes/faceswap', 'https://github.com/soimort/you-get', 'https://github.com/OpenInterpreter/open-interpreter', 'https://github.com/xai-org/grok-1', 'https://github.com/commaai/openpilot', 'https://github.com/Textualize/rich', 'https://github.com/ultralytics/yolov5', 'https://github.com/minimaxir/big-list-of-naughty-strings', 'https://github.com/iperov/DeepFaceLab', 'https://github.com/charlax/professional-programming', 'https://github.com/Z4nzu/hackingtool', 'https://github.com/pandas-dev/pandas', 'https://github.com/isocpp/CppCoreGuidelines', 'https://github.com/geekan/MetaGPT', 'https://github.com/faif/python-patterns', 'https://github.com/THUDM/ChatGLM-6B', 'https://github.com/PaddlePaddle/PaddleOCR', 'https://github.com/apachecn/ailearning', 'https://github.com/hpcaitech/ColossalAI', 'https://github.com/chubin/cheat.sh', 'https://github.com/psf/black', 'https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap', 'https://github.com/google-research/bert', 'https://github.com/getsentry/sentry', 'https://github.com/oobabooga/text-generation-webui', 'https://github.com/LAION-AI/Open-Assistant', 'https://github.com/Stability-AI/stablediffusion', 'https://github.com/0voice/interview_internal_reference', 'https://github.com/gto76/python-cheatsheet', 'https://github.com/lllyasviel/Fooocus', 'https://github.com/XingangPan/DragGAN', 'https://github.com/satwikkansal/wtfpython', 'https://github.com/mingrammer/diagrams', 'https://github.com/odoo/odoo', 'https://github.com/TencentARC/GFPGAN', 'https://github.com/apache/airflow', 'https://github.com/chenfei-wu/TaskMatrix', 'https://github.com/mitmproxy/mitmproxy', 'https://github.com/lm-sys/FastChat', 'https://github.com/comfyanonymous/ComfyUI', 'https://github.com/babysor/MockingBird', 'https://github.com/openai/gym', 'https://github.com/testerSunshine/12306', 'https://github.com/shadowsocks/shadowsocks', 'https://github.com/microsoft/DeepSpeed', 'https://github.com/XX-net/XX-Net', 'https://github.com/fxsjy/jieba', 'https://github.com/hankcs/HanLP', 'https://github.com/Asabeneh/30-Days-Of-Python', 'https://github.com/karpathy/nanoGPT', 'https://github.com/httpie/cli', 'https://github.com/streamlit/streamlit', 'https://github.com/ccxt/ccxt', 'https://github.com/run-llama/llama_index', 'https://github.com/ray-project/ray', 'https://github.com/certbot/certbot', 'https://github.com/sqlmapproject/sqlmap', 'https://github.com/geekcomputers/Python', 'https://github.com/huggingface/pytorch-image-models', 'https://github.com/coqui-ai/TTS', 'https://github.com/python-poetry/poetry', 'https://github.com/0xAX/linux-insides', 'https://github.com/facebookresearch/fairseq', 'https://github.com/gradio-app/gradio', 'https://github.com/yunjey/pytorch-tutorial', 'https://github.com/tatsu-lab/stanford_alpaca', 'https://github.com/explosion/spaCy', 'https://github.com/donnemartin/interactive-coding-challenges', 'https://github.com/facebookresearch/detectron2', 'https://github.com/Pythagora-io/gpt-pilot', 'https://github.com/google/jax', 'https://github.com/lllyasviel/ControlNet', 'https://github.com/acheong08/ChatGPT', 'https://github.com/open-mmlab/mmdetection', 'https://github.com/chatchat-space/Langchain-Chatchat', 'https://github.com/encode/django-rest-framework', 'https://github.com/tqdm/tqdm', 'https://github.com/Lightning-AI/pytorch-lightning', 'https://github.com/LC044/WeChatMsg', 'https://github.com/OWASP/CheatSheetSeries', 'https://github.com/donnemartin/data-science-ipython-notebooks', 'https://github.com/numpy/numpy', 'https://github.com/google/python-fire', 'https://github.com/xinntao/Real-ESRGAN', 'https://github.com/OpenBB-finance/OpenBBTerminal', 'https://github.com/facebookresearch/Detectron', 'https://github.com/freqtrade/freqtrade', 'https://github.com/StevenBlack/hosts', 'https://github.com/ycm-core/YouCompleteMe', 'https://github.com/spipm/Depix', 'https://github.com/zhayujie/chatgpt-on-wechat', 'https://github.com/littlecodersh/ItChat', 'https://github.com/nicolargo/glances', 'https://github.com/s0md3v/roop', 'https://github.com/getredash/redash', 'https://github.com/deezer/spleeter', 'https://github.com/Vision-CAIR/MiniGPT-4', 'https://github.com/python-telegram-bot/python-telegram-bot', 'https://github.com/pypa/pipenv', 'https://github.com/myshell-ai/OpenVoice', 'https://github.com/OpenDevin/OpenDevin', 'https://github.com/microsoft/cascadia-code', 'https://github.com/matterport/Mask_RCNN', 'https://github.com/tinygrad/tinygrad', 'https://github.com/svc-develop-team/so-vits-svc', 'https://github.com/RVC-Boss/GPT-SoVITS', 'https://github.com/jumpserver/jumpserver', 'https://github.com/locustio/locust', 'https://github.com/chubin/wttr.in', 'https://github.com/Textualize/textual', 'https://github.com/celery/celery', 'https://github.com/keon/algorithms', 'https://github.com/vnpy/vnpy', 'https://github.com/iperov/DeepFaceLive', 'https://github.com/ultralytics/ultralytics', 'https://github.com/eriklindernoren/ML-From-Scratch', 'https://github.com/microsoft/JARVIS', 'https://github.com/huggingface/diffusers', 'https://github.com/wangzheng0822/algo', 'https://github.com/mouredev/Hello-Python', 'https://github.com/Stability-AI/generative-models', 'https://github.com/sebastianruder/NLP-progress', 'https://github.com/JaidedAI/EasyOCR', 'https://github.com/kovidgoyal/kitty', 'https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix', 'https://github.com/HumanSignal/labelImg', 'https://github.com/d2l-ai/d2l-en', 'https://github.com/AtsushiSakai/PythonRobotics', 'https://github.com/pytorch/examples', 'https://github.com/cookiecutter/cookiecutter', 'https://github.com/tornadoweb/tornado', 'https://github.com/hiyouga/LLaMA-Factory', 'https://github.com/mindsdb/mindsdb', 'https://github.com/deepinsight/insightface', 'https://github.com/openai/gpt-2', 'https://github.com/luong-komorebi/Awesome-Linux-Software', 'https://github.com/WZMIAOMIAO/deep-learning-for-image-processing', 'https://github.com/drduh/macOS-Security-and-Privacy-Guide', 'https://github.com/openai/chatgpt-retrieval-plugin', 'https://github.com/plotly/dash', 'https://github.com/chriskiehl/Gooey', 'https://github.com/jhao104/proxy_pool', 'https://github.com/pyg-team/pytorch_geometric', 'https://github.com/saleor/saleor', 'https://github.com/zulip/zulip', 'https://github.com/jina-ai/jina', 'https://github.com/openai/openai-python', 'https://github.com/KurtBestor/Hitomi-Downloader', 'https://github.com/521xueweihan/GitHub520', 'https://github.com/ArchiveBox/ArchiveBox', 'https://github.com/facebookresearch/audiocraft', 'https://github.com/meta-llama/llama3', 'https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI', 'https://github.com/matplotlib/matplotlib', 'https://github.com/yoheinakajima/babyagi', 'https://github.com/Vonng/ddia', 'https://github.com/PromtEngineer/localGPT', 'https://github.com/vllm-project/vllm', 'https://github.com/ManimCommunity/manim', 'https://github.com/ungoogled-software/ungoogled-chromium', 'https://github.com/karpathy/minGPT', 'https://github.com/magenta/magenta', 'https://github.com/bokeh/bokeh', 'https://github.com/pydantic/pydantic', 'https://github.com/huggingface/datasets', 'https://github.com/microsoft/unilm', 'https://github.com/kholia/OSX-KVM', 'https://github.com/kovidgoyal/calibre', 'https://github.com/mkdocs/mkdocs', 'https://github.com/magic-wormhole/magic-wormhole', 'https://github.com/Delgan/loguru', 'https://github.com/lucidrains/vit-pytorch', 'https://github.com/nginx-proxy/nginx-proxy', 'https://github.com/recommenders-team/recommenders', 'https://github.com/RasaHQ/rasa', 'https://github.com/facebook/prophet', 'https://github.com/sanic-org/sanic', 'https://github.com/kaixindelele/ChatPaper', 'https://github.com/Jack-Cherish/python-spider', 'https://github.com/jantic/DeOldify', 'https://github.com/python/mypy', 'https://github.com/ymcui/Chinese-LLaMA-Alpaca', 'https://github.com/pyscript/pyscript', 'https://github.com/PostHog/posthog', 'https://github.com/mlflow/mlflow', 'https://github.com/spotify/luigi', 'https://github.com/wagtail/wagtail', 'https://github.com/Sanster/IOPaint', 'https://github.com/miloyip/game-programmer', 'https://github.com/joke2k/faker', 'https://github.com/mlc-ai/mlc-llm', 'https://github.com/Ciphey/Ciphey', 'https://github.com/quantopian/zipline', 'https://github.com/paperless-ngx/paperless-ngx', 'https://github.com/frappe/erpnext', 'https://github.com/stitionai/devika', 'https://github.com/rsms/inter', 'https://github.com/kivy/kivy', 'https://github.com/reflex-dev/reflex', 'https://github.com/onnx/onnx', 'https://github.com/reddit-archive/reddit', 'https://github.com/hpcaitech/Open-Sora', 'https://github.com/haotian-liu/LLaVA', 'https://github.com/chatanywhere/GPT_API_free', 'https://github.com/InstaPy/InstaPy', 'https://github.com/binux/pyspider', 'https://github.com/LiLittleCat/awesome-free-chatgpt', 'https://github.com/cool-RR/PySnooper', 'https://github.com/apple/ml-stable-diffusion', 'https://github.com/ipython/ipython', 'https://github.com/wilsonfreitas/awesome-quant', 'https://github.com/alievk/avatarify-python', 'https://github.com/Mikubill/sd-webui-controlnet', 'https://github.com/wting/autojump', 'https://github.com/trekhleb/learn-python', 'https://github.com/eriklindernoren/PyTorch-GAN', 'https://github.com/Kr1s77/awesome-python-login-model', 'https://github.com/twintproject/twint', 'https://github.com/THUDM/ChatGLM2-6B', 'https://github.com/wistbean/learn_python3_spider', 'https://github.com/mnielsen/neural-networks-and-deep-learning', 'https://github.com/pytorch/vision', 'https://github.com/h2y/Shadowrocket-ADBlock-Rules', 'https://github.com/OpenEthan/SMSBoom', 'https://github.com/openai/baselines', 'https://github.com/plotly/plotly.py', 'https://github.com/piskvorky/gensim', 'https://github.com/RunaCapital/awesome-oss-alternatives', 'https://github.com/meta-llama/codellama', 'https://github.com/pallets/click', 'https://github.com/spotDL/spotify-downloader', 'https://github.com/dgtlmoon/changedetection.io', 'https://github.com/Anjok07/ultimatevocalremovergui', 'https://github.com/netbox-community/netbox', 'https://github.com/mxrch/GHunt', 'https://github.com/ranger/ranger', 'https://github.com/tensorflow/tensor2tensor', 'https://github.com/aws/aws-cli', 'https://github.com/blakeblackshear/frigate', 'https://github.com/w-okada/voice-changer', 'https://github.com/GaiZhenbiao/ChuanhuChatGPT', 'https://github.com/PrefectHQ/prefect', 'https://github.com/jupyter/jupyter', 'https://github.com/facefusion/facefusion', 'https://github.com/danielgatis/rembg', 'https://github.com/borisdayma/dalle-mini', 'https://github.com/fabric/fabric', 'https://github.com/aio-libs/aiohttp', 'https://github.com/ddbourgin/numpy-ml', 'https://github.com/TransformerOptimus/SuperAGI', 'https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life', 'https://github.com/pyecharts/pyecharts', 'https://github.com/tiangolo/typer', 'https://github.com/Rapptz/discord.py', 'https://github.com/fauxpilot/fauxpilot', 'https://github.com/lra/mackup', 'https://github.com/apprenticeharper/DeDRM_tools', 'https://github.com/microsoft/qlib', 'https://github.com/networkx/networkx', 'https://github.com/powerline/powerline', 'https://github.com/arc53/DocsGPT', 'https://github.com/Python-World/python-mini-projects', 'https://github.com/airbytehq/airbyte', 'https://github.com/aleju/imgaug', 'https://github.com/roboflow/supervision', 'https://github.com/pjialin/py12306', 'https://github.com/hindupuravinash/the-gan-zoo', 'https://github.com/unifyai/ivy', 'https://github.com/openai/evals', 'https://github.com/horovod/horovod', 'https://github.com/huggingface/peft', 'https://github.com/NVlabs/stylegan', 'https://github.com/tgbot-collection/YYeTsBot', 'https://github.com/gunthercox/ChatterBot', 'https://github.com/UKPLab/sentence-transformers', 'https://github.com/saltstack/salt', 'https://github.com/wangshub/wechat_jump_game', 'https://github.com/youfou/wxpy', 'https://github.com/microsoft/nni', 'https://github.com/deepset-ai/haystack', 'https://github.com/codelucas/newspaper', 'https://github.com/joaomdmoura/crewAI', 'https://github.com/google/yapf', 'https://github.com/psf/requests-html', 'https://github.com/flairNLP/flair', 'https://github.com/sczhou/CodeFormer', 'https://github.com/shengqiangzhang/examples-of-web-crawlers', 'https://github.com/davidsandberg/facenet', 'https://github.com/NanmiCoder/MediaCrawler', 'https://github.com/ansible/awx', 'https://github.com/albumentations-team/albumentations', 'https://github.com/programthink/zhao', 'https://github.com/mail-in-a-box/mailinabox', 'https://github.com/sivel/speedtest-cli', 'https://github.com/searx/searx', 'https://github.com/ShangtongZhang/reinforcement-learning-an-introduction', 'https://github.com/iterative/dvc', 'https://github.com/PySimpleGUI/PySimpleGUI', 'https://github.com/mementum/backtrader', 'https://github.com/tiangolo/sqlmodel', 'https://github.com/nltk/nltk', 'https://github.com/dmlc/dgl', 'https://github.com/microsoft/Swin-Transformer', 'https://github.com/jindongwang/transferlearning', 'https://github.com/facebookresearch/detr', 'https://github.com/idank/explainshell', 'https://github.com/s0md3v/XSStrike', 'https://github.com/fortra/impacket', 'https://github.com/MetaCubeX/mihomo', 'https://github.com/wifiphisher/wifiphisher', 'https://github.com/jaakkopasanen/AutoEq', 'https://github.com/waditu/tushare', 'https://github.com/edgedb/edgedb', 'https://github.com/bloomberg/memray', 'https://github.com/ethereum/EIPs', 'https://github.com/PaddlePaddle/PaddleHub', 'https://github.com/scipy/scipy', 'https://github.com/chroma-core/chroma', 'https://github.com/sympy/sympy', 'https://github.com/beetbox/beets', 'https://github.com/postmanlabs/httpbin', 'https://github.com/labelmeai/labelme', 'https://github.com/SFTtech/openage', 'https://github.com/encode/httpx', 'https://github.com/redis/redis-py', 'https://github.com/getpelican/pelican', 'https://github.com/THUDM/ChatGLM3', 'https://github.com/jina-ai/clip-as-service', 'https://github.com/donnemartin/awesome-aws', 'https://github.com/microsoft/pyright', 'https://github.com/pre-commit/pre-commit', 'https://github.com/PaddlePaddle/PaddleDetection', 'https://github.com/ocrmypdf/OCRmyPDF', 'https://github.com/lss233/chatgpt-mirai-qq-bot', 'https://github.com/ydataai/ydata-profiling', 'https://github.com/dask/dask', 'https://github.com/mwaskom/seaborn', 'https://github.com/ranaroussi/yfinance', 'https://github.com/tonybeltramelli/pix2code', 'https://github.com/threat9/routersploit', 'https://github.com/Miserlou/Zappa', 'https://github.com/alexjc/neural-enhance', 'https://github.com/Zulko/moviepy', 'https://github.com/meolu/walle-web', 'https://github.com/OpenMOSS/MOSS', 'https://github.com/smicallef/spiderfoot', 'https://github.com/matrix-org/synapse', 'https://github.com/google-deepmind/alphafold', 'https://github.com/dbcli/pgcli', 'https://github.com/python-pillow/Pillow', 'https://github.com/BlinkDL/RWKV-LM', 'https://github.com/allenai/allennlp', 'https://github.com/LlamaFamily/Llama-Chinese', 'https://github.com/smol-ai/developer', 'https://github.com/janeczku/calibre-web', 'https://github.com/Embedding/Chinese-Word-Vectors', 'https://github.com/cookiecutter/cookiecutter-django', 'https://github.com/rougier/numpy-100', 'https://github.com/zalandoresearch/fashion-mnist']\n",
    "github_urls = ['https://github.com/TencentARC/GFPGAN', 'https://github.com/apache/airflow', 'https://github.com/chenfei-wu/TaskMatrix', 'https://github.com/mitmproxy/mitmproxy', 'https://github.com/lm-sys/FastChat', 'https://github.com/comfyanonymous/ComfyUI', 'https://github.com/babysor/MockingBird', 'https://github.com/openai/gym', 'https://github.com/testerSunshine/12306', 'https://github.com/shadowsocks/shadowsocks', 'https://github.com/microsoft/DeepSpeed', 'https://github.com/XX-net/XX-Net', 'https://github.com/fxsjy/jieba', 'https://github.com/hankcs/HanLP', 'https://github.com/Asabeneh/30-Days-Of-Python', 'https://github.com/karpathy/nanoGPT', 'https://github.com/httpie/cli', 'https://github.com/streamlit/streamlit', 'https://github.com/ccxt/ccxt', 'https://github.com/run-llama/llama_index', 'https://github.com/ray-project/ray', 'https://github.com/certbot/certbot', 'https://github.com/sqlmapproject/sqlmap', 'https://github.com/geekcomputers/Python', 'https://github.com/huggingface/pytorch-image-models', 'https://github.com/coqui-ai/TTS', 'https://github.com/python-poetry/poetry', 'https://github.com/0xAX/linux-insides', 'https://github.com/facebookresearch/fairseq', 'https://github.com/gradio-app/gradio', 'https://github.com/yunjey/pytorch-tutorial', 'https://github.com/tatsu-lab/stanford_alpaca', 'https://github.com/explosion/spaCy', 'https://github.com/donnemartin/interactive-coding-challenges', 'https://github.com/facebookresearch/detectron2', 'https://github.com/Pythagora-io/gpt-pilot', 'https://github.com/google/jax', 'https://github.com/lllyasviel/ControlNet', 'https://github.com/acheong08/ChatGPT', 'https://github.com/open-mmlab/mmdetection', 'https://github.com/chatchat-space/Langchain-Chatchat', 'https://github.com/encode/django-rest-framework', 'https://github.com/tqdm/tqdm', 'https://github.com/Lightning-AI/pytorch-lightning', 'https://github.com/LC044/WeChatMsg', 'https://github.com/OWASP/CheatSheetSeries', 'https://github.com/donnemartin/data-science-ipython-notebooks', 'https://github.com/numpy/numpy', 'https://github.com/google/python-fire', 'https://github.com/xinntao/Real-ESRGAN', 'https://github.com/OpenBB-finance/OpenBBTerminal', 'https://github.com/facebookresearch/Detectron', 'https://github.com/freqtrade/freqtrade', 'https://github.com/StevenBlack/hosts', 'https://github.com/ycm-core/YouCompleteMe', 'https://github.com/spipm/Depix', 'https://github.com/zhayujie/chatgpt-on-wechat', 'https://github.com/littlecodersh/ItChat', 'https://github.com/nicolargo/glances', 'https://github.com/s0md3v/roop', 'https://github.com/getredash/redash', 'https://github.com/deezer/spleeter', 'https://github.com/Vision-CAIR/MiniGPT-4', 'https://github.com/python-telegram-bot/python-telegram-bot', 'https://github.com/pypa/pipenv', 'https://github.com/myshell-ai/OpenVoice', 'https://github.com/OpenDevin/OpenDevin', 'https://github.com/microsoft/cascadia-code', 'https://github.com/matterport/Mask_RCNN', 'https://github.com/tinygrad/tinygrad', 'https://github.com/svc-develop-team/so-vits-svc', 'https://github.com/RVC-Boss/GPT-SoVITS', 'https://github.com/jumpserver/jumpserver', 'https://github.com/locustio/locust', 'https://github.com/chubin/wttr.in', 'https://github.com/Textualize/textual', 'https://github.com/celery/celery', 'https://github.com/keon/algorithms', 'https://github.com/vnpy/vnpy', 'https://github.com/iperov/DeepFaceLive', 'https://github.com/ultralytics/ultralytics', 'https://github.com/eriklindernoren/ML-From-Scratch', 'https://github.com/microsoft/JARVIS', 'https://github.com/huggingface/diffusers', 'https://github.com/wangzheng0822/algo', 'https://github.com/mouredev/Hello-Python', 'https://github.com/Stability-AI/generative-models', 'https://github.com/sebastianruder/NLP-progress', 'https://github.com/JaidedAI/EasyOCR', 'https://github.com/kovidgoyal/kitty', 'https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix', 'https://github.com/HumanSignal/labelImg', 'https://github.com/d2l-ai/d2l-en', 'https://github.com/AtsushiSakai/PythonRobotics', 'https://github.com/pytorch/examples', 'https://github.com/cookiecutter/cookiecutter', 'https://github.com/tornadoweb/tornado', 'https://github.com/hiyouga/LLaMA-Factory', 'https://github.com/mindsdb/mindsdb', 'https://github.com/deepinsight/insightface', 'https://github.com/openai/gpt-2', 'https://github.com/luong-komorebi/Awesome-Linux-Software', 'https://github.com/WZMIAOMIAO/deep-learning-for-image-processing', 'https://github.com/drduh/macOS-Security-and-Privacy-Guide', 'https://github.com/openai/chatgpt-retrieval-plugin', 'https://github.com/plotly/dash', 'https://github.com/chriskiehl/Gooey', 'https://github.com/jhao104/proxy_pool', 'https://github.com/pyg-team/pytorch_geometric', 'https://github.com/saleor/saleor', 'https://github.com/zulip/zulip', 'https://github.com/jina-ai/jina', 'https://github.com/openai/openai-python', 'https://github.com/KurtBestor/Hitomi-Downloader', 'https://github.com/521xueweihan/GitHub520', 'https://github.com/ArchiveBox/ArchiveBox', 'https://github.com/facebookresearch/audiocraft', 'https://github.com/meta-llama/llama3', 'https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI', 'https://github.com/matplotlib/matplotlib', 'https://github.com/yoheinakajima/babyagi', 'https://github.com/Vonng/ddia', 'https://github.com/PromtEngineer/localGPT', 'https://github.com/vllm-project/vllm', 'https://github.com/ManimCommunity/manim', 'https://github.com/ungoogled-software/ungoogled-chromium', 'https://github.com/karpathy/minGPT', 'https://github.com/magenta/magenta', 'https://github.com/bokeh/bokeh', 'https://github.com/pydantic/pydantic', 'https://github.com/huggingface/datasets', 'https://github.com/microsoft/unilm', 'https://github.com/kholia/OSX-KVM', 'https://github.com/kovidgoyal/calibre', 'https://github.com/mkdocs/mkdocs', 'https://github.com/magic-wormhole/magic-wormhole', 'https://github.com/Delgan/loguru', 'https://github.com/lucidrains/vit-pytorch', 'https://github.com/nginx-proxy/nginx-proxy', 'https://github.com/recommenders-team/recommenders', 'https://github.com/RasaHQ/rasa', 'https://github.com/facebook/prophet', 'https://github.com/sanic-org/sanic', 'https://github.com/kaixindelele/ChatPaper', 'https://github.com/Jack-Cherish/python-spider', 'https://github.com/jantic/DeOldify', 'https://github.com/python/mypy', 'https://github.com/ymcui/Chinese-LLaMA-Alpaca', 'https://github.com/pyscript/pyscript', 'https://github.com/PostHog/posthog', 'https://github.com/mlflow/mlflow', 'https://github.com/spotify/luigi', 'https://github.com/wagtail/wagtail', 'https://github.com/Sanster/IOPaint', 'https://github.com/miloyip/game-programmer', 'https://github.com/joke2k/faker', 'https://github.com/mlc-ai/mlc-llm', 'https://github.com/Ciphey/Ciphey', 'https://github.com/quantopian/zipline', 'https://github.com/paperless-ngx/paperless-ngx', 'https://github.com/frappe/erpnext', 'https://github.com/stitionai/devika', 'https://github.com/rsms/inter', 'https://github.com/kivy/kivy', 'https://github.com/reflex-dev/reflex', 'https://github.com/onnx/onnx', 'https://github.com/reddit-archive/reddit', 'https://github.com/hpcaitech/Open-Sora', 'https://github.com/haotian-liu/LLaVA', 'https://github.com/chatanywhere/GPT_API_free', 'https://github.com/InstaPy/InstaPy', 'https://github.com/binux/pyspider', 'https://github.com/LiLittleCat/awesome-free-chatgpt', 'https://github.com/cool-RR/PySnooper', 'https://github.com/apple/ml-stable-diffusion', 'https://github.com/ipython/ipython', 'https://github.com/wilsonfreitas/awesome-quant', 'https://github.com/alievk/avatarify-python', 'https://github.com/Mikubill/sd-webui-controlnet', 'https://github.com/wting/autojump', 'https://github.com/trekhleb/learn-python', 'https://github.com/eriklindernoren/PyTorch-GAN', 'https://github.com/Kr1s77/awesome-python-login-model', 'https://github.com/twintproject/twint', 'https://github.com/THUDM/ChatGLM2-6B', 'https://github.com/wistbean/learn_python3_spider', 'https://github.com/mnielsen/neural-networks-and-deep-learning', 'https://github.com/pytorch/vision', 'https://github.com/h2y/Shadowrocket-ADBlock-Rules', 'https://github.com/OpenEthan/SMSBoom', 'https://github.com/openai/baselines', 'https://github.com/plotly/plotly.py', 'https://github.com/piskvorky/gensim', 'https://github.com/RunaCapital/awesome-oss-alternatives', 'https://github.com/meta-llama/codellama', 'https://github.com/pallets/click', 'https://github.com/spotDL/spotify-downloader', 'https://github.com/dgtlmoon/changedetection.io', 'https://github.com/Anjok07/ultimatevocalremovergui', 'https://github.com/netbox-community/netbox', 'https://github.com/mxrch/GHunt', 'https://github.com/ranger/ranger', 'https://github.com/tensorflow/tensor2tensor', 'https://github.com/aws/aws-cli', 'https://github.com/blakeblackshear/frigate', 'https://github.com/w-okada/voice-changer', 'https://github.com/GaiZhenbiao/ChuanhuChatGPT', 'https://github.com/PrefectHQ/prefect', 'https://github.com/jupyter/jupyter', 'https://github.com/facefusion/facefusion', 'https://github.com/danielgatis/rembg', 'https://github.com/borisdayma/dalle-mini', 'https://github.com/fabric/fabric', 'https://github.com/aio-libs/aiohttp', 'https://github.com/ddbourgin/numpy-ml', 'https://github.com/TransformerOptimus/SuperAGI', 'https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life', 'https://github.com/pyecharts/pyecharts', 'https://github.com/tiangolo/typer', 'https://github.com/Rapptz/discord.py', 'https://github.com/fauxpilot/fauxpilot', 'https://github.com/lra/mackup', 'https://github.com/apprenticeharper/DeDRM_tools', 'https://github.com/microsoft/qlib', 'https://github.com/networkx/networkx', 'https://github.com/powerline/powerline', 'https://github.com/arc53/DocsGPT', 'https://github.com/Python-World/python-mini-projects', 'https://github.com/airbytehq/airbyte', 'https://github.com/aleju/imgaug', 'https://github.com/roboflow/supervision', 'https://github.com/pjialin/py12306', 'https://github.com/hindupuravinash/the-gan-zoo', 'https://github.com/unifyai/ivy', 'https://github.com/openai/evals', 'https://github.com/horovod/horovod', 'https://github.com/huggingface/peft', 'https://github.com/NVlabs/stylegan', 'https://github.com/tgbot-collection/YYeTsBot', 'https://github.com/gunthercox/ChatterBot', 'https://github.com/UKPLab/sentence-transformers', 'https://github.com/saltstack/salt', 'https://github.com/wangshub/wechat_jump_game', 'https://github.com/youfou/wxpy', 'https://github.com/microsoft/nni', 'https://github.com/deepset-ai/haystack', 'https://github.com/codelucas/newspaper', 'https://github.com/joaomdmoura/crewAI', 'https://github.com/google/yapf', 'https://github.com/psf/requests-html', 'https://github.com/flairNLP/flair', 'https://github.com/sczhou/CodeFormer', 'https://github.com/shengqiangzhang/examples-of-web-crawlers', 'https://github.com/davidsandberg/facenet', 'https://github.com/NanmiCoder/MediaCrawler', 'https://github.com/ansible/awx', 'https://github.com/albumentations-team/albumentations', 'https://github.com/programthink/zhao', 'https://github.com/mail-in-a-box/mailinabox', 'https://github.com/sivel/speedtest-cli', 'https://github.com/searx/searx', 'https://github.com/ShangtongZhang/reinforcement-learning-an-introduction', 'https://github.com/iterative/dvc', 'https://github.com/PySimpleGUI/PySimpleGUI', 'https://github.com/mementum/backtrader', 'https://github.com/tiangolo/sqlmodel', 'https://github.com/nltk/nltk', 'https://github.com/dmlc/dgl', 'https://github.com/microsoft/Swin-Transformer', 'https://github.com/jindongwang/transferlearning', 'https://github.com/facebookresearch/detr', 'https://github.com/idank/explainshell', 'https://github.com/s0md3v/XSStrike', 'https://github.com/fortra/impacket', 'https://github.com/MetaCubeX/mihomo', 'https://github.com/wifiphisher/wifiphisher', 'https://github.com/jaakkopasanen/AutoEq', 'https://github.com/waditu/tushare', 'https://github.com/edgedb/edgedb', 'https://github.com/bloomberg/memray', 'https://github.com/ethereum/EIPs', 'https://github.com/PaddlePaddle/PaddleHub', 'https://github.com/scipy/scipy', 'https://github.com/chroma-core/chroma', 'https://github.com/sympy/sympy', 'https://github.com/beetbox/beets', 'https://github.com/postmanlabs/httpbin', 'https://github.com/labelmeai/labelme', 'https://github.com/SFTtech/openage', 'https://github.com/encode/httpx', 'https://github.com/redis/redis-py', 'https://github.com/getpelican/pelican', 'https://github.com/THUDM/ChatGLM3', 'https://github.com/jina-ai/clip-as-service', 'https://github.com/donnemartin/awesome-aws', 'https://github.com/microsoft/pyright', 'https://github.com/pre-commit/pre-commit', 'https://github.com/PaddlePaddle/PaddleDetection', 'https://github.com/ocrmypdf/OCRmyPDF', 'https://github.com/lss233/chatgpt-mirai-qq-bot', 'https://github.com/ydataai/ydata-profiling', 'https://github.com/dask/dask', 'https://github.com/mwaskom/seaborn', 'https://github.com/ranaroussi/yfinance', 'https://github.com/tonybeltramelli/pix2code', 'https://github.com/threat9/routersploit', 'https://github.com/Miserlou/Zappa', 'https://github.com/alexjc/neural-enhance', 'https://github.com/Zulko/moviepy', 'https://github.com/meolu/walle-web', 'https://github.com/OpenMOSS/MOSS', 'https://github.com/smicallef/spiderfoot', 'https://github.com/matrix-org/synapse', 'https://github.com/google-deepmind/alphafold', 'https://github.com/dbcli/pgcli', 'https://github.com/python-pillow/Pillow', 'https://github.com/BlinkDL/RWKV-LM', 'https://github.com/allenai/allennlp', 'https://github.com/LlamaFamily/Llama-Chinese', 'https://github.com/smol-ai/developer', 'https://github.com/janeczku/calibre-web', 'https://github.com/Embedding/Chinese-Word-Vectors', 'https://github.com/cookiecutter/cookiecutter-django', 'https://github.com/rougier/numpy-100', 'https://github.com/zalandoresearch/fashion-mnist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d9f1f-de7c-4de6-a4d5-605cbda7875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store CSV files\n",
    "output_dir = \"github_repo_source_code\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for url in github_urls:\n",
    "    try:\n",
    "        repo_name, concatenated_code = collect_source_code(url)\n",
    "        txt_file_name = f\"{repo_name}.txt\"\n",
    "        txt_file_name = os.path.join(output_dir, txt_file_name)\n",
    "        with open(txt_file_name, 'w', encoding='utf-8') as txt_file:\n",
    "            txt_file.write(concatenated_code)\n",
    "        print(f\"Successfully processed and saved {url} to {txt_file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "print(\"All repositories processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2313a",
   "metadata": {},
   "source": [
    "# HNSWLIB Context Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hnswlib sentence_transformers langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905a519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "def get_context(sentences, embeds, question_embed):\n",
    "    dim = embeds.shape[1]\n",
    "    num_elements = embeds.shape[0]\n",
    "\n",
    "    # Generating sample data\n",
    "    data = embeds\n",
    "    ids = np.arange(num_elements)\n",
    "\n",
    "    # Declaring index\n",
    "    p = hnswlib.Index(space = 'cosine', dim = dim) # possible options are l2, cosine or ip\n",
    "\n",
    "    # Initializing index - the maximum number of elements should be known beforehand\n",
    "    p.init_index(max_elements = num_elements, ef_construction = 200, M = 16)\n",
    "\n",
    "    # Element insertion (can be called several times):\n",
    "    p.add_items(data, ids)\n",
    "\n",
    "    # Controlling the recall by setting ef:\n",
    "    p.set_ef(50) # ef should always be > k\n",
    "\n",
    "    # Query dataset, k - number of the closest elements (returns 2 numpy arrays)\n",
    "    labels, distances = p.knn_query(question_embed, k = 4)\n",
    "\n",
    "    return \"\".join([sentences[index] for index in labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c04ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "root_dir = \"./\"\n",
    "context_root_dir = \"./github_repo_source_code/\"\n",
    "readme_root_dir = \"./output_csv_files/\"\n",
    "\n",
    "with open('./repo_urls.pickle', 'rb') as f:\n",
    "    repo_name_list = pickle.load(f)\n",
    "\n",
    "new_rows = []\n",
    "for repo in repo_name_list:\n",
    "    repo_name = repo.split(\"/\")[-1]\n",
    "    file1 = repo_name +\".txt\"\n",
    "    with open(os.path.join(context_root_dir, file1)) as f:\n",
    "        data = f.read()\n",
    "    sentences = text_splitter.split_text(data)\n",
    "    embeddings = model.encode(sentences)\n",
    "    print(embeddings.shape)\n",
    "\n",
    "    file2 = repo_name +\".csv\"\n",
    "    df2 = pd.read_csv(os.path.join(readme_root_dir, file2))\n",
    "    for i, row in df2.iterrows():\n",
    "        title = row[\"Title\"]\n",
    "        content = row[\"Content\"]\n",
    "        if \"?\" in title:\n",
    "            question = f\"In context to the project {repo_name}, answer the following. \" + title\n",
    "            question_embedding = model.encode([question])\n",
    "            context = get_context(sentences, embeddings, question_embedding)\n",
    "            new_row  = {\"Question\": question, \"Context\": context, \"Answer\": content, \"Repo Url\": repo, \"Repo\": repo_name}\n",
    "            new_rows.append(new_row)\n",
    "        else:\n",
    "            question = f\"Provide the README content for the section with heading \\\"{title}\\\" starting with ## {title}.\"\n",
    "            question_embedding = model.encode([question])\n",
    "            context = get_context(sentences, embeddings, question_embedding)\n",
    "            new_row  = {\"Question\": question, \"Context\": context, \"Answer\": content, \"Repo Url\": repo, \"Repo\": repo_name}\n",
    "            new_rows.append(new_row)\n",
    "    print(len(new_rows))\n",
    "    df3 = pd.DataFrame(new_rows, index=None)\n",
    "    df3.to_csv(os.path.join(root_dir, \"readme_qa.csv\"), mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7595e4",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62492c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "  \"\"\"Remove URLs from a given text string.\"\"\"\n",
    "  url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "  return re.sub(url_pattern, '', text)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "  \"\"\"Remove HTML tags from a given text string.\"\"\"\n",
    "  html_pattern = r'<.*?>'\n",
    "  return re.sub(html_pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21ef45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Define the regular expression pattern for HTTP URLs\n",
    "    http_pattern = re.compile(r'http://[^\\s]+')\n",
    "    # Remove HTTP URLs\n",
    "    text = http_pattern.sub('', str(text))\n",
    "\n",
    "    https_pattern = re.compile(r'https://[^\\s]+')\n",
    "    # Remove HTTPS URLs\n",
    "    text = https_pattern.sub('', str(text))\n",
    "    \n",
    "    # Define the regular expression pattern for <img> tags\n",
    "    img_pattern = re.compile(r'<img[^>]*>')\n",
    "    # Remove <img> tags\n",
    "    text = img_pattern.sub('', str(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8cc5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_emoji(tx):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols \n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport \n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'', tx)\n",
    "\n",
    "def text_cleaner(tx):\n",
    "\n",
    "    text = re.sub(r\"won\\'t\", \"would not\", tx)\n",
    "    text = re.sub(r\"im\", \"i am\", tx)\n",
    "    text = re.sub(r\"Im\", \"I am\", tx)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"don\\'t\", \"do not\", text)\n",
    "    text = re.sub(r\"shouldn\\'t\", \"should not\", text)\n",
    "    text = re.sub(r\"needn\\'t\", \"need not\", text)\n",
    "    text = re.sub(r\"hasn\\'t\", \"has not\", text)\n",
    "    text = re.sub(r\"haven\\'t\", \"have not\", text)\n",
    "    text = re.sub(r\"weren\\'t\", \"were not\", text)\n",
    "    text = re.sub(r\"mightn\\'t\", \"might not\", text)\n",
    "    text = re.sub(r\"didn\\'t\", \"did not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    # text = re.sub('https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'https?://[^\\s\\\")]+', '', text)\n",
    "    text = re.sub(r'http?://[^\\s\\\")]+', '', text)\n",
    "    text = re.sub(r'http%3A%2F%2F[^\\s\\\")]+', '', text)\n",
    "    text = re.sub(r'https%3A%2F%2F[^\\s\\\")]+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\!\\?\\.\\@]',' ' , text)\n",
    "    text = re.sub(r'[!]+' , '!' , text)\n",
    "    text = re.sub(r'[?]+' , '?' , text)\n",
    "    text = re.sub(r'[.]+' , '.' , text)\n",
    "    text = re.sub(r'[@]+' , '@' , text)\n",
    "    text = re.sub(r'unk' , '<UNK>' , text)\n",
    "    # text = re.sub('\\n', '<NL>', text)\n",
    "    # text = re.sub('\\t', '<TAB>', text)\n",
    "    # text = re.sub(r'\\s+', '<SP>', text)\n",
    "    # text = re.sub(r'(<img[^>]*\\bsrc=\")[^\"]*(\")', '<img src=<IMG_SRC>', text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[ ]+' , ' ' , text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bb59731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"scripts/readme_qa.csv\")\n",
    "df.columns = [str(q).strip() for q in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9986405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Explore popular APIs and see them work in Postman. <br > <p> <a href=\"https://apilayer.com\"> <div> <img src=\".github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png\" width=\"250\" alt=\"APILayer Logo\" /> </div> </a> </p> [APILayer](https://apilayer.com/) is the fastest way to integrate APIs into any product. They created this repository to support the community in easily finding public APIs. Explore their collections on the [Postman API Network](https://www.postman.com/apilayer/workspace/apilayer/overview).',\n",
       "       '| API | Description | Call this API | |:---|:---|:---| | [IP Stack](https://ipstack.com/) | Locate and Identify Website Visitors by IP Address | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)| | [Marketstack](https://marketstack.com/) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)| | [Weatherstack](https://weatherstack.com/) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)| | [Numverify](https://numverify.com/) | Global Phone Number Validation & Lookup JSON API |[<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)| | [Fixer](https://fixer.io/) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)| <br >',\n",
       "       '| API | Description | Auth | Call this API | |:---|:---|:---|:---| | [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/25426789-12bc9867-e424-4de8-b4ee-662632714f6c?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D25426789-12bc9867-e424-4de8-b4ee-662632714f6c%26entityType%3Dcollection%26workspaceId%3De4d9a7d3-b961-474e-a054-51861ed481f6) | | [Sportmonks Football](https://docs.sportmonks.com/football/) | Football score/schedule, news API, tv channels, stats, history, display standing e.g. epl, la liga | `apiKey` | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/25426789-b21c360e-6b87-431d-9b39-74e824f29e45?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D25426789-b21c360e-6b87-431d-9b39-74e824f29e45%26entityType%3Dcollection%26workspaceId%3De4d9a7d3-b961-474e-a054-51861ed481f6)| | [Google Maps](https://developers.notion.com) | Create/customize digital maps based on Google Maps data | `apiKey`  | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/25426789-2c9bbe63-f45b-45d4-9327-ec3376542b64?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D25426789-2c9bbe63-f45b-45d4-9327-ec3376542b64%26entityType%3Dcollection%26workspaceId%3De4d9a7d3-b961-474e-a054-51861ed481f6)| | [Notion](https://developers.notion.com) | Integrate with Notion | `apiKey`  | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/25426789-68f0e9e4-b7bc-4543-945a-b50ae385c540?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D25426789-68f0e9e4-b7bc-4543-945a-b50ae385c540%26entityType%3Dcollection%26workspaceId%3De4d9a7d3-b961-474e-a054-51861ed481f6)| | [Plaid](https://www.plaid.com/docs) | Connect with user\\'s bank accounts and access transaction data\\t | `apiKey` | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/25426789-ae5e66eb-613e-4553-a99c-0f58d875ff88?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D25426789-ae5e66eb-613e-4553-a99c-0f58d875ff88%26entityType%3Dcollection%26workspaceId%3De4d9a7d3-b961-474e-a054-51861ed481f6)| <br >',\n",
       "       '* [Animals](#animals) * [Anime](#anime) * [Art & Design](#art--design) * [Authentication & Authorization](#authentication--authorization) * [Blockchain](#blockchain) * [Business](#business) * [Currency Exchange](#currency-exchange) * [Social](#social) <br >',\n",
       "       '<br > <strong>Get Involved</strong> * [Contributing Guide](CONTRIBUTING.md) * [API for this project](https://github.com/davemachado/public-api) * [Issues](https://github.com/public-apis/public-apis/issues) * [Pull Requests](https://github.com/public-apis/public-apis/pulls) * [LICENSE](LICENSE) Own an API? Publish your own [Run in Postman](https://learning.postman.com/docs/collaborating-in-postman/public-api-network/public-api-network-overview/) button. <br /> <strong>More Resources</strong> * [Postman API Network](https://postman.com/explore) * [Free APIs](https://free-apis.github.io) * [Dev Resources](https://devresourc.es/tools-and-utilities/public-apis) * [Apihouse](https://apihouse.vercel.app) * [Collective APIs](https://collective-api.vercel.app) </div> </details> <br /> <br />'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Answer\"].values[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ea28b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Context</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Repo Url</th>\n",
       "      <th>Repo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Provide the README content for the section wit...</td>\n",
       "      <td>Discussions in issues and pull requests:\\n    ...</td>\n",
       "      <td>Explore popular APIs and see them work in Post...</td>\n",
       "      <td>https://github.com/public-apis/public-apis</td>\n",
       "      <td>public-apis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Provide the README content for the section wit...</td>\n",
       "      <td>Discussions in issues and pull requests:\\n    ...</td>\n",
       "      <td>| API | Description | Call this API | |:---|:-...</td>\n",
       "      <td>https://github.com/public-apis/public-apis</td>\n",
       "      <td>public-apis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Provide the README content for the section wit...</td>\n",
       "      <td>Discussions in issues and pull requests:\\n    ...</td>\n",
       "      <td>| API | Description | Auth | Call this API | |...</td>\n",
       "      <td>https://github.com/public-apis/public-apis</td>\n",
       "      <td>public-apis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Provide the README content for the section wit...</td>\n",
       "      <td># check each category for the minimum number o...</td>\n",
       "      <td>* [Animals](#animals) * [Anime](#anime) * [Art...</td>\n",
       "      <td>https://github.com/public-apis/public-apis</td>\n",
       "      <td>public-apis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Provide the README content for the section wit...</td>\n",
       "      <td>Discussions in issues and pull requests:\\n    ...</td>\n",
       "      <td>&lt;br &gt; &lt;strong&gt;Get Involved&lt;/strong&gt; * [Contrib...</td>\n",
       "      <td>https://github.com/public-apis/public-apis</td>\n",
       "      <td>public-apis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Provide the README content for the section wit...   \n",
       "1  Provide the README content for the section wit...   \n",
       "2  Provide the README content for the section wit...   \n",
       "3  Provide the README content for the section wit...   \n",
       "4  Provide the README content for the section wit...   \n",
       "\n",
       "                                             Context  \\\n",
       "0  Discussions in issues and pull requests:\\n    ...   \n",
       "1  Discussions in issues and pull requests:\\n    ...   \n",
       "2  Discussions in issues and pull requests:\\n    ...   \n",
       "3  # check each category for the minimum number o...   \n",
       "4  Discussions in issues and pull requests:\\n    ...   \n",
       "\n",
       "                                              Answer  \\\n",
       "0  Explore popular APIs and see them work in Post...   \n",
       "1  | API | Description | Call this API | |:---|:-...   \n",
       "2  | API | Description | Auth | Call this API | |...   \n",
       "3  * [Animals](#animals) * [Anime](#anime) * [Art...   \n",
       "4  <br > <strong>Get Involved</strong> * [Contrib...   \n",
       "\n",
       "                                     Repo Url         Repo  \n",
       "0  https://github.com/public-apis/public-apis  public-apis  \n",
       "1  https://github.com/public-apis/public-apis  public-apis  \n",
       "2  https://github.com/public-apis/public-apis  public-apis  \n",
       "3  https://github.com/public-apis/public-apis  public-apis  \n",
       "4  https://github.com/public-apis/public-apis  public-apis  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df.replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=[\"Answer\"], inplace=True)\n",
    "df = df[[\"Question\", \"Context\", \"Answer\", \"Repo Url\", \"Repo\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04dc8c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Context</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Repo Url</th>\n",
       "      <th>Repo</th>\n",
       "      <th>detect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Provide the README content for the section wit...</td>\n",
       "      <td>Discussions in issues and pull requests:\\n    ...</td>\n",
       "      <td>Explore popular APIs and see them work in Post...</td>\n",
       "      <td>https://github.com/public-apis/public-apis</td>\n",
       "      <td>public-apis</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Provide the README content for the section wit...</td>\n",
       "      <td>Discussions in issues and pull requests:\\n    ...</td>\n",
       "      <td>| API | Description | Call this API | |:---|:-...</td>\n",
       "      <td>https://github.com/public-apis/public-apis</td>\n",
       "      <td>public-apis</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Provide the README content for the section wit...</td>\n",
       "      <td>Discussions in issues and pull requests:\\n    ...</td>\n",
       "      <td>| API | Description | Auth | Call this API | |...</td>\n",
       "      <td>https://github.com/public-apis/public-apis</td>\n",
       "      <td>public-apis</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Provide the README content for the section wit...</td>\n",
       "      <td># check each category for the minimum number o...</td>\n",
       "      <td>* [Animals](#animals) * [Anime](#anime) * [Art...</td>\n",
       "      <td>https://github.com/public-apis/public-apis</td>\n",
       "      <td>public-apis</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Provide the README content for the section wit...</td>\n",
       "      <td>Discussions in issues and pull requests:\\n    ...</td>\n",
       "      <td>&lt;br &gt; &lt;strong&gt;Get Involved&lt;/strong&gt; * [Contrib...</td>\n",
       "      <td>https://github.com/public-apis/public-apis</td>\n",
       "      <td>public-apis</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Provide the README content for the section wit...   \n",
       "1  Provide the README content for the section wit...   \n",
       "2  Provide the README content for the section wit...   \n",
       "3  Provide the README content for the section wit...   \n",
       "4  Provide the README content for the section wit...   \n",
       "\n",
       "                                             Context  \\\n",
       "0  Discussions in issues and pull requests:\\n    ...   \n",
       "1  Discussions in issues and pull requests:\\n    ...   \n",
       "2  Discussions in issues and pull requests:\\n    ...   \n",
       "3  # check each category for the minimum number o...   \n",
       "4  Discussions in issues and pull requests:\\n    ...   \n",
       "\n",
       "                                              Answer  \\\n",
       "0  Explore popular APIs and see them work in Post...   \n",
       "1  | API | Description | Call this API | |:---|:-...   \n",
       "2  | API | Description | Auth | Call this API | |...   \n",
       "3  * [Animals](#animals) * [Anime](#anime) * [Art...   \n",
       "4  <br > <strong>Get Involved</strong> * [Contrib...   \n",
       "\n",
       "                                     Repo Url         Repo detect  \n",
       "0  https://github.com/public-apis/public-apis  public-apis     en  \n",
       "1  https://github.com/public-apis/public-apis  public-apis     en  \n",
       "2  https://github.com/public-apis/public-apis  public-apis     en  \n",
       "3  https://github.com/public-apis/public-apis  public-apis     en  \n",
       "4  https://github.com/public-apis/public-apis  public-apis     en  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "df['detect'] = detect(str(df['Answer']))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e83daab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12803"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['detect'] == 'en']\n",
    "df = df[[\"Question\", \"Context\", \"Answer\", \"Repo Url\", \"Repo\"]]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa2a67e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['explore popular apis and see them work in postman. br p a href div img src .github cs1586 apilayerlogoupdate2022 lj v2 highres.png width 250 alt apilayer logo div a p apilayer is the fastest way to integrate apis into any product. they created this repository to support the community in easily finding public apis. explore their collections on the postman api network .',\n",
       "       ' api description call this api ip stack locate and identify website visitors by ip address img src alt run in postman style width 128px height 32px marketstack free easy to use rest api interface delivering worldwide stock market data in json format img src alt run in postman style width 128px height 32px weatherstack retrieve instant accurate weather information for any location in the world in lightweight json format img src alt run in postman style width 128px height 32px numverify global phone number validation lookup json api img src alt run in postman style width 128px height 32px fixer fixer is a simple and lightweight api for current and historical foreign exchange forex rates. img src alt run in postman style width 128px height 32px br ',\n",
       "       ' api description auth call this api http cat cat for every http status no img src alt run in postman style width 128px height 32px sportmonks football football score schedule news api tv channels stats history display standing e.g. epl la liga apikey img src alt run in postman style width 128px height 32px google maps create customize digital maps based on google maps data apikey img src alt run in postman style width 128px height 32px notion integrate with notion apikey img src alt run in postman style width 128px height 32px plaid connect with user is bank accounts and access transaction data apikey img src alt run in postman style width 128px height 32px br ',\n",
       "       ' animals animals anime anime art design art design authentication authorization authentication authorization blockchain blockchain business business currency exchange currency exchange social social br ',\n",
       "       ' br strong get involved strong contributing guide contributing.md api for this project issues pull requests license license own an api? publish your own run in postman button. br strong more resources strong postman api network free apis dev resources apihouse collective apis div details br br '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"Answer\"] = df[\"Answer\"].apply(clean_text)\n",
    "df[\"Answer\"] = df[\"Answer\"].apply(text_cleaner)\n",
    "df[\"Answer\"] = df[\"Answer\"].apply(clean_emoji)\n",
    "df[\"Context\"] = df[\"Context\"].apply(text_cleaner)\n",
    "df[\"Answer\"].values[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c56da770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "339"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = ['allennlp', 'autojump', 'typer', 'spotify-downloader', 'spleeter', 'python-fire', 'numpy-ml', 'magenta'] \n",
    "   \n",
    "# selecting rows based on condition \n",
    "df = df[df['Repo'].isin(options)]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "034d8d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['poetry run pytest tests ',\n",
       "       ' deezer research source separation engine story deezer.io blog post english version japanese version music source separation tool with pre trained models ismir2019 extended abstract if you use spleeter in your work please cite bibtex @article spleeter2020 doi 10.21105 joss.02154 url year 2020 publisher the open journal volume 5 number 50 pages 2154 author romain hennequin and anis khlif and felix voituret and manuel moussallam title spleeter a fast and efficient music source separation tool with pre trained models journal journal of open source software note deezer research ',\n",
       "       'the code of spleeter is mit licensed license .',\n",
       "       'if you plan to use spleeter on copyrighted material make sure you get proper authorization from right owners beforehand.',\n",
       "       ' spleeter is a complex piece of software and although we continously try to improve and test it you may encounter unexpected issues running it. if that is the case please check the faq page first as well as the list of currently open issues ',\n",
       "       'it appears that sometimes the shortcut command spleeter does not work properly on windows. this is a known issue that we will hopefully fix soon. in the meantime replace spleeter separate by python m spleeter separate in command line and it should work.',\n",
       "       'if you would like to participate in the development of spleeter you are more than welcome to do so. do not hesitate to throw us a pull request and we will do our best to examine it quickly. please check out our guidelines .github contributing.md first.',\n",
       "       'this repository include a demo audio file audio example.mp3 which is an excerpt from slow motion dream by steven m bryant c copyright 2011 licensed under a creative commons attribution 3.0 license ft csoul alex beroza robert siekawitch',\n",
       "       'ever wish you had an inefficient but somewhat legible collection of machine learning algorithms implemented exclusively in numpy? no?',\n",
       "       'to use this code as a starting point for ml prototyping experimentation just clone the repository create a new virtualenv and start hacking sh git clone cd numpy ml virtualenv npml source npml bin activate pip3 install r requirements dev.txt ',\n",
       "       'if you do not plan to modify the source you can also install numpy ml as a python package pip3 install u numpy ml . the reinforcement learning agents train on environments defined in the openai gym . to install these alongside numpy ml you can use pip3 install u numpy ml rl .',\n",
       "       'for more details on the available models see the project documentation .',\n",
       "       ' details summary click to expand! summary 1. gaussian mixture model em training 2. hidden markov model viterbi decoding likelihood computation mle parameter estimation via baum welch forward backward algorithm 3. latent dirichlet allocation topic model standard model with mle parameter estimation via variational em smoothed model with map parameter estimation via mcmc 4. neural networks layers layer wise ops add flatten multiply softmax fully connected dense sparse evolutionary connections lstm elman style rnn max average pooling dot product attention embedding layer restricted boltzmann machine w. cd n training 2d deconvolution w. padding and stride 2d convolution w. padding dilation and stride 1d convolution w. padding dilation stride and causality modules bidirectional lstm resnet style residual blocks identity and convolution wavenet style residual blocks with dilated causal convolutions transformer style multi headed scaled dot product attention regularizers dropout normalization batch normalization spatial and temporal layer normalization spatial and temporal optimizers sgd w momentum adagrad rmsprop adam learning rate schedulers constant exponential noam transformer dlib scheduler weight initializers glorot xavier uniform and normal he kaiming uniform and normal standard and truncated normal losses cross entropy squared error bernoulli vae loss wasserstein loss with gradient penalty noise contrastive estimation loss activations relu tanh affine sigmoid leaky relu elu selu gelu exponential hard sigmoid softplus models bernoulli variational autoencoder wasserstein gan with gradient penalty word2vec encoder with skip gram and cbow architectures utilities col2im matlab port im2col matlab port conv1d conv2d deconv2d minibatch 5. tree based models decision trees cart bagging random forests boosting gradient boosted decision trees 6. linear models ridge regression logistic regression ordinary least squares weighted linear regression generalized linear model log logit and identity link gaussian naive bayes classifier bayesian linear regression w conjugate priors unknown mean known variance gaussian prior unknown mean <unk>nown variance normal gamma normal inverse wishart prior 7. n gram sequence models maximum likelihood scores additive lidstone smoothing simple good turing smoothing 8. multi armed bandit models ucb1 linucb epsilon greedy thompson sampling w conjugate priors beta bernoulli sampler linucb 8. reinforcement learning models cross entropy method agent first visit on policy monte carlo agent weighted incremental importance sampling monte carlo agent expected sarsa agent td 0 q learning agent dyna q dyna q with prioritized sweeping 9. nonparameteric models nadaraya watson kernel regression k nearest neighbors classification and regression gaussian process regression 10. matrix factorization regularized alternating least squares non negative matrix factorization 11. preprocessing discrete fourier transform 1d signals discrete cosine transform type ii 1d signals bilinear interpolation 2d signals nearest neighbor interpolation 1d and 2d signals autocorrelation 1d signals signal windowing text tokenization feature hashing feature standardization one hot encoding decoding huffman coding decoding byte pair encoding decoding term frequency inverse document frequency tf idf encoding mfcc encoding 12. utilities similarity kernels distance metrics priority queue ball tree discrete sampler graph processing and generators details ',\n",
       "       'am i missing your favorite model? is there something that could be cleaner less confusing? did i mess something up? submit a pr! the only requirement is that your models are written with just the python standard library and numpy . the scipy library is also permitted under special circumstances see full contributing guidelines here . contributing.md .',\n",
       "       'ever wish you had an inefficient but somewhat legible collection of machine learning algorithms implemented exclusively in numpy? no?',\n",
       "       'to use this code as a starting point for ml prototyping experimentation just clone the repository create a new virtualenv and start hacking sh git clone cd numpy ml virtualenv npml source npml bin activate pip3 install r requirements dev.txt ',\n",
       "       'if you do not plan to modify the source you can also install numpy ml as a python package pip3 install u numpy ml . the reinforcement learning agents train on environments defined in the openai gym . to install these alongside numpy ml you can use pip3 install u numpy ml rl .',\n",
       "       'for more details on the available models see the project documentation .',\n",
       "       ' details summary click to expand! summary 1. gaussian mixture model em training 2. hidden markov model viterbi decoding likelihood computation mle parameter estimation via baum welch forward backward algorithm 3. latent dirichlet allocation topic model standard model with mle parameter estimation via variational em smoothed model with map parameter estimation via mcmc 4. neural networks layers layer wise ops add flatten multiply softmax fully connected dense sparse evolutionary connections lstm elman style rnn max average pooling dot product attention embedding layer restricted boltzmann machine w. cd n training 2d deconvolution w. padding and stride 2d convolution w. padding dilation and stride 1d convolution w. padding dilation stride and causality modules bidirectional lstm resnet style residual blocks identity and convolution wavenet style residual blocks with dilated causal convolutions transformer style multi headed scaled dot product attention regularizers dropout normalization batch normalization spatial and temporal layer normalization spatial and temporal optimizers sgd w momentum adagrad rmsprop adam learning rate schedulers constant exponential noam transformer dlib scheduler weight initializers glorot xavier uniform and normal he kaiming uniform and normal standard and truncated normal losses cross entropy squared error bernoulli vae loss wasserstein loss with gradient penalty noise contrastive estimation loss activations relu tanh affine sigmoid leaky relu elu selu gelu exponential hard sigmoid softplus models bernoulli variational autoencoder wasserstein gan with gradient penalty word2vec encoder with skip gram and cbow architectures utilities col2im matlab port im2col matlab port conv1d conv2d deconv2d minibatch 5. tree based models decision trees cart bagging random forests boosting gradient boosted decision trees 6. linear models ridge regression logistic regression ordinary least squares weighted linear regression generalized linear model log logit and identity link gaussian naive bayes classifier bayesian linear regression w conjugate priors unknown mean known variance gaussian prior unknown mean <unk>nown variance normal gamma normal inverse wishart prior 7. n gram sequence models maximum likelihood scores additive lidstone smoothing simple good turing smoothing 8. multi armed bandit models ucb1 linucb epsilon greedy thompson sampling w conjugate priors beta bernoulli sampler linucb 8. reinforcement learning models cross entropy method agent first visit on policy monte carlo agent weighted incremental importance sampling monte carlo agent expected sarsa agent td 0 q learning agent dyna q dyna q with prioritized sweeping 9. nonparameteric models nadaraya watson kernel regression k nearest neighbors classification and regression gaussian process regression 10. matrix factorization regularized alternating least squares non negative matrix factorization 11. preprocessing discrete fourier transform 1d signals discrete cosine transform type ii 1d signals bilinear interpolation 2d signals nearest neighbor interpolation 1d and 2d signals autocorrelation 1d signals signal windowing text tokenization feature hashing feature standardization one hot encoding decoding huffman coding decoding byte pair encoding decoding term frequency inverse document frequency tf idf encoding mfcc encoding 12. utilities similarity kernels distance metrics priority queue ball tree discrete sampler graph processing and generators details ',\n",
       "       'am i missing your favorite model? is there something that could be cleaner less confusing? did i mess something up? submit a pr! the only requirement is that your models are written with just the python standard library and numpy . the scipy library is also permitted under special circumstances see full contributing guidelines here . contributing.md .',\n",
       "       'ever wish you had an inefficient but somewhat legible collection of machine learning algorithms implemented exclusively in numpy? no?',\n",
       "       'to use this code as a starting point for ml prototyping experimentation just clone the repository create a new virtualenv and start hacking sh git clone cd numpy ml virtualenv npml source npml bin activate pip3 install r requirements dev.txt ',\n",
       "       'if you do not plan to modify the source you can also install numpy ml as a python package pip3 install u numpy ml . the reinforcement learning agents train on environments defined in the openai gym . to install these alongside numpy ml you can use pip3 install u numpy ml rl .',\n",
       "       'for more details on the available models see the project documentation .',\n",
       "       ' details summary click to expand! summary 1. gaussian mixture model em training 2. hidden markov model viterbi decoding likelihood computation mle parameter estimation via baum welch forward backward algorithm 3. latent dirichlet allocation topic model standard model with mle parameter estimation via variational em smoothed model with map parameter estimation via mcmc 4. neural networks layers layer wise ops add flatten multiply softmax fully connected dense sparse evolutionary connections lstm elman style rnn max average pooling dot product attention embedding layer restricted boltzmann machine w. cd n training 2d deconvolution w. padding and stride 2d convolution w. padding dilation and stride 1d convolution w. padding dilation stride and causality modules bidirectional lstm resnet style residual blocks identity and convolution wavenet style residual blocks with dilated causal convolutions transformer style multi headed scaled dot product attention regularizers dropout normalization batch normalization spatial and temporal layer normalization spatial and temporal optimizers sgd w momentum adagrad rmsprop adam learning rate schedulers constant exponential noam transformer dlib scheduler weight initializers glorot xavier uniform and normal he kaiming uniform and normal standard and truncated normal losses cross entropy squared error bernoulli vae loss wasserstein loss with gradient penalty noise contrastive estimation loss activations relu tanh affine sigmoid leaky relu elu selu gelu exponential hard sigmoid softplus models bernoulli variational autoencoder wasserstein gan with gradient penalty word2vec encoder with skip gram and cbow architectures utilities col2im matlab port im2col matlab port conv1d conv2d deconv2d minibatch 5. tree based models decision trees cart bagging random forests boosting gradient boosted decision trees 6. linear models ridge regression logistic regression ordinary least squares weighted linear regression generalized linear model log logit and identity link gaussian naive bayes classifier bayesian linear regression w conjugate priors unknown mean known variance gaussian prior unknown mean <unk>nown variance normal gamma normal inverse wishart prior 7. n gram sequence models maximum likelihood scores additive lidstone smoothing simple good turing smoothing 8. multi armed bandit models ucb1 linucb epsilon greedy thompson sampling w conjugate priors beta bernoulli sampler linucb 8. reinforcement learning models cross entropy method agent first visit on policy monte carlo agent weighted incremental importance sampling monte carlo agent expected sarsa agent td 0 q learning agent dyna q dyna q with prioritized sweeping 9. nonparameteric models nadaraya watson kernel regression k nearest neighbors classification and regression gaussian process regression 10. matrix factorization regularized alternating least squares non negative matrix factorization 11. preprocessing discrete fourier transform 1d signals discrete cosine transform type ii 1d signals bilinear interpolation 2d signals nearest neighbor interpolation 1d and 2d signals autocorrelation 1d signals signal windowing text tokenization feature hashing feature standardization one hot encoding decoding huffman coding decoding byte pair encoding decoding term frequency inverse document frequency tf idf encoding mfcc encoding 12. utilities similarity kernels distance metrics priority queue ball tree discrete sampler graph processing and generators details ',\n",
       "       'am i missing your favorite model? is there something that could be cleaner less confusing? did i mess something up? submit a pr! the only requirement is that your models are written with just the python standard library and numpy . the scipy library is also permitted under special circumstances see full contributing guidelines here . contributing.md .',\n",
       "       'ever wish you had an inefficient but somewhat legible collection of machine learning algorithms implemented exclusively in numpy? no?',\n",
       "       'to use this code as a starting point for ml prototyping experimentation just clone the repository create a new virtualenv and start hacking sh git clone cd numpy ml virtualenv npml source npml bin activate pip3 install r requirements dev.txt ',\n",
       "       'if you do not plan to modify the source you can also install numpy ml as a python package pip3 install u numpy ml . the reinforcement learning agents train on environments defined in the openai gym . to install these alongside numpy ml you can use pip3 install u numpy ml rl .',\n",
       "       'for more details on the available models see the project documentation .',\n",
       "       ' details summary click to expand! summary 1. gaussian mixture model em training 2. hidden markov model viterbi decoding likelihood computation mle parameter estimation via baum welch forward backward algorithm 3. latent dirichlet allocation topic model standard model with mle parameter estimation via variational em smoothed model with map parameter estimation via mcmc 4. neural networks layers layer wise ops add flatten multiply softmax fully connected dense sparse evolutionary connections lstm elman style rnn max average pooling dot product attention embedding layer restricted boltzmann machine w. cd n training 2d deconvolution w. padding and stride 2d convolution w. padding dilation and stride 1d convolution w. padding dilation stride and causality modules bidirectional lstm resnet style residual blocks identity and convolution wavenet style residual blocks with dilated causal convolutions transformer style multi headed scaled dot product attention regularizers dropout normalization batch normalization spatial and temporal layer normalization spatial and temporal optimizers sgd w momentum adagrad rmsprop adam learning rate schedulers constant exponential noam transformer dlib scheduler weight initializers glorot xavier uniform and normal he kaiming uniform and normal standard and truncated normal losses cross entropy squared error bernoulli vae loss wasserstein loss with gradient penalty noise contrastive estimation loss activations relu tanh affine sigmoid leaky relu elu selu gelu exponential hard sigmoid softplus models bernoulli variational autoencoder wasserstein gan with gradient penalty word2vec encoder with skip gram and cbow architectures utilities col2im matlab port im2col matlab port conv1d conv2d deconv2d minibatch 5. tree based models decision trees cart bagging random forests boosting gradient boosted decision trees 6. linear models ridge regression logistic regression ordinary least squares weighted linear regression generalized linear model log logit and identity link gaussian naive bayes classifier bayesian linear regression w conjugate priors unknown mean known variance gaussian prior unknown mean <unk>nown variance normal gamma normal inverse wishart prior 7. n gram sequence models maximum likelihood scores additive lidstone smoothing simple good turing smoothing 8. multi armed bandit models ucb1 linucb epsilon greedy thompson sampling w conjugate priors beta bernoulli sampler linucb 8. reinforcement learning models cross entropy method agent first visit on policy monte carlo agent weighted incremental importance sampling monte carlo agent expected sarsa agent td 0 q learning agent dyna q dyna q with prioritized sweeping 9. nonparameteric models nadaraya watson kernel regression k nearest neighbors classification and regression gaussian process regression 10. matrix factorization regularized alternating least squares non negative matrix factorization 11. preprocessing discrete fourier transform 1d signals discrete cosine transform type ii 1d signals bilinear interpolation 2d signals nearest neighbor interpolation 1d and 2d signals autocorrelation 1d signals signal windowing text tokenization feature hashing feature standardization one hot encoding decoding huffman coding decoding byte pair encoding decoding term frequency inverse document frequency tf idf encoding mfcc encoding 12. utilities similarity kernels distance metrics priority queue ball tree discrete sampler graph processing and generators details ',\n",
       "       'am i missing your favorite model? is there something that could be cleaner less confusing? did i mess something up? submit a pr! the only requirement is that your models are written with just the python standard library and numpy . the scipy library is also permitted under special circumstances see full contributing guidelines here . contributing.md .',\n",
       "       'ever wish you had an inefficient but somewhat legible collection of machine learning algorithms implemented exclusively in numpy? no?',\n",
       "       'to use this code as a starting point for ml prototyping experimentation just clone the repository create a new virtualenv and start hacking sh git clone cd numpy ml virtualenv npml source npml bin activate pip3 install r requirements dev.txt ',\n",
       "       'if you do not plan to modify the source you can also install numpy ml as a python package pip3 install u numpy ml . the reinforcement learning agents train on environments defined in the openai gym . to install these alongside numpy ml you can use pip3 install u numpy ml rl .',\n",
       "       'for more details on the available models see the project documentation .',\n",
       "       ' details summary click to expand! summary 1. gaussian mixture model em training 2. hidden markov model viterbi decoding likelihood computation mle parameter estimation via baum welch forward backward algorithm 3. latent dirichlet allocation topic model standard model with mle parameter estimation via variational em smoothed model with map parameter estimation via mcmc 4. neural networks layers layer wise ops add flatten multiply softmax fully connected dense sparse evolutionary connections lstm elman style rnn max average pooling dot product attention embedding layer restricted boltzmann machine w. cd n training 2d deconvolution w. padding and stride 2d convolution w. padding dilation and stride 1d convolution w. padding dilation stride and causality modules bidirectional lstm resnet style residual blocks identity and convolution wavenet style residual blocks with dilated causal convolutions transformer style multi headed scaled dot product attention regularizers dropout normalization batch normalization spatial and temporal layer normalization spatial and temporal optimizers sgd w momentum adagrad rmsprop adam learning rate schedulers constant exponential noam transformer dlib scheduler weight initializers glorot xavier uniform and normal he kaiming uniform and normal standard and truncated normal losses cross entropy squared error bernoulli vae loss wasserstein loss with gradient penalty noise contrastive estimation loss activations relu tanh affine sigmoid leaky relu elu selu gelu exponential hard sigmoid softplus models bernoulli variational autoencoder wasserstein gan with gradient penalty word2vec encoder with skip gram and cbow architectures utilities col2im matlab port im2col matlab port conv1d conv2d deconv2d minibatch 5. tree based models decision trees cart bagging random forests boosting gradient boosted decision trees 6. linear models ridge regression logistic regression ordinary least squares weighted linear regression generalized linear model log logit and identity link gaussian naive bayes classifier bayesian linear regression w conjugate priors unknown mean known variance gaussian prior unknown mean <unk>nown variance normal gamma normal inverse wishart prior 7. n gram sequence models maximum likelihood scores additive lidstone smoothing simple good turing smoothing 8. multi armed bandit models ucb1 linucb epsilon greedy thompson sampling w conjugate priors beta bernoulli sampler linucb 8. reinforcement learning models cross entropy method agent first visit on policy monte carlo agent weighted incremental importance sampling monte carlo agent expected sarsa agent td 0 q learning agent dyna q dyna q with prioritized sweeping 9. nonparameteric models nadaraya watson kernel regression k nearest neighbors classification and regression gaussian process regression 10. matrix factorization regularized alternating least squares non negative matrix factorization 11. preprocessing discrete fourier transform 1d signals discrete cosine transform type ii 1d signals bilinear interpolation 2d signals nearest neighbor interpolation 1d and 2d signals autocorrelation 1d signals signal windowing text tokenization feature hashing feature standardization one hot encoding decoding huffman coding decoding byte pair encoding decoding term frequency inverse document frequency tf idf encoding mfcc encoding 12. utilities similarity kernels distance metrics priority queue ball tree discrete sampler graph processing and generators details ',\n",
       "       'am i missing your favorite model? is there something that could be cleaner less confusing? did i mess something up? submit a pr! the only requirement is that your models are written with just the python standard library and numpy . the scipy library is also permitted under special circumstances see full contributing guidelines here . contributing.md .',\n",
       "       ' typer is a href class external link target blank fastapi a is little sibling it is the fastapi of clis.',\n",
       "       ' div class termy console pip install typer 100 successfully installed typer rich shellingham div ',\n",
       "       ' create a file main.py with python def main name str print f hello name this script does not even use typer internally. but you can use the typer command to run it as a cli application.',\n",
       "       'run your application with the typer command div class termy console run your application typer main.py run you get a nice error you are missing name usage typer path or module run options name try notyper path or module run help for help. error missing argument name . you get a help for free typer main.py run help usage typer path or module run options name run the provided typer app. arguments name text default none required options help show this message and exit. now pass the name argument typer main.py run camila hello camila it works! div this is the simplest use case not even using typer internally but it can already be quite useful for simple scripts. note auto completion works when you create a python package and run it with install completion or when you use the typer command.',\n",
       "       'now let is start using typer in your own code update main.py with python import typer def main name str print f hello name if name main typer.run main now you could run it with python directly div class termy console run your application python main.py you get a nice error you are missing name usage main.py options name try amain.py help for help. error missing argument name . you get a help for free python main.py help usage main.py options name arguments name text default none required options help show this message and exit. now pass the name argument python main.py camila hello camila it works! div note you can also call this same script with the typer command but you do not need to.',\n",
       "       'this was the simplest example possible. now let is see one a bit more complex.',\n",
       "       'modify the file main.py . create a typer.typer app and create two subcommands with their parameters. python hl lines 3 6 11 20 import typer app typer.typer @app.command def hello name str print f hello name @app.command def goodbye name str formal bool false if formal print f goodbye ms. name . have a good day. else print f bye name ! if name main app and that will explicitly create a typer.typer app. the previous typer.run actually creates one implicitly for you. add two subcommands with @app.command . execute the app itself as if it was a function instead of typer.run .',\n",
       "       'check the new help div class termy console python main.py help usage main.py options command args . options install completion install completion for the current shell. show completion show completion for the current shell to copy it or customize the installation. help show this message and exit. commands goodbye hello when you create a package you get auto completion for free installed with install completion you have 2 subcommands the 2 functions goodbye and hello div now check the help for the hello command div class termy console python main.py hello help usage main.py hello options name arguments name text default none required options help show this message and exit. div and now check the help for the goodbye command div class termy console python main.py goodbye help usage main.py goodbye options name arguments name text default none required options formal no formal default no formal help show this message and exit. automatic formal and no formal for the bool option div now you can try out the new command line application div class termy console use it with the hello command python main.py hello camila hello camila and with the goodbye command python main.py goodbye camila bye camila! and with formal python main.py goodbye formal camila goodbye ms. camila. have a good day. div ',\n",
       "       'in summary you declare once the types of parameters cli arguments and cli options as function parameters. you do that with standard modern python types. you do not have to learn a new syntax the methods or classes of a specific library etc. just standard python . for example for an int python total int or for a bool flag python force bool and similarly for files paths enums choices etc. and there are tools to create groups of subcommands add metadata extra validation etc. you get great editor support including completion and type checks everywhere. your users get automatic help auto completion in their terminal bash zsh fish powershell when they install your package or when using the typer command. for a more complete example including more features see the a href tutorial user guide a .',\n",
       "       ' typer stands on the shoulders of a giant. its only internal required dependency is a href class external link target blank click a . by default it also comes with extra standard dependencies a href class external link target blank code rich code a to show nicely formatted errors automatically. a href class external link target blank code shellingham code a to automatically detect the current shell when installing completion. with shellingham you can just use install completion . without shellingham you have to pass the name of the shell to install completion for e.g. install completion bash .',\n",
       "       'if you do not want the extra standard optional dependencies install typer slim instead. when you install with bash pip install typer .it includes the same code and dependencies as bash pip install typer slim standard the standard extra dependencies are rich and shellingham . note the typer command is only included in the typer package.',\n",
       "       'this project is licensed under the terms of the mit license.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Answer\"].values[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5271dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"scripts/readme_qa_cleaned_small_v5.csv\", index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985ccd1",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee64205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 28.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 398.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.04 seconds, 23.79 sentences/sec\n",
      "tensor([0.8705]) tensor([0.8412]) tensor([0.8556])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# from torchmetrics.text.bert import BERTScore\n",
    "import bert_score\n",
    "import re\n",
    "\n",
    "# with open(\"/home/scp6004/doc_generator/output/spleeter/docs/data/README_LLAMA2_7B_CHAT_GPTQ.md\", 'r', encoding='utf-8') as f:\n",
    "#     pred = f.read()\n",
    "# with open(\"/home/scp6004/doc_generator/spleeter/README.md\", 'r', encoding='utf-8') as f:\n",
    "#     target = f.read()\n",
    "\n",
    "pred = \"\"\"\n",
    "Research repository for TouchPose\n",
    "This folder contains pre-trained models for various tasks, such as hand detection, finger segmentation, and gesture recognition.\n",
    "\n",
    "Disclaimer:\n",
    "The Licensor will not be liable to you on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. The disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n",
    " \n",
    "License\n",
    "This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License.\"\"\"\n",
    "\n",
    "target = \"\"\"\n",
    "Research repository for TouchPose\n",
    "This repository contains the part of the dataset we collected on pairs of capacitive touch images and depth maps of fingers and hands.\n",
    "\n",
    "Disclaimer:\n",
    "IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW THE AUTHOR WILL BE LIABLE TO YOU\n",
    "FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL\n",
    "DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT\n",
    "NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES\n",
    "SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH\n",
    "ANY OTHER PROGRAMS), EVEN IF THE AUTHOR HAS BEEN ADVISED OF THE POSSIBILITY OF\n",
    "SUCH DAMAGES.\n",
    "\n",
    "License\n",
    "This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
    "\"\"\"\n",
    "\n",
    "pred = re.sub(r' +', ' ', pred)\n",
    "target = re.sub(r' +', ' ', target)\n",
    "P, R, F1 = bert_score.score([pred], [target], lang='en', model_type='roberta-large', verbose=True)\n",
    "print(P,R,F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02cebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18306802223623997\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Download the required NLTK data\n",
    "# nltk.download('punkt')\n",
    "\n",
    "pred = \"\"\"\n",
    "## About\n",
    "Spleeter is an open-source tool for music information retrieval and generation. It is designed to be highly extensible and customizable, allowing users to create their own plugins and integrations. Spleeter is built using Python and uses a variety of libraries and frameworks, including TensorFlow, NumPy, and Librosa.\n",
    "The goal of Spleeter is to provide a flexible and powerful platform for music information retrieval and generation, while also being easy to use and understand. It is designed to be used by both developers and non-developers, and can be used for a wide range of applications, such as creating music databases, generating music metadata, and more.\n",
    "Spleeter is actively maintained and developed by the community, with new features and improvements being added regularly. It is released under the MIT License, which means that it is free to use and modify for any purpose.\n",
    "If you are interested in learning more about Spleeter or getting involved in the development process, please visit the <a href=\"https://github.com/deezer/spleeter/wiki\">Spleeter documentation</a>.\n",
    "\n",
    "### Projects and Softwares using **Spleeter**\n",
    "Spleeter is a powerful tool for audio signal processing and analysis, and many projects and softwares have already benefited from its capabilities. Here are some examples of projects and softwares that use or have used Spleeter:\n",
    "\n",
    "## Spleeter Pro (Commercial version)\n",
    "Spleeter Pro is a commercial version of Spleeter, a Python library for audio signal processing. It offers additional features and improvements over the free version, including:\n",
    "Improved performance: Spleeter Pro has been optimized for faster processing times, making it ideal for large-scale audio processing tasks.\n",
    "Enhanced functionality: Spleeter Pro includes additional features such as noise reduction, equalization, and compression, providing more options for audio processing.\n",
    "Better support: Spleeter Pro comes with dedicated customer support, ensuring that you get the help you need when you need it.\n",
    "\n",
    "## Troubleshooting\n",
    "In case none of the above solutions work, please open an issue on the <code>spleeter</code> GitHub repository with as much detail as possible, including any error messages you encountered and the steps you took leading up to the issue.\n",
    "\"\"\"\n",
    "\n",
    "target = \"\"\"\n",
    "## About\n",
    "Spleeter is [Deezer](https://www.deezer.com/) source separation library with pretrained models\n",
    "written in [Python](https://www.python.org/) and uses [Tensorflow](https://tensorflow.org/). Spleeter is also very fast as it can perform separation of audio files to 4 stems 100x faster than real-time when run on a GPU.\n",
    "\n",
    "We designed Spleeter so you can use it straight from [command line](https://github.com/deezer/spleeter/wiki/2.-Getting-started#usage)\n",
    "as well as directly in your own development pipeline as a [Python library](https://github.com/deezer/spleeter/wiki/4.-API-Reference#separator). It can be installed with [pip](https://github.com/deezer/spleeter/wiki/1.-Installation#using-pip) or be used with\n",
    "[Docker](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-docker-image).\n",
    "\n",
    "### Projects and Softwares using **Spleeter**\n",
    "**Spleeter** pre-trained models have also been used by professionnal audio softwares. Here's a non-exhaustive list:\n",
    "\n",
    "## Spleeter Pro (Commercial version)\n",
    "Check out our commercial version : [Spleeter Pro](https://www.deezer-techservices.com/solutions/spleeter/). Benefit from our expertise for precise audio separation, faster processing speeds, and dedicated professional support. \n",
    "\n",
    "## Troubleshooting\n",
    "Spleeter is a complex piece of software and although we continously try to improve and test it you may encounter unexpected issues running it. If that's the case please check the [FAQ page](https://github.com/deezer/spleeter/wiki/5.-FAQ) first as well as the list of [currently open issues](https://github.com/deezer/spleeter/issues)\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GPTQ\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# import tiktoken\n",
    "# enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference_tokens = tokenizer.tokenize(reference)\n",
    "    candidate_tokens = tokenizer.tokenize(candidate)\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothie)\n",
    "\n",
    "bleu_score = calculate_bleu(target, pred)\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cba04a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics.text import BLEUScore\n",
    "bleu = BLEUScore(n_gram=4, smooth=True)\n",
    "reference_tokens = tokenizer.tokenize(target)\n",
    "candidate_tokens = tokenizer.tokenize(pred)\n",
    "print(bleu(reference_tokens, candidate_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torcheval.metrics.text import Perplexity\n",
    "# metric=Perplexity()\n",
    "# metric.update(input, target)\n",
    "# metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
