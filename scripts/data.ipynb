{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4fd5fa2",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb7e5c-6f9d-4ab1-9b43-40c756b66ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, quote\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def parse_markdown_to_csv(md_content, csv_file_path):\n",
    "    heading_pattern = re.compile(r'^(#+)\\s*(.*)', re.MULTILINE)\n",
    "    headings_contents = []\n",
    "    current_heading = None\n",
    "    current_content = []\n",
    "    \n",
    "    for line in md_content.split('\\n'):\n",
    "        match = heading_pattern.match(line)\n",
    "        if match:\n",
    "            if current_heading is not None:\n",
    "                headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "            current_heading = match.group(2).strip()\n",
    "            current_content = []\n",
    "        else:\n",
    "            if line.strip():\n",
    "                current_content.append(line.strip())\n",
    "    \n",
    "    if current_heading is not None:\n",
    "        headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "    \n",
    "    df = pd.DataFrame(headings_contents, columns=['Title', 'Content'])\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "def fetch_and_convert_readme_to_csv(repo_urls, output_dir):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # GitHub API endpoint for fetching the contents of the README file\n",
    "    for url in repo_urls:\n",
    "        parsed_url = urlparse(url)\n",
    "        parts = parsed_url.path.strip('/').split('/')\n",
    "        repo_user, repo_name = parts[0], parts[1]\n",
    "        api_url = f\"https://api.github.com/repos/{repo_user}/{repo_name}/readme\"\n",
    "        \n",
    "        # Set up appropriate headers for GitHub API including the token for authorization\n",
    "        headers = {\n",
    "            'Accept': 'application/vnd.github.v3.raw',\n",
    "            'Authorization': 'YOUR_GITHUB_TOKEN'  # Replace 'YOUR_GITHUB_TOKEN' with your actual GitHub token\n",
    "        }\n",
    "        \n",
    "        response = requests.get(api_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            readme_content = response.text\n",
    "            csv_file_path = os.path.join(output_dir, f\"{repo_name}.csv\")\n",
    "            parse_markdown_to_csv(readme_content, csv_file_path)\n",
    "            print(f\"Processed {repo_name}.csv\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch README for {repo_name}: {response.status_code}\")\n",
    "\n",
    "# Example usage:\n",
    "repo_urls = [\n",
    "    'https://github.com/context-labs/autodoc'\n",
    "]\n",
    "\n",
    "fetch_and_convert_readme_to_csv(repo_urls, 'output_csv_files')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d005ac-8225-4d78-8650-09b98ff4ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import base64\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def fetch_and_concatenate_source_code(repo_urls, output_dir, token):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'token {token}',\n",
    "        'Accept': 'application/vnd.github.v3.raw'  # Requests raw content directly\n",
    "    }\n",
    "\n",
    "    for url in repo_urls:\n",
    "        parsed_url = urlparse(url)\n",
    "        parts = parsed_url.path.strip('/').split('/')\n",
    "        repo_user, repo_name = parts[0], parts[1]\n",
    "\n",
    "        # Fetch the default branch\n",
    "        repo_info_url = f'https://api.github.com/repos/{repo_user}/{repo_name}'\n",
    "        repo_info_response = requests.get(repo_info_url, headers=headers)\n",
    "        if repo_info_response.status_code == 200:\n",
    "            default_branch = repo_info_response.json()['default_branch']\n",
    "        else:\n",
    "            print(f'Failed to fetch repo info for {repo_name}: {repo_info_response.status_code}')\n",
    "            continue\n",
    "\n",
    "        api_url = f'https://api.github.com/repos/{repo_user}/{repo_name}/git/trees/{default_branch}?recursive=true'\n",
    "        response = requests.get(api_url, headers={'Authorization': f'token {token}', 'Accept': 'application/vnd.github.v3+json'})\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            all_files_content = []\n",
    "\n",
    "            for file in data['tree']:\n",
    "                if file['type'] == 'blob' and file['path'].endswith(('.py', '.c', '.cpp', '.java', '.js', '.ts', '.go')):\n",
    "                    file_url = f\"https://api.github.com/repos/{repo_user}/{repo_name}/contents/{file['path']}?ref={default_branch}\"\n",
    "                    file_response = requests.get(file_url, headers=headers)\n",
    "                    if file_response.status_code == 200:\n",
    "                        file_content = file_response.text\n",
    "                        all_files_content.append(file_content)\n",
    "\n",
    "            concatenated_content = \"\\n\".join(all_files_content)\n",
    "            df = pd.DataFrame([concatenated_content], columns=['SourceCode'])\n",
    "            df.to_csv(os.path.join(output_dir, f'{repo_name}_context.csv'), index=False)\n",
    "            print(f'Saved {repo_name}_context.csv')\n",
    "        else:\n",
    "            print(f'Failed to fetch repository data for {repo_name}: {response.status_code}')\n",
    "\n",
    "# Example usage:\n",
    "repo_urls = [\n",
    "    \"https://github.com/context-labs/autodoc\"\n",
    "]\n",
    "output_directory = 'output_csv_files'\n",
    "github_token = 'YOUR_GITHUB_TOKEN'  # Replace with your GitHub access token\n",
    "\n",
    "fetch_and_concatenate_source_code(repo_urls, output_directory, github_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78d0b1-b921-468b-8376-d3651bd2d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, quote\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "# Clone repository to a local path\n",
    "def git_clone(repo_url, clone_path):\n",
    "    if os.path.exists(clone_path):\n",
    "        subprocess.run(['rm', '-rf', clone_path], check=True)\n",
    "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)\n",
    "\n",
    "# Parse the README.md content into a CSV\n",
    "def parse_markdown_to_csv(md_file_path, csv_file_path):\n",
    "    with open(md_file_path, 'r', encoding='utf-8') as file:\n",
    "        md_content = file.read()\n",
    "\n",
    "    heading_pattern = re.compile(r'^(#+)\\s*(.*)', re.MULTILINE)\n",
    "    headings_contents = []\n",
    "    current_heading = None\n",
    "    current_content = []\n",
    "\n",
    "    for line in md_content.split('\\n'):\n",
    "        match = heading_pattern.match(line)\n",
    "        if match:\n",
    "            if current_heading is not None:\n",
    "                headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "            current_heading = match.group(2).strip()\n",
    "            current_content = []\n",
    "        else:\n",
    "            if line.strip():\n",
    "                current_content.append(line.strip())\n",
    "\n",
    "    if current_heading is not None:\n",
    "        headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "\n",
    "    df = pd.DataFrame(headings_contents, columns=['Title', 'Content'])\n",
    "    df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Process a list of GitHub repository URLs\n",
    "def process_repos(repo_urls, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for url in repo_urls:\n",
    "        parsed_url = urlparse(url)\n",
    "        parts = parsed_url.path.strip('/').split('/')\n",
    "        repo_user, repo_name = parts[0], parts[1]\n",
    "        clone_path = f\"/tmp/{repo_name}\"  # Temporary path for cloning\n",
    "        git_clone(url, clone_path)\n",
    "\n",
    "        readme_path = os.path.join(clone_path, 'README.md')\n",
    "        csv_file_path = os.path.join(output_dir, f\"{repo_name}.csv\")\n",
    "        if os.path.exists(readme_path):\n",
    "            parse_markdown_to_csv(readme_path, csv_file_path)\n",
    "            print(f\"Processed {repo_name}.csv\")\n",
    "        else:\n",
    "            print(f\"README.md not found for {repo_name}\")\n",
    "\n",
    "        # Remove the repository directory to clean up\n",
    "        subprocess.run(['rm', '-rf', clone_path], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f50bec-95dd-4f3d-85da-6d60768cea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this list with your own list of 300 URLs\n",
    "repo_urls = []\n",
    "output_directory = 'output_csv_files'\n",
    "process_repos(repo_urls, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f96d8c-589b-446b-bceb-63f4fa4b9498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f62fd8-1e33-4250-8c9a-f14d5d9cdbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clone a GitHub repository and collect all source code into a single string\n",
    "def collect_source_code(repo_url):\n",
    "    # Extract the repo name from the URL\n",
    "    repo_name = repo_url.rstrip('/').split('/')[-1]\n",
    "    subprocess.run(['git', 'clone', repo_url], check=True)\n",
    "    \n",
    "    # Collect all source code files into a single string\n",
    "    source_code = []\n",
    "    for root, dirs, files in os.walk(repo_name):\n",
    "        for file in files:\n",
    "            # Filter for source code files only (adjust filters as needed)\n",
    "            if file.endswith(('.py', '.js', '.java', '.cpp', '.c', '.h', '.html', '.css', '.ts', '.go', '.rb', '.php')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', errors='ignore') as f:\n",
    "                    source_code.append(f.read())\n",
    "                    \n",
    "    # Join all source code files as one big string\n",
    "    concatenated_code = \"\\n\".join(source_code)\n",
    "    \n",
    "    # Delete the repo after extraction\n",
    "    shutil.rmtree(repo_name)\n",
    "    \n",
    "    return repo_name, concatenated_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca9c11-5104-4102-bfd4-607705f3aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store CSV files\n",
    "output_dir = \"github_repo_source_code\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create a CSV file per GitHub repo\n",
    "for url in github_urls:\n",
    "    try:\n",
    "        repo_name, concatenated_code = collect_source_code(url)\n",
    "        csv_file_name = f\"{repo_name}.csv\"\n",
    "        csv_file_path = os.path.join(output_dir, csv_file_name)\n",
    "        with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([concatenated_code])\n",
    "        print(f\"Successfully processed and saved {url} to {csv_file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "print(\"All repositories processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f097f-1bb8-43ec-9d45-ce799b9dc254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, quote\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Function to clone a GitHub repository and collect all source code into a single string\n",
    "def collect_source_code(repo_url):\n",
    "    # Extract the repo name from the URL\n",
    "    repo_name = repo_url.rstrip('/').split('/')[-1]\n",
    "    subprocess.run(['git', 'clone', repo_url], check=True)\n",
    "    \n",
    "    # Collect all source code files into a single string\n",
    "    source_code = []\n",
    "    for root, dirs, files in os.walk(repo_name):\n",
    "        for file in files:\n",
    "            # Filter for source code files only (adjust filters as needed)\n",
    "            if file.endswith(('.py', '.js', '.java', '.cpp', '.c', '.h', '.html', '.css', '.ts', '.go', '.rb', '.php')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', errors='ignore') as f:\n",
    "                    source_code.append(f.read())\n",
    "                    \n",
    "    # Join all source code files as one big string\n",
    "    concatenated_code = \"\\n\".join(source_code)\n",
    "    print(type(concatenated_code))\n",
    "    \n",
    "    # Delete the repo after extraction\n",
    "    shutil.rmtree(repo_name)\n",
    "    \n",
    "    return repo_name, concatenated_code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d9f1f-de7c-4de6-a4d5-605cbda7875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store CSV files\n",
    "output_dir = \"github_repo_source_code\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for url in github_urls:\n",
    "    try:\n",
    "        repo_name, concatenated_code = collect_source_code(url)\n",
    "        txt_file_name = f\"{repo_name}.txt\"\n",
    "        txt_file_name = os.path.join(output_dir, txt_file_name)\n",
    "        with open(txt_file_name, 'w', encoding='utf-8') as txt_file:\n",
    "            txt_file.write(concatenated_code)\n",
    "        print(f\"Successfully processed and saved {url} to {txt_file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "print(\"All repositories processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd2313a",
   "metadata": {},
   "source": [
    "# HNSWLIB Context Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hnswlib sentence_transformers langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905a519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "def get_context(sentences, embeds, question_embed):\n",
    "    dim = embeds.shape[1]\n",
    "    num_elements = embeds.shape[0]\n",
    "\n",
    "    # Generating sample data\n",
    "    data = embeds\n",
    "    ids = np.arange(num_elements)\n",
    "\n",
    "    # Declaring index\n",
    "    p = hnswlib.Index(space = 'cosine', dim = dim) # possible options are l2, cosine or ip\n",
    "\n",
    "    # Initializing index - the maximum number of elements should be known beforehand\n",
    "    p.init_index(max_elements = num_elements, ef_construction = 200, M = 16)\n",
    "\n",
    "    # Element insertion (can be called several times):\n",
    "    p.add_items(data, ids)\n",
    "\n",
    "    # Controlling the recall by setting ef:\n",
    "    p.set_ef(50) # ef should always be > k\n",
    "\n",
    "    # Query dataset, k - number of the closest elements (returns 2 numpy arrays)\n",
    "    labels, distances = p.knn_query(question_embed, k = 4)\n",
    "\n",
    "    return \"\".join([sentences[index] for index in labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c04ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "root_dir = \"./\"\n",
    "context_root_dir = \"./github_repo_source_code/\"\n",
    "readme_root_dir = \"./output_csv_files/\"\n",
    "\n",
    "with open('./repo_urls.pickle', 'rb') as f:\n",
    "    repo_name_list = pickle.load(f)\n",
    "\n",
    "new_rows = []\n",
    "for repo in repo_name_list:\n",
    "    repo_name = repo.split(\"/\")[-1]\n",
    "    file1 = repo_name +\".txt\"\n",
    "    with open(os.path.join(context_root_dir, file1)) as f:\n",
    "        data = f.read()\n",
    "    sentences = text_splitter.split_text(data)\n",
    "    embeddings = model.encode(sentences)\n",
    "    print(embeddings.shape)\n",
    "\n",
    "    file2 = repo_name +\".csv\"\n",
    "    df2 = pd.read_csv(os.path.join(readme_root_dir, file2))\n",
    "    for i, row in df2.iterrows():\n",
    "        title = row[\"Title\"]\n",
    "        content = row[\"Content\"]\n",
    "        if \"?\" in title:\n",
    "            question = f\"In context to the project {repo_name}, answer the following. \" + title\n",
    "            question_embedding = model.encode([question])\n",
    "            context = get_context(sentences, embeddings, question_embedding)\n",
    "            new_row  = {\"Question\": question, \"Context\": context, \"Answer\": content, \"Repo Url\": repo, \"Repo\": repo_name}\n",
    "            new_rows.append(new_row)\n",
    "        else:\n",
    "            question = f\"Provide the README content for the section with heading \\\"{title}\\\" starting with ## {title}.\"\n",
    "            question_embedding = model.encode([question])\n",
    "            context = get_context(sentences, embeddings, question_embedding)\n",
    "            new_row  = {\"Question\": question, \"Context\": context, \"Answer\": content, \"Repo Url\": repo, \"Repo\": repo_name}\n",
    "            new_rows.append(new_row)\n",
    "    print(len(new_rows))\n",
    "    df3 = pd.DataFrame(new_rows, index=None)\n",
    "    df3.to_csv(os.path.join(root_dir, \"readme_qa.csv\"), mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7595e4",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62492c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "  \"\"\"Remove URLs from a given text string.\"\"\"\n",
    "  url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "  return re.sub(url_pattern, '', text)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "  \"\"\"Remove HTML tags from a given text string.\"\"\"\n",
    "  html_pattern = r'<.*?>'\n",
    "  return re.sub(html_pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Define the regular expression pattern for HTTP URLs\n",
    "    http_pattern = re.compile(r'http://[^\\s]+')\n",
    "    # Remove HTTP URLs\n",
    "    text = http_pattern.sub('', str(text))\n",
    "\n",
    "    https_pattern = re.compile(r'https://[^\\s]+')\n",
    "    # Remove HTTPS URLs\n",
    "    text = https_pattern.sub('', str(text))\n",
    "    \n",
    "    # Define the regular expression pattern for <img> tags\n",
    "    img_pattern = re.compile(r'<img[^>]*>')\n",
    "    # Remove <img> tags\n",
    "    text = img_pattern.sub('', str(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_emoji(tx):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols \n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport \n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'', tx)\n",
    "\n",
    "def text_cleaner(tx):\n",
    "\n",
    "    text = re.sub(r\"won\\'t\", \"would not\", tx)\n",
    "    text = re.sub(r\"im\", \"i am\", tx)\n",
    "    text = re.sub(r\"Im\", \"I am\", tx)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"don\\'t\", \"do not\", text)\n",
    "    text = re.sub(r\"shouldn\\'t\", \"should not\", text)\n",
    "    text = re.sub(r\"needn\\'t\", \"need not\", text)\n",
    "    text = re.sub(r\"hasn\\'t\", \"has not\", text)\n",
    "    text = re.sub(r\"haven\\'t\", \"have not\", text)\n",
    "    text = re.sub(r\"weren\\'t\", \"were not\", text)\n",
    "    text = re.sub(r\"mightn\\'t\", \"might not\", text)\n",
    "    text = re.sub(r\"didn\\'t\", \"did not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    # text = re.sub('https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'https?://[^\\s\\\")]+', '', text)\n",
    "    text = re.sub(r'http?://[^\\s\\\")]+', '', text)\n",
    "    text = re.sub(r'http%3A%2F%2F[^\\s\\\")]+', '', text)\n",
    "    text = re.sub(r'https%3A%2F%2F[^\\s\\\")]+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\!\\?\\.\\@]',' ' , text)\n",
    "    text = re.sub(r'[!]+' , '!' , text)\n",
    "    text = re.sub(r'[?]+' , '?' , text)\n",
    "    text = re.sub(r'[.]+' , '.' , text)\n",
    "    text = re.sub(r'[@]+' , '@' , text)\n",
    "    text = re.sub(r'unk' , '<UNK>' , text)\n",
    "    # text = re.sub('\\n', '<NL>', text)\n",
    "    # text = re.sub('\\t', '<TAB>', text)\n",
    "    # text = re.sub(r'\\s+', '<SP>', text)\n",
    "    # text = re.sub(r'(<img[^>]*\\bsrc=\")[^\"]*(\")', '<img src=<IMG_SRC>', text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[ ]+' , ' ' , text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb59731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"scripts/readme_qa.csv\")\n",
    "df.columns = [str(q).strip() for q in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9986405",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Answer\"].values[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea28b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df.replace('', np.nan, inplace=True)\n",
    "df.dropna(subset=[\"Answer\"], inplace=True)\n",
    "df = df[[\"Question\", \"Context\", \"Answer\", \"Repo Url\", \"Repo\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "df['detect'] = detect(str(df['Answer']))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83daab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['detect'] == 'en']\n",
    "df = df[[\"Question\", \"Context\", \"Answer\", \"Repo Url\", \"Repo\"]]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Answer\"] = df[\"Answer\"].apply(clean_text)\n",
    "df[\"Answer\"] = df[\"Answer\"].apply(text_cleaner)\n",
    "df[\"Answer\"] = df[\"Answer\"].apply(clean_emoji)\n",
    "df[\"Context\"] = df[\"Context\"].apply(text_cleaner)\n",
    "df[\"Answer\"].values[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56da770",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ['allennlp', 'autojump', 'typer', 'spotify-downloader', 'spleeter', 'python-fire', 'numpy-ml', 'magenta'] \n",
    "   \n",
    "# selecting rows based on condition \n",
    "df = df[df['Repo'].isin(options)]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d8d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Answer\"].values[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"scripts/readme_qa.csv\", index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985ccd1",
   "metadata": {},
   "source": [
    "# Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee64205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# from torchmetrics.text.bert import BERTScore\n",
    "import bert_score\n",
    "import re\n",
    "\n",
    "with open(\"README_LLAMA2_7B_CHAT_GPTQ.md\", 'r', encoding='utf-8') as f:\n",
    "    pred = f.read()\n",
    "with open(\"spleeter/README.md\", 'r', encoding='utf-8') as f:\n",
    "    target = f.read()\n",
    "\n",
    "pred = re.sub(r' +', ' ', pred)\n",
    "target = re.sub(r' +', ' ', target)\n",
    "P, R, F1 = bert_score.score([pred], [target], lang='en', model_type='roberta-large', verbose=True)\n",
    "print(P,R,F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02cebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Download the required NLTK data\n",
    "# nltk.download('punkt')\n",
    "\n",
    "pred = \"\"\n",
    "target = \"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GPTQ\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference_tokens = tokenizer.tokenize(reference)\n",
    "    candidate_tokens = tokenizer.tokenize(candidate)\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothie)\n",
    "\n",
    "bleu_score = calculate_bleu(target, pred)\n",
    "print(bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f3f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymarkdown.api import PyMarkdownApi\n",
    "\n",
    "source_path = \"README_LLAMA2_7B_CHAT_GPTQ.md\"\n",
    "errors = PyMarkdownApi().scan_path(source_path)\n",
    "print(len(errors.scan_failures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376cd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
