{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Fine-Tuning with Llama 2, Bits and Bytes, and QLoRA\n","\n","Today we'll explore fine-tuning the Llama 2 model available on Kaggle Models using QLoRA, Bits and Bytes, and PEFT.\n","\n","- QLoRA: [Quantized Low Rank Adapters](https://arxiv.org/pdf/2305.14314.pdf) - this is a method for fine-tuning LLMs that uses a small number of quantized, updateable parameters to limit the complexity of training. This technique also allows those small sets of parameters to be added efficiently into the model itself, which means you can do fine-tuning on lots of data sets, potentially, and swap these \"adapters\" into your model when necessary.\n","- [Bits and Bytes](https://github.com/TimDettmers/bitsandbytes): An excellent package by Tim Dettmers et al., which provides a lightweight wrapper around custom CUDA functions that make LLMs go faster - optimizers, matrix mults, and quantization. In this notebook we'll be using the library to load our model as efficiently as possible.\n","- [PEFT](https://github.com/huggingface/peft): An excellent Huggingface library that enables a number Parameter Efficient Fine-tuning (PEFT) methods, which again make it less expensive to fine-tune LLMs - especially on more lightweight hardware like that present in Kaggle notebooks.\n","\n","Many thanks to [Bojan Tunguz](https://www.kaggle.com/tunguz) for his excellent [Jeopardy dataset](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions)!\n","\n","This notebook is based on [an excellent example from LangChain](https://github.com/asokraju/LangChainDatasetForge/blob/main/Finetuning_Falcon_7b.ipynb)."]},{"cell_type":"markdown","metadata":{"id":"wIa8WRIHvuZy"},"source":["## Package Installation\n","\n","Note that we're loading very specific versions of these libraries. Dependencies in this space can be quite difficult to untangle, and simply taking the latest version of each library can lead to conflicting version requirements. It's a good idea to take note of which versions work for your particular use case, and `pip install` them directly."]},{"cell_type":"code","execution_count":2,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-05-23T12:21:58.849851Z","iopub.status.busy":"2024-05-23T12:21:58.849106Z","iopub.status.idle":"2024-05-23T12:24:42.714261Z","shell.execute_reply":"2024-05-23T12:24:42.713141Z","shell.execute_reply.started":"2024-05-23T12:21:58.849820Z"},"id":"g3agEMJEnKsl","outputId":"56ab843a-e9c8-4158-8c04-18d77aec44fd","trusted":true},"outputs":[],"source":["!pip install -qqq bitsandbytes\n","!pip install -qqq torch\n","!pip install -qqq -U git+https://github.com/huggingface/transformers.git\n","!pip install -qqq -U git+https://github.com/huggingface/peft.git\n","!pip install -qqq -U git+https://github.com/huggingface/accelerate.git\n","!pip install -qqq datasets\n","!pip install -qqq loralib\n","!pip install -qqq einops"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T11:09:09.148558Z","iopub.status.busy":"2024-05-21T11:09:09.147392Z","iopub.status.idle":"2024-05-21T11:09:30.589305Z","shell.execute_reply":"2024-05-21T11:09:30.588042Z","shell.execute_reply.started":"2024-05-21T11:09:09.148520Z"},"trusted":true},"outputs":[],"source":["# !pip install -q auto-gptq==0.5.0"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:24:42.716682Z","iopub.status.busy":"2024-05-23T12:24:42.716361Z","iopub.status.idle":"2024-05-23T12:24:48.696015Z","shell.execute_reply":"2024-05-23T12:24:48.695079Z","shell.execute_reply.started":"2024-05-23T12:24:42.716652Z"},"id":"dv3aJo8Anhyw","outputId":"66f7b274-28a9-45b4-c0e6-d25194424594","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","import json\n","import os\n","from pprint import pprint\n","import bitsandbytes as bnb\n","import torch\n","import torch.nn as nn\n","import transformers\n","from datasets import load_dataset, Dataset\n","from huggingface_hub import notebook_login\n","\n","from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n","from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""]},{"cell_type":"markdown","metadata":{"id":"AgqJriqjwMyK"},"source":["# Loading and preparing our model\n","\n","We're going to use the Llama 2 7B model for our test. We'll be using Bits and Bytes to load it in 4-bit format, which should reduce memory consumption considerably, at a cost of some accuracy.\n","\n","Note the parameters in `BitsAndBytesConfig` - this is a fairly standard 4-bit quantization configuration, loading the weights in 4-bit format, using a straightforward format (`normal float 4`) with double quantization to improve QLoRA's resolution. The weights are converted back to `bfloat16` for weight updates, then the extra precision is discarded."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"mllA2Ka_ol13","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["CUDA extension not installed.\n","CUDA extension not installed.\n","/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/modeling_utils.py:4484: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n","  warnings.warn(\n","You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]}],"source":["model = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n","MODEL_NAME = model\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n","    quantization_config=bnb_config\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:24:48.697613Z","iopub.status.busy":"2024-05-23T12:24:48.697147Z","iopub.status.idle":"2024-05-23T12:25:25.163356Z","shell.execute_reply":"2024-05-23T12:25:25.162575Z","shell.execute_reply.started":"2024-05-23T12:24:48.697586Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\n"]}],"source":["model = \"google/gemma-2b-it\"\n","MODEL_NAME = model\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n","    quantization_config=bnb_config,\n","    token='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'\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token='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')\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["CUDA extension not installed.\n","CUDA extension not installed.\n"]},{"name":"stderr","output_type":"stream","text":["/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/modeling_utils.py:4484: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n","  warnings.warn(\n","You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]}],"source":["MODEL_NAME = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_NAME,\n","        torch_dtype=torch.float16,\n","        trust_remote_code=True,\n","        device_map=\"auto\",\n","    )\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"markdown","metadata":{},"source":["Below, we'll use a nice PEFT wrapper to set up our model for training / fine-tuning. Specifically this function sets the output embedding layer to allow gradient updates, as well as performing some type casting on various components to ensure the model is ready to be updated."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.165686Z","iopub.status.busy":"2024-05-23T12:25:25.165377Z","iopub.status.idle":"2024-05-23T12:25:25.184291Z","shell.execute_reply":"2024-05-23T12:25:25.183565Z","shell.execute_reply.started":"2024-05-23T12:25:25.165661Z"},"id":"na8DUq4IoqpB","trusted":true},"outputs":[],"source":["model = prepare_model_for_kbit_training(model)"]},{"cell_type":"markdown","metadata":{},"source":["Below, we define some helper functions - their purpose is to properly identify our update layers so we can... update them!"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.185688Z","iopub.status.busy":"2024-05-23T12:25:25.185329Z","iopub.status.idle":"2024-05-23T12:25:25.193947Z","shell.execute_reply":"2024-05-23T12:25:25.193083Z","shell.execute_reply.started":"2024-05-23T12:25:25.185654Z"},"trusted":true},"outputs":[{"data":{"text/plain":["GemmaForCausalLM(\n","  (model): GemmaModel(\n","    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n","    (layers): ModuleList(\n","      (0-17): 18 x GemmaDecoderLayer(\n","        (self_attn): GemmaSdpaAttention(\n","          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n","          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n","          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","          (rotary_emb): GemmaRotaryEmbedding()\n","        )\n","        (mlp): GemmaMLP(\n","          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n","          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n","          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n","          (act_fn): PytorchGELUTanh()\n","        )\n","        (input_layernorm): GemmaRMSNorm()\n","        (post_attention_layernorm): GemmaRMSNorm()\n","      )\n","    )\n","    (norm): GemmaRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",")"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.195344Z","iopub.status.busy":"2024-05-23T12:25:25.195023Z","iopub.status.idle":"2024-05-23T12:25:25.203441Z","shell.execute_reply":"2024-05-23T12:25:25.202469Z","shell.execute_reply.started":"2024-05-23T12:25:25.195319Z"},"trusted":true},"outputs":[],"source":["import re\n","def get_num_layers(model):\n","    numbers = set()\n","    for name, _ in model.named_parameters():\n","        for number in re.findall(r'\\d+', name):\n","            numbers.add(int(number))\n","    print(max(numbers))\n","    return max(numbers)\n","\n","def get_last_layer_linears(model):\n","    names = []\n","    \n","    num_layers = get_num_layers(model)\n","    for name, module in model.named_modules():\n","        if str(num_layers) in name and not \"encoder\" in name:\n","            if isinstance(module, torch.nn.Linear):\n","                names.append(name)\n","    print(names)\n","    return names"]},{"cell_type":"markdown","metadata":{},"source":["## LORA config\n","\n","Some key elements from this configuration:\n","1. `r` is the width of the small update layer. In theory, this should be set wide enough to capture the complexity of the problem you're attempting to fine-tune for. More simple problems may be able to get away with smaller `r`. In our case, we'll go very small, largely for the sake of speed.\n","2. `target_modules` is set using our helper functions - every layer identified by that function will be included in the PEFT update."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.205599Z","iopub.status.busy":"2024-05-23T12:25:25.205231Z","iopub.status.idle":"2024-05-23T12:25:25.236478Z","shell.execute_reply":"2024-05-23T12:25:25.235650Z","shell.execute_reply.started":"2024-05-23T12:25:25.205568Z"},"id":"C4Qk3fGLoraw","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["17\n","['model.layers.17.self_attn.q_proj', 'model.layers.17.self_attn.k_proj', 'model.layers.17.self_attn.v_proj', 'model.layers.17.self_attn.o_proj', 'model.layers.17.mlp.gate_proj', 'model.layers.17.mlp.up_proj', 'model.layers.17.mlp.down_proj']\n"]}],"source":["config = LoraConfig(\n","    r=2,\n","    lora_alpha=32,\n","    target_modules=get_last_layer_linears(model),\n","    # target_modules = [\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","model = get_peft_model(model, config)"]},{"cell_type":"markdown","metadata":{},"source":["## Load some data\n","\n","Here, we're loading a 200,000 question Jeopardy dataset. In the interests of time we won't load all of them - just the first 1000 - but we'll fine-tune our model using the question and answers. Note that what we're training the model to do is use its existing knowledge (plus whatever little it learns from our question-answer pairs) to answer questions in the *format* we want, specifically short answers."]},{"cell_type":"code","execution_count":125,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.237679Z","iopub.status.busy":"2024-05-23T12:25:25.237419Z","iopub.status.idle":"2024-05-23T12:25:25.298238Z","shell.execute_reply":"2024-05-23T12:25:25.297528Z","shell.execute_reply.started":"2024-05-23T12:25:25.237656Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"readme_qa.csv\")\n","df.columns = [str(q).strip() for q in df.columns]\n","\n","data = Dataset.from_pandas(df)"]},{"cell_type":"code","execution_count":126,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.299509Z","iopub.status.busy":"2024-05-23T12:25:25.299244Z","iopub.status.idle":"2024-05-23T12:25:25.305588Z","shell.execute_reply":"2024-05-23T12:25:25.304719Z","shell.execute_reply.started":"2024-05-23T12:25:25.299485Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array(['Explore popular APIs and see them work in Postman. <br > <p> <a href=\"https://apilayer.com\"> <div> <img src=\".github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png\" width=\"250\" alt=\"APILayer Logo\" /> </div> </a> </p> [APILayer](https://apilayer.com/) is the fastest way to integrate APIs into any product. They created this repository to support the community in easily finding public APIs. Explore their collections on the [Postman API Network](https://www.postman.com/apilayer/workspace/apilayer/overview).',\n","       '| API | Description | Call this API | |:---|:---|:---| | [IP Stack](https://ipstack.com/) | Locate and Identify Website Visitors by IP Address | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)| | [Marketstack](https://marketstack.com/) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)| | [Weatherstack](https://weatherstack.com/) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)| | [Numverify](https://numverify.com/) | Global Phone Number Validation & Lookup JSON API |[<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)| | [Fixer](https://fixer.io/) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)| <br >',\n","       '| API | Description | Auth | Call this API | |:---|:---|:---|:---| | [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/25426789-12bc9867-e424-4de8-b4ee-662632714f6c?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D25426789-12bc9867-e424-4de8-b4ee-662632714f6c%26entityType%3Dcollection%26workspaceId%3De4d9a7d3-b961-474e-a054-51861ed481f6) | | [Sportmonks Football](https://docs.sportmonks.com/football/) | Football score/schedule, news API, tv channels, stats, history, display standing e.g. epl, la liga | `apiKey` | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/25426789-b21c360e-6b87-431d-9b39-74e824f29e45?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D25426789-b21c360e-6b87-431d-9b39-74e824f29e45%26entityType%3Dcollection%26workspaceId%3De4d9a7d3-b961-474e-a054-51861ed481f6)| | [Google Maps](https://developers.notion.com) | Create/customize digital maps based on Google Maps data | `apiKey`  | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/25426789-2c9bbe63-f45b-45d4-9327-ec3376542b64?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D25426789-2c9bbe63-f45b-45d4-9327-ec3376542b64%26entityType%3Dcollection%26workspaceId%3De4d9a7d3-b961-474e-a054-51861ed481f6)| | [Notion](https://developers.notion.com) | Integrate with Notion | `apiKey`  | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/25426789-68f0e9e4-b7bc-4543-945a-b50ae385c540?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D25426789-68f0e9e4-b7bc-4543-945a-b50ae385c540%26entityType%3Dcollection%26workspaceId%3De4d9a7d3-b961-474e-a054-51861ed481f6)| | [Plaid](https://www.plaid.com/docs) | Connect with user\\'s bank accounts and access transaction data\\t | `apiKey` | [<img src=\"https://run.pstmn.io/button.svg\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](https://god.gw.postman.com/run-collection/25426789-ae5e66eb-613e-4553-a99c-0f58d875ff88?action=collection%2Ffork&source=rip_markdown&collection-url=entityId%3D25426789-ae5e66eb-613e-4553-a99c-0f58d875ff88%26entityType%3Dcollection%26workspaceId%3De4d9a7d3-b961-474e-a054-51861ed481f6)| <br >',\n","       '* [Animals](#animals) * [Anime](#anime) * [Art & Design](#art--design) * [Authentication & Authorization](#authentication--authorization) * [Blockchain](#blockchain) * [Business](#business) * [Currency Exchange](#currency-exchange) * [Social](#social) <br >',\n","       '<br > <strong>Get Involved</strong> * [Contributing Guide](CONTRIBUTING.md) * [API for this project](https://github.com/davemachado/public-api) * [Issues](https://github.com/public-apis/public-apis/issues) * [Pull Requests](https://github.com/public-apis/public-apis/pulls) * [LICENSE](LICENSE) Own an API? Publish your own [Run in Postman](https://learning.postman.com/docs/collaborating-in-postman/public-api-network/public-api-network-overview/) button. <br /> <strong>More Resources</strong> * [Postman API Network](https://postman.com/explore) * [Free APIs](https://free-apis.github.io) * [Dev Resources](https://devresourc.es/tools-and-utilities/public-apis) * [Apihouse](https://apihouse.vercel.app) * [Collective APIs](https://collective-api.vercel.app) </div> </details> <br /> <br />'],\n","      dtype=object)"]},"execution_count":126,"metadata":{},"output_type":"execute_result"}],"source":["df[\"Answer\"].values[0:5]"]},{"cell_type":"code","execution_count":127,"metadata":{},"outputs":[{"data":{"text/plain":["13848"]},"execution_count":127,"metadata":{},"output_type":"execute_result"}],"source":["len(df)"]},{"cell_type":"code","execution_count":128,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Question</th>\n","      <th>Context</th>\n","      <th>Answer</th>\n","      <th>Repo Url</th>\n","      <th>Repo</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>Provide the README content for the section wit...</td>\n","      <td>Discussions in issues and pull requests:\\n    ...</td>\n","      <td>Explore popular APIs and see them work in Post...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>Provide the README content for the section wit...</td>\n","      <td>Discussions in issues and pull requests:\\n    ...</td>\n","      <td>| API | Description | Call this API | |:---|:-...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>Provide the README content for the section wit...</td>\n","      <td>Discussions in issues and pull requests:\\n    ...</td>\n","      <td>| API | Description | Auth | Call this API | |...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.0</td>\n","      <td>Provide the README content for the section wit...</td>\n","      <td># check each category for the minimum number o...</td>\n","      <td>* [Animals](#animals) * [Anime](#anime) * [Art...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.0</td>\n","      <td>Provide the README content for the section wit...</td>\n","      <td>Discussions in issues and pull requests:\\n    ...</td>\n","      <td>&lt;br &gt; &lt;strong&gt;Get Involved&lt;/strong&gt; * [Contrib...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                           Question  \\\n","0         0.0  Provide the README content for the section wit...   \n","1         1.0  Provide the README content for the section wit...   \n","2         2.0  Provide the README content for the section wit...   \n","3         3.0  Provide the README content for the section wit...   \n","4         4.0  Provide the README content for the section wit...   \n","\n","                                             Context  \\\n","0  Discussions in issues and pull requests:\\n    ...   \n","1  Discussions in issues and pull requests:\\n    ...   \n","2  Discussions in issues and pull requests:\\n    ...   \n","3  # check each category for the minimum number o...   \n","4  Discussions in issues and pull requests:\\n    ...   \n","\n","                                              Answer  \\\n","0  Explore popular APIs and see them work in Post...   \n","1  | API | Description | Call this API | |:---|:-...   \n","2  | API | Description | Auth | Call this API | |...   \n","3  * [Animals](#animals) * [Anime](#anime) * [Art...   \n","4  <br > <strong>Get Involved</strong> * [Contrib...   \n","\n","                                     Repo Url         Repo  \n","0  https://github.com/public-apis/public-apis  public-apis  \n","1  https://github.com/public-apis/public-apis  public-apis  \n","2  https://github.com/public-apis/public-apis  public-apis  \n","3  https://github.com/public-apis/public-apis  public-apis  \n","4  https://github.com/public-apis/public-apis  public-apis  "]},"execution_count":128,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":95,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T06:36:39.569842Z","iopub.status.busy":"2024-05-08T06:36:39.569492Z","iopub.status.idle":"2024-05-08T06:36:39.577848Z","shell.execute_reply":"2024-05-08T06:36:39.576834Z","shell.execute_reply.started":"2024-05-08T06:36:39.569808Z"},"id":"_Pb9RA5NovNS","trusted":true},"outputs":[],"source":["# prompt = df[\"Question\"].values[0] + \". Answer as briefly as possible: \".strip()\n","# prompt"]},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.309091Z","iopub.status.busy":"2024-05-23T12:25:25.308834Z","iopub.status.idle":"2024-05-23T12:25:25.331344Z","shell.execute_reply":"2024-05-23T12:25:25.330676Z","shell.execute_reply.started":"2024-05-23T12:25:25.309069Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Context</th>\n","      <th>Answer</th>\n","      <th>Repo Url</th>\n","      <th>Repo</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Provide the README content for the section wit...</td>\n","      <td>Discussions in issues and pull requests:\\n    ...</td>\n","      <td>Explore popular APIs and see them work in Post...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Provide the README content for the section wit...</td>\n","      <td>Discussions in issues and pull requests:\\n    ...</td>\n","      <td>| API | Description | Call this API | |:---|:-...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Provide the README content for the section wit...</td>\n","      <td>Discussions in issues and pull requests:\\n    ...</td>\n","      <td>| API | Description | Auth | Call this API | |...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Provide the README content for the section wit...</td>\n","      <td># check each category for the minimum number o...</td>\n","      <td>* [Animals](#animals) * [Anime](#anime) * [Art...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Provide the README content for the section wit...</td>\n","      <td>Discussions in issues and pull requests:\\n    ...</td>\n","      <td>&lt;br &gt; &lt;strong&gt;Get Involved&lt;/strong&gt; * [Contrib...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Question  \\\n","0  Provide the README content for the section wit...   \n","1  Provide the README content for the section wit...   \n","2  Provide the README content for the section wit...   \n","3  Provide the README content for the section wit...   \n","4  Provide the README content for the section wit...   \n","\n","                                             Context  \\\n","0  Discussions in issues and pull requests:\\n    ...   \n","1  Discussions in issues and pull requests:\\n    ...   \n","2  Discussions in issues and pull requests:\\n    ...   \n","3  # check each category for the minimum number o...   \n","4  Discussions in issues and pull requests:\\n    ...   \n","\n","                                              Answer  \\\n","0  Explore popular APIs and see them work in Post...   \n","1  | API | Description | Call this API | |:---|:-...   \n","2  | API | Description | Auth | Call this API | |...   \n","3  * [Animals](#animals) * [Anime](#anime) * [Art...   \n","4  <br > <strong>Get Involved</strong> * [Contrib...   \n","\n","                                     Repo Url         Repo  \n","0  https://github.com/public-apis/public-apis  public-apis  \n","1  https://github.com/public-apis/public-apis  public-apis  \n","2  https://github.com/public-apis/public-apis  public-apis  \n","3  https://github.com/public-apis/public-apis  public-apis  \n","4  https://github.com/public-apis/public-apis  public-apis  "]},"execution_count":129,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","df.replace('', np.nan, inplace=True)\n","df.dropna(subset=[\"Answer\"], inplace=True)\n","df = df[[\"Question\", \"Context\", \"Answer\", \"Repo Url\", \"Repo\"]]\n","df.head()"]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Context</th>\n","      <th>Answer</th>\n","      <th>Repo Url</th>\n","      <th>Repo</th>\n","      <th>detect</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Provide the README content for the section wit...</td>\n","      <td>Discussions in issues and pull requests:\\n    ...</td>\n","      <td>Explore popular APIs and see them work in Post...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Provide the README content for the section wit...</td>\n","      <td>Discussions in issues and pull requests:\\n    ...</td>\n","      <td>| API | Description | Call this API | |:---|:-...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Provide the README content for the section wit...</td>\n","      <td>Discussions in issues and pull requests:\\n    ...</td>\n","      <td>| API | Description | Auth | Call this API | |...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Provide the README content for the section wit...</td>\n","      <td># check each category for the minimum number o...</td>\n","      <td>* [Animals](#animals) * [Anime](#anime) * [Art...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","      <td>en</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Provide the README content for the section wit...</td>\n","      <td>Discussions in issues and pull requests:\\n    ...</td>\n","      <td>&lt;br &gt; &lt;strong&gt;Get Involved&lt;/strong&gt; * [Contrib...</td>\n","      <td>https://github.com/public-apis/public-apis</td>\n","      <td>public-apis</td>\n","      <td>en</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Question  \\\n","0  Provide the README content for the section wit...   \n","1  Provide the README content for the section wit...   \n","2  Provide the README content for the section wit...   \n","3  Provide the README content for the section wit...   \n","4  Provide the README content for the section wit...   \n","\n","                                             Context  \\\n","0  Discussions in issues and pull requests:\\n    ...   \n","1  Discussions in issues and pull requests:\\n    ...   \n","2  Discussions in issues and pull requests:\\n    ...   \n","3  # check each category for the minimum number o...   \n","4  Discussions in issues and pull requests:\\n    ...   \n","\n","                                              Answer  \\\n","0  Explore popular APIs and see them work in Post...   \n","1  | API | Description | Call this API | |:---|:-...   \n","2  | API | Description | Auth | Call this API | |...   \n","3  * [Animals](#animals) * [Anime](#anime) * [Art...   \n","4  <br > <strong>Get Involved</strong> * [Contrib...   \n","\n","                                     Repo Url         Repo detect  \n","0  https://github.com/public-apis/public-apis  public-apis     en  \n","1  https://github.com/public-apis/public-apis  public-apis     en  \n","2  https://github.com/public-apis/public-apis  public-apis     en  \n","3  https://github.com/public-apis/public-apis  public-apis     en  \n","4  https://github.com/public-apis/public-apis  public-apis     en  "]},"execution_count":130,"metadata":{},"output_type":"execute_result"}],"source":["from langdetect import detect\n","df['detect'] = detect(str(df['Answer']))\n","df.head()"]},{"cell_type":"code","execution_count":131,"metadata":{},"outputs":[{"data":{"text/plain":["12803"]},"execution_count":131,"metadata":{},"output_type":"execute_result"}],"source":["df = df[df['detect'] == 'en']\n","df = df[[\"Question\", \"Context\", \"Answer\", \"Repo Url\", \"Repo\"]]\n","len(df)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.332735Z","iopub.status.busy":"2024-05-23T12:25:25.332383Z","iopub.status.idle":"2024-05-23T12:25:25.338916Z","shell.execute_reply":"2024-05-23T12:25:25.337852Z","shell.execute_reply.started":"2024-05-23T12:25:25.332702Z"},"trusted":true},"outputs":[],"source":["def clean_text(text):\n","    # Define the regular expression pattern for HTTP URLs\n","    http_pattern = re.compile(r'http://[^\\s]+')\n","    # Remove HTTP URLs\n","    text = http_pattern.sub('', str(text))\n","\n","    https_pattern = re.compile(r'https://[^\\s]+')\n","    # Remove HTTPS URLs\n","    text = https_pattern.sub('', str(text))\n","    \n","    # Define the regular expression pattern for <img> tags\n","    img_pattern = re.compile(r'<img[^>]*>')\n","    # Remove <img> tags\n","    text = img_pattern.sub('', str(text))\n","    \n","    return text"]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[],"source":["import re\n","def clean_emoji(tx):\n","    emoji_pattern = re.compile(\"[\"\n","                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                           u\"\\U0001F300-\\U0001F5FF\"  # symbols \n","                           u\"\\U0001F680-\\U0001F6FF\"  # transport \n","                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n","                           u\"\\U00002702-\\U000027B0\"\n","                           u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","\n","    return emoji_pattern.sub(r'', tx)\n","\n","def text_cleaner(tx):\n","\n","    text = re.sub(r\"won\\'t\", \"would not\", tx)\n","    text = re.sub(r\"im\", \"i am\", tx)\n","    text = re.sub(r\"Im\", \"I am\", tx)\n","    text = re.sub(r\"can\\'t\", \"can not\", text)\n","    text = re.sub(r\"don\\'t\", \"do not\", text)\n","    text = re.sub(r\"shouldn\\'t\", \"should not\", text)\n","    text = re.sub(r\"needn\\'t\", \"need not\", text)\n","    text = re.sub(r\"hasn\\'t\", \"has not\", text)\n","    text = re.sub(r\"haven\\'t\", \"have not\", text)\n","    text = re.sub(r\"weren\\'t\", \"were not\", text)\n","    text = re.sub(r\"mightn\\'t\", \"might not\", text)\n","    text = re.sub(r\"didn\\'t\", \"did not\", text)\n","    text = re.sub(r\"n\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"\\'s\", \" is\", text)\n","    text = re.sub(r\"\\'d\", \" would\", text)\n","    text = re.sub(r\"\\'ll\", \" will\", text)\n","    text = re.sub(r\"\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"\\'m\", \" am\", text)\n","    # text = re.sub('https?://\\S+|www\\.\\S+', '<URL>', text)\n","    # text = re.sub(r'[^a-zA-Z0-9\\!\\?\\.\\@]',' ' , text)\n","    text = re.sub(r'https?://[^\\s\\\")]+', '<URL>', text)\n","    text = re.sub(r'http?://[^\\s\\\")]+', '<URL>', text)\n","    text = re.sub(r'http%3A%2F%2F[^\\s\\\")]+', '<URL>', text)\n","    text = re.sub(r'https%3A%2F%2F[^\\s\\\")]+', '<URL>', text)\n","    text = re.sub(r'[!]+' , '!' , text)\n","    text = re.sub(r'[?]+' , '?' , text)\n","    text = re.sub(r'[.]+' , '.' , text)\n","    text = re.sub(r'[@]+' , '@' , text)\n","    text = re.sub(r'unk' , '<UNK>' , text)\n","    text = re.sub('\\n', '<NL>', text)\n","    text = re.sub('\\t', '<TAB>', text)\n","    # text = re.sub(r'\\s+', '<SP>', text)\n","    # text = re.sub(r'(<img[^>]*\\bsrc=\")[^\"]*(\")', '<img src=<IMG_SRC>', text)\n","    \n","    # text = text.lower()\n","    # text = re.sub(r'[ ]+' , ' ' , text)\n","\n","    return text"]},{"cell_type":"code","execution_count":133,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.340158Z","iopub.status.busy":"2024-05-23T12:25:25.339882Z","iopub.status.idle":"2024-05-23T12:25:25.362151Z","shell.execute_reply":"2024-05-23T12:25:25.361115Z","shell.execute_reply.started":"2024-05-23T12:25:25.340135Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array(['Explore popular APIs and see them work in Postman. <br > <p> <a href=\"<URL>\"> <div> <img src=\".github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png\" width=\"250\" alt=\"APILayer Logo\" /> </div> </a> </p> [APILayer](<URL>) is the fastest way to integrate APIs into any product. They created this repository to support the community in easily finding public APIs. Explore their collections on the [Postman API Network](<URL>).',\n","       '| API | Description | Call this API | |:---|:---|:---| | [IP Stack](<URL>) | Locate and Identify Website Visitors by IP Address | [<img src=\"<URL>\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](<URL>)| | [Marketstack](<URL>) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [<img src=\"<URL>\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](<URL>)| | [Weatherstack](<URL>) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [<img src=\"<URL>\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](<URL>)| | [Numverify](<URL>) | Global Phone Number Validation & Lookup JSON API |[<img src=\"<URL>\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](<URL>)| | [Fixer](<URL>) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[<img src=\"<URL>\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](<URL>)| <br >',\n","       '| API | Description | Auth | Call this API | |:---|:---|:---|:---| | [HTTP Cat](<URL>) | Cat for every HTTP Status | No | [<img src=\"<URL>\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](<URL>) | | [Sportmonks Football](<URL>) | Football score/schedule, news API, tv channels, stats, history, display standing e.g. epl, la liga | `apiKey` | [<img src=\"<URL>\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](<URL>)| | [Google Maps](<URL>) | Create/customize digital maps based on Google Maps data | `apiKey`  | [<img src=\"<URL>\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](<URL>)| | [Notion](<URL>) | Integrate with Notion | `apiKey`  | [<img src=\"<URL>\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](<URL>)| | [Plaid](<URL>) | Connect with user is bank accounts and access transaction data<TAB> | `apiKey` | [<img src=\"<URL>\" alt=\"Run In Postman\" style=\"width: 128px; height: 32px;\">](<URL>)| <br >',\n","       '* [Animals](#animals) * [Anime](#anime) * [Art & Design](#art--design) * [Authentication & Authorization](#authentication--authorization) * [Blockchain](#blockchain) * [Business](#business) * [Currency Exchange](#currency-exchange) * [Social](#social) <br >',\n","       '<br > <strong>Get Involved</strong> * [Contributing Guide](CONTRIBUTING.md) * [API for this project](<URL>) * [Issues](<URL>) * [Pull Requests](<URL>) * [LICENSE](LICENSE) Own an API? Publish your own [Run in Postman](<URL>) button. <br /> <strong>More Resources</strong> * [Postman API Network](<URL>) * [Free APIs](<URL>) * [Dev Resources](<URL>) * [Apihouse](<URL>) * [Collective APIs](<URL>) </div> </details> <br /> <br />'],\n","      dtype=object)"]},"execution_count":133,"metadata":{},"output_type":"execute_result"}],"source":["# df[\"Answer\"] = df[\"Answer\"].apply(clean_text)\n","df[\"Answer\"] = df[\"Answer\"].apply(text_cleaner)\n","df[\"Answer\"] = df[\"Answer\"].apply(clean_emoji)\n","df[\"Context\"] = df[\"Context\"].apply(text_cleaner)\n","df[\"Answer\"].values[0:5]"]},{"cell_type":"code","execution_count":135,"metadata":{},"outputs":[{"data":{"text/plain":["array(['Our Python package gives you more control over each setting. To replicate and connect to LM Studio, use these settings: ```python from interpreter import interpreter interpreter.offline = True # Disables online features like Open Procedures interpreter.llm.model = \"openai/x\" # Tells OI to send messages in OpenAI is format interpreter.llm.api_key = \"fake_key\" # LiteLLM, which we use to talk to LM Studio, requires this interpreter.llm.api_base = \"<URL>\" # Point this at any OpenAI compatible server interpreter.chat() ```',\n","       'You can modify the `max_tokens` and `context_window` (in tokens) of locally running models. For local mode, smaller context windows will use less RAM, so we recommend trying a much shorter window (~1000) if it is failing / if it is slow. Make sure `max_tokens` is less than `context_window`. ```shell interpreter --local --max_tokens 1000 --context_window 3000 ```',\n","       'To help you inspect Open Interpreter we have a `--verbose` mode for debugging. You can activate verbose mode by using its flag (`interpreter --verbose`), or mid-chat: ```shell $ interpreter . > %verbose true <- Turns on verbose mode > %verbose false <- Turns off verbose mode ```',\n","       'In the interactive mode, you can use the below commands to enhance your experience. Here is a list of available commands: **Available Commands:** - `%verbose [true/false]`: Toggle verbose mode. Without arguments or with `true` it enters verbose mode. With `false` it exits verbose mode. - `%reset`: Resets the current session is conversation. - `%undo`: Removes the previous user message and the AI is response from the message history. - `%tokens [prompt]`: (_Experimental_) Calculate the tokens that will be sent with the next prompt as context and estimate their cost. Optionally calculate the tokens and estimated cost of a `prompt` if one is provided. Relies on [LiteLLM is `cost_per_token()` method](<URL>) for estimated costs. - `%help`: Show the help message.',\n","       'Open Interpreter allows you to set default behaviors using `yaml` files. This provides a flexible way to configure the interpreter without changing command-line arguments every time. Run the following command to open the profiles directory: ``` interpreter --profiles ``` You can add `yaml` files there. The default profile is named `default.yaml`.',\n","       'Open Interpreter supports multiple `yaml` files, allowing you to easily switch between configurations: ``` interpreter --profile my_profile.yaml ```',\n","       'The generator update enables Open Interpreter to be controlled via HTTP REST endpoints: ```python',\n","       'from fastapi import FastAPI from fastapi.responses import StreamingResponse from interpreter import interpreter app = FastAPI() @app.get(\"/chat\") def chat_endpoint(message: str): def event_stream(): for result in interpreter.chat(message, stream=True): yield f\"data: {result}\\\\n\\\\n\" return StreamingResponse(event_stream(), media_type=\"text/event-stream\") @app.get(\"/history\") def history_endpoint(): return interpreter.messages ``` ```shell pip install fastapi uvicorn uvicorn server:app --reload ``` You can also start a server identical to the one above by simply running `interpreter.server()`.',\n","       'The step-by-step guide for installing Open Interpreter on your Android device can be found in the [open-interpreter-termux repo](<URL>).',\n","       'Since generated code is executed in your local environment, it can interact with your files and system settings, potentially leading to unexpected outcomes like data loss or security risks. ** Open Interpreter will ask for user confirmation before executing code.** You can run `interpreter -y` or set `interpreter.auto_run = True` to bypass this confirmation, in which case: - Be cautious when requesting commands that modify files or system settings. - Watch Open Interpreter like a self-driving car, and be prepared to end the process by closing your terminal. - Consider running Open Interpreter in a restricted environment like Google Colab or Replit. These environments are more isolated, reducing the risks of executing arbitrary code. There is **experimental** support for a [safe mode](<URL>) to help mitigate some risks.',\n","       'Open Interpreter equips a [function-calling language model](<URL>) with an `exec()` function, which accepts a `language` (like \"Python\" or \"JavaScript\") and `code` to run. We then stream the model is messages, code, and your system is outputs to the terminal as Markdown.',\n","       'The full [documentation](<URL>) is accessible on-the-go without the need for an internet connection. [Node](<URL>) is a pre-requisite: - Version 18.17.0 or any later 18.x.x version. - Version 20.3.0 or any later 20.x.x version. - Any version starting from 21.0.0 onwards, with no upper limit specified. Install [Mintlify](<URL>): ```bash npm i -g mintlify@latest ``` Change into the docs directory and run the appropriate command: ```bash',\n","       'cd ./docs',\n","       'mintlify dev ``` A new browser window should open. The documentation will be available at [<URL>) as long as the documentation server is running.',\n","       'Thank you for your interest in contributing! We welcome involvement from the community. Please see our [contributing guidelines](<URL>) for more details on how to get involved.',\n","       'Visit [our roadmap](<URL>) to preview the future of Open Interpreter. **Note**: This software is not affiliated with OpenAI. ![thumbnail-ncu](<URL>) > Having access to a junior programmer working at the speed of your fingertips . can make new workflows effortless and efficient, as well as open the benefits of programming to new audiences. > > — _OpenAI is Code Interpreter Release_ <br>',\n","       'This repository contains JAX example code for loading and running the Grok-1 open-weights model. Make sure to download the checkpoint and place the `ckpt-0` directory in `checkpoints` - see [Downloading the weights](#downloading-the-weights) Then, run ```shell pip install -r requirements.txt python run.py ``` to test the code. The script loads the checkpoint and samples from the model on a test input. Due to the large size of the model (314B parameters), a machine with enough GPU memory is required to test the model with the example code. The implementation of the MoE layer in this repository is not efficient. The implementation was chosen to avoid the need for custom kernels to validate the correctness of the model.',\n","       'Grok-1 is currently designed with the following specifications: - **Parameters:** 314B - **Architecture:** Mixture of 8 Experts (MoE) - **Experts Utilization:** 2 experts used per token - **Layers:** 64 - **Attention Heads:** 48 for queries, 8 for keys/values - **Embedding Size:** 6,144 - **Tokenization:** SentencePiece tokenizer with 131,072 tokens - **Additional Features:** - Rotary embeddings (RoPE) - Supports activation sharding and 8-bit quantization - **Maximum Sequence Length (context):** 8,192 tokens',\n","       'You can download the weights using a torrent client and this magnet link: ``` magnet:?xt=urn:btih:5f96d43576e3d386c9ba65b883210a393b68210e&tr=<URL> ``` or directly using [HuggingFace 🤗 Hub](<URL>): ``` git clone <URL> && cd grok-1 pip install huggingface_hub[hf_transfer] huggingface-cli download xai-org/grok-1 --repo-type model --include ckpt-0/* --local-dir checkpoints --local-dir-use-symlinks False ```',\n","       'The code and associated Grok-1 weights in this release are licensed under the Apache 2.0 license. The license only applies to the source files in this repository and the model weights of Grok-1.'],\n","      dtype=object)"]},"execution_count":135,"metadata":{},"output_type":"execute_result"}],"source":["df[\"Answer\"].values[-20:]"]},{"cell_type":"code","execution_count":136,"metadata":{},"outputs":[],"source":["df.to_csv(\"readme_qa_cleaned_v2.csv\", index =False)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:26:49.814219Z","iopub.status.busy":"2024-05-23T12:26:49.813856Z","iopub.status.idle":"2024-05-23T12:26:49.824269Z","shell.execute_reply":"2024-05-23T12:26:49.823392Z","shell.execute_reply.started":"2024-05-23T12:26:49.814190Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'You are an AI assistant for a software project called diagrams. You are trained on all the docs that makes up this project.\\n    The docs for the project is located at https://github.com/mingrammer/diagrams.\\n    You are given a repository which might contain several modules and each module will contain a set of files.\\n    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\\n    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\\n\\n    Assume the reader is a smart developer but is not deeply familiar with diagrams.\\n    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\\n    If you don\\'t know how to fill up the readme.md file in one of its sections, leave that part blank. Don\\'t try to make up any content.\\n    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\\n    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\\n\\n    Question: Provide the README content for the section with heading \"Getting Started\" starting with ## Getting Started.\\n    Context:\\n    headerLinks: [\\n    {doc: \\'getting-started/installation\\', label: \\'Docs\\'},\\n    {doc: \\'guides/diagram\\', label: \\'Guides\\'},\\n    {doc: \\'nodes/aws\\', label: \\'Nodes\\'},\\n    {href: \\'https://github.com/mingrammer/diagrams\\', label: \\'GitHub\\'},\\n    {href: \\'https://www.buymeacoffee.com/mingrammer\\', label: \\'Sponsoring\\'},\\n  ],\\n\\n  headerIcon: \\'img/diagrams.ico\\',\\n  footerIcon: \\'img/diagrams.ico\\',\\n  favicon: \\'img/diagrams.ico\\',\\n\\n  colors: {\\n    primaryColor: \\'#5E73E5\\',\\n    secondaryColor: \\'#5E89E5\\',\\n  },\\n\\n  copyright: `Copyright © ${new Date().getFullYear()} mingrammer`,\\n\\n  highlight: {\\n    // Highlight.js theme to use for syntax highlighting in code blocks.\\n    theme: \\'default\\',\\n  },\\n\\n  // Add custom scripts here that would be placed in <script> tags.\\n  scripts: [\\'https://buttons.github.io/buttons.js\\'],\\n\\n  // On page navigation for the current documentation page.\\n  onPageNav: \\'separate\\',\\n  cleanUrl: true,\"CommandLineInterface\": \"CLI\",\\n            \"DeveloperTools\": \"DevTools\",\\n        },\\n        \"engagement\": {\\n            \"SimpleEmailServiceSes\": \"SES\",\\n        },\\n        \"general\": {\\n            \"GenericOfficeBuilding\": \"OfficeBuilding\",\\n        },\\n        \"integration\": {\\n            \"SimpleNotificationServiceSns\": \"SNS\",\\n            \"SimpleQueueServiceSqs\": \"SQS\",\\n            \"StepFunctions\": \"SF\",\\n        },\\n        \"iot\": {\\n            \"Freertos\": \"FreeRTOS\",\\n            \"IotHardwareBoard\": \"IotBoard\",\\n        },\\n        \"management\": {\\n            \"SystemsManager\": \"SSM\",\\n            \"SystemsManagerParameterStore\": \"ParameterStore\",\\n        },\\n        \"migration\": {\\n            \"ApplicationDiscoveryService\": \"ADS\",\\n            \"CloudendureMigration\": \"CEM\",\\n            \"DatabaseMigrationService\": \"DMS\",\\n            \"MigrationAndTransfer\": \"MAT\",\\n            \"ServerMigrationService\": \"SMS\",\\n        },\\n        \"ml\": {\\n            \"DeepLearningContainers\": \"DLC\",FILE_PREFIXES = {\\n    \"onprem\": (),\\n    \"aws\": (\"Amazon-\", \"AWS-\"),\\n    \"azure\": (\"Azure-\",),\\n    \"digitalocean\": (),\\n    \"gcp\": (\"Cloud-\",),\\n    \"firebase\": (\"Cloud-\",),\\n    \"ibm\": (),\\n    \"k8s\": (),\\n    \"alibabacloud\": (),\\n    \"oci\": (\"OCI-icon-\",),\\n    \"programming\": (),\\n    \"saas\": (),\\n    \"elastic\": (),\\n    \"outscale\": (),\\n    \"generic\": (),\\n    \"openstack\": (),\\n}\\n\\n#########################\\n#  Doc Auto Generation  #\\n#########################\\n\\nTMPL_APIDOC = \"apidoc.tmpl\"\\n\\n#########################\\n# Class Auto Generation #\\n#########################\\n\\nTMPL_MODULE = \"module.tmpl\"},\\n        \"gitops\": {\\n            \"Argocd\": \"ArgoCD\",\\n        },\\n        \"logging\": {\\n            \"Fluentbit\": \"FluentBit\",\\n            \"Rsyslog\": \"RSyslog\",\\n        },\\n        \"network\": {\\n            \"Etcd\": \"ETCD\",\\n            \"Haproxy\": \"HAProxy\",\\n            \"OpenServiceMesh\": \"OSM\",\\n            \"Opnsense\": \"OPNSense\",\\n            \"Pfsense\": \"PFSense\",\\n            \"Vyos\": \"VyOS\"\\n        },\\n        \"proxmox\": {\\n            \"Pve\": \"ProxmoxVE\",\\n        },\\n        \"queue\": {\\n            \"Activemq\": \"ActiveMQ\",\\n            \"Emqx\": \"EMQX\",\\n            \"Rabbitmq\": \"RabbitMQ\",\\n            \"Zeromq\": \"ZeroMQ\",\\n        },\\n        \"storage\": {\\n            \"Ceph\": \"CEPH\",\\n            \"CephOsd\": \"CEPH_OSD\",\\n        },\\n        \"workflow\": {\\n            \"Kubeflow\": \"KubeFlow\",\\n            \"Nifi\": \"NiFi\",\\n        }\\n    },\\n    \"aws\": {\\n        \"analytics\": {\\n            \"ElasticsearchService\": \"ES\",\\n        },\\n        \"business\": {\\n            \"AlexaForBusiness\": \"A4B\"\\n        },\\n\\n    Answer in Markdown:'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["project_name = df[\"Repo\"].values[10820]\n","repository_url = df[\"Repo Url\"].values[10820]\n","target_audience = \"smart developer\"\n","question = df[\"Question\"].values[10820]\n","context = df[\"Context\"].values[10820]\n","content_type = \"docs\"\n","prompt = f\"\"\"You are an AI assistant for a software project called {project_name}. You are trained on all the {content_type} that makes up this project.\n","    The {content_type} for the project is located at {repository_url}.\n","    You are given a repository which might contain several modules and each module will contain a set of files.\n","    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\n","    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\n","\n","    Assume the reader is a {target_audience} but is not deeply familiar with {project_name}.\n","    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\n","    If you don't know how to fill up the readme.md file in one of its sections, leave that part blank. Don't try to make up any content.\n","    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\n","    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\n","\n","    Question: {question}\n","    Context:\n","    {context}\n","\n","    Answer in Markdown:\"\"\"\n","prompt"]},{"cell_type":"markdown","metadata":{"id":"VHYgWlyvwy4E"},"source":["## Let's generate!\n","\n","Below we're setting up our generative model:\n","- Top P: a method for choosing from among a selection of most probable outputs, as opposed to greedily just taking the highest)\n","- Temperature: a modulation on the softmax function used to determine the values of our outputs\n","- We limit the return sequences to 1 - only one answer is allowed! - and deliberately force the answer to be short."]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.363470Z","iopub.status.busy":"2024-05-23T12:25:25.363233Z","iopub.status.idle":"2024-05-23T12:25:25.371384Z","shell.execute_reply":"2024-05-23T12:25:25.370516Z","shell.execute_reply.started":"2024-05-23T12:25:25.363443Z"},"id":"YiqCdCD2oyPH","trusted":true},"outputs":[],"source":["generation_config = model.generation_config\n","generation_config.max_new_tokens = 512\n","generation_config.temperature = 0.7\n","generation_config.top_p = 0.7\n","generation_config.num_return_sequences = 1\n","generation_config.repetition_penalty = 1.1\n","generation_config.pad_token_id = tokenizer.eos_token_id\n","generation_config.eos_token_id = tokenizer.eos_token_id"]},{"cell_type":"markdown","metadata":{},"source":["Now, we'll generate an answer to our first question, just to see how the model does!\n","\n","It's fascinatingly wrong. :-)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:06:55.308570Z","iopub.status.busy":"2024-05-23T12:06:55.308190Z","iopub.status.idle":"2024-05-23T12:07:08.421661Z","shell.execute_reply":"2024-05-23T12:07:08.420686Z","shell.execute_reply.started":"2024-05-23T12:06:55.308539Z"},"id":"o2ELFG0no1xR","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:533: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:538: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["You are an AI assistant for a software project called diagrams. You are trained on all the docs that makes up this project.\n","    The docs for the project is located at https://github.com/mingrammer/diagrams.\n","    You are given a repository which might contain several modules and each module will contain a set of files.\n","    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\n","    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\n","\n","    Assume the reader is a smart developer but is not deeply familiar with diagrams.\n","    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\n","    If you don't know how to fill up the readme.md file in one of its sections, leave that part blank. Don't try to make up any content.\n","    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\n","    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\n","\n","    Question: Provide the README content for the section with heading \"Getting Started\" starting with ## Getting Started.\n","    Context:\n","    headerLinks: [\n","    {doc: 'getting-started/installation', label: 'Docs'},\n","    {doc: 'guides/diagram', label: 'Guides'},\n","    {doc: 'nodes/aws', label: 'Nodes'},\n","    {href: 'https://github.com/mingrammer/diagrams', label: 'GitHub'},\n","    {href: 'https://www.buymeacoffee.com/mingrammer', label: 'Sponsoring'},\n","  ],\n","\n","  headerIcon: 'img/diagrams.ico',\n","  footerIcon: 'img/diagrams.ico',\n","  favicon: 'img/diagrams.ico',\n","\n","  colors: {\n","    primaryColor: '#5E73E5',\n","    secondaryColor: '#5E89E5',\n","  },\n","\n","  copyright: `Copyright © ${new Date().getFullYear()} mingrammer`,\n","\n","  highlight: {\n","    // Highlight.js theme to use for syntax highlighting in code blocks.\n","    theme: 'default',\n","  },\n","\n","  // Add custom scripts here that would be placed in <script> tags.\n","  scripts: ['https://buttons.github.io/buttons.js'],\n","\n","  // On page navigation for the current documentation page.\n","  onPageNav: 'separate',\n","  cleanUrl: true,\"CommandLineInterface\": \"CLI\",\n","            \"DeveloperTools\": \"DevTools\",\n","        },\n","        \"engagement\": {\n","            \"SimpleEmailServiceSes\": \"SES\",\n","        },\n","        \"general\": {\n","            \"GenericOfficeBuilding\": \"OfficeBuilding\",\n","        },\n","        \"integration\": {\n","            \"SimpleNotificationServiceSns\": \"SNS\",\n","            \"SimpleQueueServiceSqs\": \"SQS\",\n","            \"StepFunctions\": \"SF\",\n","        },\n","        \"iot\": {\n","            \"Freertos\": \"FreeRTOS\",\n","            \"IotHardwareBoard\": \"IotBoard\",\n","        },\n","        \"management\": {\n","            \"SystemsManager\": \"SSM\",\n","            \"SystemsManagerParameterStore\": \"ParameterStore\",\n","        },\n","        \"migration\": {\n","            \"ApplicationDiscoveryService\": \"ADS\",\n","            \"CloudendureMigration\": \"CEM\",\n","            \"DatabaseMigrationService\": \"DMS\",\n","            \"MigrationAndTransfer\": \"MAT\",\n","            \"ServerMigrationService\": \"SMS\",\n","        },\n","        \"ml\": {\n","            \"DeepLearningContainers\": \"DLC\",FILE_PREFIXES = {\n","    \"onprem\": (),\n","    \"aws\": (\"Amazon-\", \"AWS-\"),\n","    \"azure\": (\"Azure-\",),\n","    \"digitalocean\": (),\n","    \"gcp\": (\"Cloud-\",),\n","    \"firebase\": (\"Cloud-\",),\n","    \"ibm\": (),\n","    \"k8s\": (),\n","    \"alibabacloud\": (),\n","    \"oci\": (\"OCI-icon-\",),\n","    \"programming\": (),\n","    \"saas\": (),\n","    \"elastic\": (),\n","    \"outscale\": (),\n","    \"generic\": (),\n","    \"openstack\": (),\n","}\n","\n","#########################\n","#  Doc Auto Generation  #\n","#########################\n","\n","TMPL_APIDOC = \"apidoc.tmpl\"\n","\n","#########################\n","# Class Auto Generation #\n","#########################\n","\n","TMPL_MODULE = \"module.tmpl\"},\n","        \"gitops\": {\n","            \"Argocd\": \"ArgoCD\",\n","        },\n","        \"logging\": {\n","            \"Fluentbit\": \"FluentBit\",\n","            \"Rsyslog\": \"RSyslog\",\n","        },\n","        \"network\": {\n","            \"Etcd\": \"ETCD\",\n","            \"Haproxy\": \"HAProxy\",\n","            \"OpenServiceMesh\": \"OSM\",\n","            \"Opnsense\": \"OPNSense\",\n","            \"Pfsense\": \"PFSense\",\n","            \"Vyos\": \"VyOS\"\n","        },\n","        \"proxmox\": {\n","            \"Pve\": \"ProxmoxVE\",\n","        },\n","        \"queue\": {\n","            \"Activemq\": \"ActiveMQ\",\n","            \"Emqx\": \"EMQX\",\n","            \"Rabbitmq\": \"RabbitMQ\",\n","            \"Zeromq\": \"ZeroMQ\",\n","        },\n","        \"storage\": {\n","            \"Ceph\": \"CEPH\",\n","            \"CephOsd\": \"CEPH_OSD\",\n","        },\n","        \"workflow\": {\n","            \"Kubeflow\": \"KubeFlow\",\n","            \"Nifi\": \"NiFi\",\n","        }\n","    },\n","    \"aws\": {\n","        \"analytics\": {\n","            \"ElasticsearchService\": \"ES\",\n","        },\n","        \"business\": {\n","            \"AlexaForBusiness\": \"A4B\"\n","        },\n","\n","    Answer in Markdown:\n","\n","## Getting Started\n","\n","To get started with Diagrams, follow these steps:\n","\n","1. Install Diagrams using npm by running `npm install diagrams` in your terminal.\n","2. Create a new diagram by running `npx diagrams init MyDiagram` (replace `MyDiagram` with the name of your diagram).\n","3. Edit the diagram by modifying the `src/diagrams/MyDiagram.json` file.\n","4. Run the diagram by running `npx diagrams run MyDiagram`.\n","\n","That's it! You can now start creating diagrams using Diagrams.\n","\n","Note: This section is just an example, you may want to add more details or links depending on the context of your project.\n","CPU times: user 34.3 s, sys: 7.33 s, total: 41.7 s\n","Wall time: 41.7 s\n"]}],"source":["%%time\n","device = \"cuda\"\n","\n","encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","with torch.no_grad():\n","    outputs = model.generate(\n","        input_ids = encoding.input_ids,\n","        attention_mask = encoding.attention_mask,\n","        generation_config = generation_config\n","    )\n","\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{"id":"QAe7n7T4jP-D"},"source":["## Format our fine-tuning data\n","\n","We'll match the prompt setup we used above."]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:26:55.514848Z","iopub.status.idle":"2024-05-23T12:26:56.254319Z","shell.execute_reply":"2024-05-23T12:26:56.253352Z","shell.execute_reply.started":"2024-05-23T12:26:55.515569Z"},"id":"lm60o2_No7Jz","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map:   0%|          | 0/11372 [00:00<?, ? examples/s]"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 11372/11372 [00:22<00:00, 514.55 examples/s]\n"]}],"source":["def generate_prompt(data_point):\n","#     return f\"\"\"\n","#             {data_point[\"Question\"]}. \n","#             Answer as briefly as possible: {data_point[\"Answer\"]}\n","#             \"\"\".strip()\n","    return f\"\"\"You are an AI assistant for a software project called {data_point[\"Repo\"]}. You are trained on all the {content_type} that makes up this project.\n","    The docs for the project is located at {data_point[\"Repo Url\"]}.\n","    You are given a repository which might contain several modules and each module will contain a set of files.\n","    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\n","    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\n","\n","    Assume the reader is a smart developer but is not deeply familiar with {data_point[\"Repo\"]}.\n","    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\n","    If you don't know how to fill up the readme.md file in one of its sections, leave that part blank. Don't try to make up any content.\n","    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\n","    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\n","\n","    Question: {data_point[\"Question\"]}\n","    Context:\n","    {data_point[\"Context\"]}\n","\n","    Answer in Markdown:\n","    {data_point[\"Answer\"]}\n","    \"\"\"\n","\n","def generate_and_tokenize_prompt(data_point):\n","    full_prompt = generate_prompt(data_point)\n","    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n","    return tokenized_full_prompt\n","\n","data = Dataset.from_pandas(df)\n","data = data.shuffle().map(generate_and_tokenize_prompt)"]},{"cell_type":"markdown","metadata":{"id":"QCrTXUqXk0S9"},"source":["## Train!\n","\n","Now, we'll use our data to update our model. Using the Huggingface `transformers` library, let's set up our training loop and then run it. Note that we are ONLY making one pass on all this data."]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["import os\n","\n","os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n","os.environ['TORCH_USE_CUDA_DSA']=\"1\""]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:32:56.159367Z","iopub.status.busy":"2024-05-23T12:32:56.158408Z","iopub.status.idle":"2024-05-23T12:33:03.663596Z","shell.execute_reply":"2024-05-23T12:33:03.662176Z","shell.execute_reply.started":"2024-05-23T12:32:56.159329Z"},"id":"PGneIe1xpUJV","outputId":"8cdac9ac-d6bf-4d8f-b954-febf7f140591","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n","/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='275' max='11372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  275/11372 07:40 < 5:12:16, 0.59 it/s, Epoch 0.02/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 21\u001b[0m\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     18\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/trainer.py:1912\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1913\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1914\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1915\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1916\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1917\u001b[0m     )\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2245\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2248\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2251\u001b[0m ):\n\u001b[1;32m   2252\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2253\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/trainer.py:3284\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3282\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/accelerate/accelerator.py:2121\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["training_args = transformers.TrainingArguments(\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=1,\n","    num_train_epochs=1,\n","    learning_rate=1e-4,\n","    fp16=True,\n","    output_dir=\"outputs_llama2-7b-chat-gptq_v3\",\n","    optim=\"paged_adamw_8bit\",\n","    lr_scheduler_type=\"cosine\",\n","    warmup_ratio=0.01,\n","    report_to=\"none\"\n",")\n","\n","trainer = transformers.Trainer(\n","    model=model,\n","    train_dataset=data,\n","    args=training_args,\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",")\n","model.config.use_cache = False\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"hQRUzbH9oaqG"},"source":["\n"]},{"cell_type":"markdown","metadata":{},"source":["## Loading and using the model later\n","\n","Now, we'll save the PEFT fine-tuned model, then load it and use it to generate some more answers."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T06:59:35.522808Z","iopub.status.busy":"2024-05-08T06:59:35.522441Z","iopub.status.idle":"2024-05-08T06:59:50.303758Z","shell.execute_reply":"2024-05-08T06:59:50.302828Z","shell.execute_reply.started":"2024-05-08T06:59:35.522781Z"},"id":"Vmce-aSesAHV","outputId":"4bf93e78-2a0b-404c-8b05-3748db1bdc52","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["CUDA extension not installed.\n","CUDA extension not installed.\n"]},{"name":"stderr","output_type":"stream","text":["/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/modeling_utils.py:4484: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n","  warnings.warn(\n","You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]}],"source":["# model.save_pretrained(\"trained-model\")\n","\n","PEFT_MODEL = \"outputs_llama2-7b-chat-gptq_v2/checkpoint-11000\"\n","\n","config = PeftConfig.from_pretrained(PEFT_MODEL)\n","model = AutoModelForCausalLM.from_pretrained(\n","    config.base_model_name_or_path,\n","    return_dict=True,\n","    # quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    trust_remote_code=True\n",")\n","\n","tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","model = PeftModel.from_pretrained(model, PEFT_MODEL)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T07:00:06.408820Z","iopub.status.busy":"2024-05-08T07:00:06.407590Z","iopub.status.idle":"2024-05-08T07:00:06.414259Z","shell.execute_reply":"2024-05-08T07:00:06.413181Z","shell.execute_reply.started":"2024-05-08T07:00:06.408782Z"},"id":"vgIHyPUasD0b","trusted":true},"outputs":[],"source":["generation_config = model.generation_config\n","generation_config.max_new_tokens = 512\n","generation_config.temperature = 0.7\n","generation_config.top_p = 0.7\n","generation_config.num_return_sequences = 1\n","generation_config.repetition_penalty = 1.1\n","generation_config.pad_token_id = tokenizer.eos_token_id\n","generation_config.eos_token_id = tokenizer.eos_token_id"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T07:00:07.966509Z","iopub.status.busy":"2024-05-08T07:00:07.966076Z","iopub.status.idle":"2024-05-08T07:00:07.971440Z","shell.execute_reply":"2024-05-08T07:00:07.970162Z","shell.execute_reply.started":"2024-05-08T07:00:07.966474Z"},"id":"I--juWjcCGpS","trusted":true},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T07:00:10.373393Z","iopub.status.busy":"2024-05-08T07:00:10.373001Z","iopub.status.idle":"2024-05-08T07:00:12.135173Z","shell.execute_reply":"2024-05-08T07:00:12.134130Z","shell.execute_reply.started":"2024-05-08T07:00:10.373362Z"},"id":"63Zxai-isGhJ","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:533: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:538: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["You are an AI assistant for a software project called diagrams. You are trained on all the docs that makes up this project.\n","    The docs for the project is located at https://github.com/mingrammer/diagrams.\n","    You are given a repository which might contain several modules and each module will contain a set of files.\n","    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\n","    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\n","\n","    Assume the reader is a smart developer but is not deeply familiar with diagrams.\n","    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\n","    If you don't know how to fill up the readme.md file in one of its sections, leave that part blank. Don't try to make up any content.\n","    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\n","    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\n","\n","    Question: Provide the README content for the section with heading \"Getting Started\" starting with ## Getting Started.\n","    Context:\n","    headerLinks: [\n","    {doc: 'getting-started/installation', label: 'Docs'},\n","    {doc: 'guides/diagram', label: 'Guides'},\n","    {doc: 'nodes/aws', label: 'Nodes'},\n","    {href: 'https://github.com/mingrammer/diagrams', label: 'GitHub'},\n","    {href: 'https://www.buymeacoffee.com/mingrammer', label: 'Sponsoring'},\n","  ],\n","\n","  headerIcon: 'img/diagrams.ico',\n","  footerIcon: 'img/diagrams.ico',\n","  favicon: 'img/diagrams.ico',\n","\n","  colors: {\n","    primaryColor: '#5E73E5',\n","    secondaryColor: '#5E89E5',\n","  },\n","\n","  copyright: `Copyright © ${new Date().getFullYear()} mingrammer`,\n","\n","  highlight: {\n","    // Highlight.js theme to use for syntax highlighting in code blocks.\n","    theme: 'default',\n","  },\n","\n","  // Add custom scripts here that would be placed in <script> tags.\n","  scripts: ['https://buttons.github.io/buttons.js'],\n","\n","  // On page navigation for the current documentation page.\n","  onPageNav: 'separate',\n","  cleanUrl: true,\"CommandLineInterface\": \"CLI\",\n","            \"DeveloperTools\": \"DevTools\",\n","        },\n","        \"engagement\": {\n","            \"SimpleEmailServiceSes\": \"SES\",\n","        },\n","        \"general\": {\n","            \"GenericOfficeBuilding\": \"OfficeBuilding\",\n","        },\n","        \"integration\": {\n","            \"SimpleNotificationServiceSns\": \"SNS\",\n","            \"SimpleQueueServiceSqs\": \"SQS\",\n","            \"StepFunctions\": \"SF\",\n","        },\n","        \"iot\": {\n","            \"Freertos\": \"FreeRTOS\",\n","            \"IotHardwareBoard\": \"IotBoard\",\n","        },\n","        \"management\": {\n","            \"SystemsManager\": \"SSM\",\n","            \"SystemsManagerParameterStore\": \"ParameterStore\",\n","        },\n","        \"migration\": {\n","            \"ApplicationDiscoveryService\": \"ADS\",\n","            \"CloudendureMigration\": \"CEM\",\n","            \"DatabaseMigrationService\": \"DMS\",\n","            \"MigrationAndTransfer\": \"MAT\",\n","            \"ServerMigrationService\": \"SMS\",\n","        },\n","        \"ml\": {\n","            \"DeepLearningContainers\": \"DLC\",FILE_PREFIXES = {\n","    \"onprem\": (),\n","    \"aws\": (\"Amazon-\", \"AWS-\"),\n","    \"azure\": (\"Azure-\",),\n","    \"digitalocean\": (),\n","    \"gcp\": (\"Cloud-\",),\n","    \"firebase\": (\"Cloud-\",),\n","    \"ibm\": (),\n","    \"k8s\": (),\n","    \"alibabacloud\": (),\n","    \"oci\": (\"OCI-icon-\",),\n","    \"programming\": (),\n","    \"saas\": (),\n","    \"elastic\": (),\n","    \"outscale\": (),\n","    \"generic\": (),\n","    \"openstack\": (),\n","}\n","\n","#########################\n","#  Doc Auto Generation  #\n","#########################\n","\n","TMPL_APIDOC = \"apidoc.tmpl\"\n","\n","#########################\n","# Class Auto Generation #\n","#########################\n","\n","TMPL_MODULE = \"module.tmpl\"},\n","        \"gitops\": {\n","            \"Argocd\": \"ArgoCD\",\n","        },\n","        \"logging\": {\n","            \"Fluentbit\": \"FluentBit\",\n","            \"Rsyslog\": \"RSyslog\",\n","        },\n","        \"network\": {\n","            \"Etcd\": \"ETCD\",\n","            \"Haproxy\": \"HAProxy\",\n","            \"OpenServiceMesh\": \"OSM\",\n","            \"Opnsense\": \"OPNSense\",\n","            \"Pfsense\": \"PFSense\",\n","            \"Vyos\": \"VyOS\"\n","        },\n","        \"proxmox\": {\n","            \"Pve\": \"ProxmoxVE\",\n","        },\n","        \"queue\": {\n","            \"Activemq\": \"ActiveMQ\",\n","            \"Emqx\": \"EMQX\",\n","            \"Rabbitmq\": \"RabbitMQ\",\n","            \"Zeromq\": \"ZeroMQ\",\n","        },\n","        \"storage\": {\n","            \"Ceph\": \"CEPH\",\n","            \"CephOsd\": \"CEPH_OSD\",\n","        },\n","        \"workflow\": {\n","            \"Kubeflow\": \"KubeFlow\",\n","            \"Nifi\": \"NiFi\",\n","        }\n","    },\n","    \"aws\": {\n","        \"analytics\": {\n","            \"ElasticsearchService\": \"ES\",\n","        },\n","        \"business\": {\n","            \"AlexaForBusiness\": \"A4B\"\n","        },\n","\n","    Answer in Markdown:\n","    Diagrams is a free, open-source diagramming library built on top of ReactJS. It provides a simple and intuitive API for creating diagrams from various types of nodes. It also includes a powerful editor for creating diagrams from scratch. [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![](\n","CPU times: user 1min 43s, sys: 25.2 s, total: 2min 8s\n","Wall time: 2min 9s\n"]}],"source":["%%time\n","project_name = df[\"Repo\"].values[10820]\n","repository_url = df[\"Repo Url\"].values[10820]\n","target_audience = \"smart developer\"\n","question = df[\"Question\"].values[10820]\n","context = df[\"Context\"].values[10820]\n","content_type = \"docs\"\n","prompt = f\"\"\"You are an AI assistant for a software project called {project_name}. You are trained on all the {content_type} that makes up this project.\n","    The {content_type} for the project is located at {repository_url}.\n","    You are given a repository which might contain several modules and each module will contain a set of files.\n","    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\n","    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\n","\n","    Assume the reader is a {target_audience} but is not deeply familiar with {project_name}.\n","    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\n","    If you don't know how to fill up the readme.md file in one of its sections, leave that part blank. Don't try to make up any content.\n","    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\n","    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\n","\n","    Question: {question}\n","    Context:\n","    {context}\n","\n","    Answer in Markdown:\"\"\"\n","    \n","device = \"cuda\"\n","encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","with torch.inference_mode():\n","  outputs = model.generate(\n","      input_ids = encoding.input_ids,\n","      attention_mask = encoding.attention_mask,\n","      generation_config = generation_config\n","  )\n","\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":412543,"sourceId":789660,"sourceType":"datasetVersion"},{"datasetId":5010308,"sourceId":8417065,"sourceType":"datasetVersion"},{"modelInstanceId":3093,"sourceId":4298,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":28785,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
