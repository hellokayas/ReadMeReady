{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Fine-Tuning with Llama 2, Bits and Bytes, and QLoRA\n","\n","Today we'll explore fine-tuning the Llama 2 model available on Kaggle Models using QLoRA, Bits and Bytes, and PEFT.\n","\n","- QLoRA: [Quantized Low Rank Adapters](https://arxiv.org/pdf/2305.14314.pdf) - this is a method for fine-tuning LLMs that uses a small number of quantized, updateable parameters to limit the complexity of training. This technique also allows those small sets of parameters to be added efficiently into the model itself, which means you can do fine-tuning on lots of data sets, potentially, and swap these \"adapters\" into your model when necessary.\n","- [Bits and Bytes](https://github.com/TimDettmers/bitsandbytes): An excellent package by Tim Dettmers et al., which provides a lightweight wrapper around custom CUDA functions that make LLMs go faster - optimizers, matrix mults, and quantization. In this notebook we'll be using the library to load our model as efficiently as possible.\n","- [PEFT](https://github.com/huggingface/peft): An excellent Huggingface library that enables a number Parameter Efficient Fine-tuning (PEFT) methods, which again make it less expensive to fine-tune LLMs - especially on more lightweight hardware like that present in Kaggle notebooks.\n","\n","Many thanks to [Bojan Tunguz](https://www.kaggle.com/tunguz) for his excellent [Jeopardy dataset](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions)!\n","\n","This notebook is based on [an excellent example from LangChain](https://github.com/asokraju/LangChainDatasetForge/blob/main/Finetuning_Falcon_7b.ipynb)."]},{"cell_type":"markdown","metadata":{"id":"wIa8WRIHvuZy"},"source":["## Package Installation\n","\n","Note that we're loading very specific versions of these libraries. Dependencies in this space can be quite difficult to untangle, and simply taking the latest version of each library can lead to conflicting version requirements. It's a good idea to take note of which versions work for your particular use case, and `pip install` them directly."]},{"cell_type":"code","execution_count":2,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2024-05-23T12:21:58.849851Z","iopub.status.busy":"2024-05-23T12:21:58.849106Z","iopub.status.idle":"2024-05-23T12:24:42.714261Z","shell.execute_reply":"2024-05-23T12:24:42.713141Z","shell.execute_reply.started":"2024-05-23T12:21:58.849820Z"},"id":"g3agEMJEnKsl","outputId":"56ab843a-e9c8-4158-8c04-18d77aec44fd","trusted":true},"outputs":[],"source":["!pip install -qqq bitsandbytes\n","!pip install -qqq torch\n","!pip install -qqq -U git+https://github.com/huggingface/transformers.git\n","!pip install -qqq -U git+https://github.com/huggingface/peft.git\n","!pip install -qqq -U git+https://github.com/huggingface/accelerate.git\n","!pip install -qqq datasets\n","!pip install -qqq loralib\n","!pip install -qqq einops"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-21T11:09:09.148558Z","iopub.status.busy":"2024-05-21T11:09:09.147392Z","iopub.status.idle":"2024-05-21T11:09:30.589305Z","shell.execute_reply":"2024-05-21T11:09:30.588042Z","shell.execute_reply.started":"2024-05-21T11:09:09.148520Z"},"trusted":true},"outputs":[],"source":["# !pip install -q auto-gptq==0.5.0"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:24:42.716682Z","iopub.status.busy":"2024-05-23T12:24:42.716361Z","iopub.status.idle":"2024-05-23T12:24:48.696015Z","shell.execute_reply":"2024-05-23T12:24:48.695079Z","shell.execute_reply.started":"2024-05-23T12:24:42.716652Z"},"id":"dv3aJo8Anhyw","outputId":"66f7b274-28a9-45b4-c0e6-d25194424594","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","import json\n","import os\n","from pprint import pprint\n","import bitsandbytes as bnb\n","import torch\n","import torch.nn as nn\n","import transformers\n","from datasets import load_dataset, Dataset\n","from huggingface_hub import notebook_login\n","\n","from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n","from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""]},{"cell_type":"markdown","metadata":{"id":"AgqJriqjwMyK"},"source":["# Loading and preparing our model\n","\n","We're going to use the Llama 2 7B model for our test. We'll be using Bits and Bytes to load it in 4-bit format, which should reduce memory consumption considerably, at a cost of some accuracy.\n","\n","Note the parameters in `BitsAndBytesConfig` - this is a fairly standard 4-bit quantization configuration, loading the weights in 4-bit format, using a straightforward format (`normal float 4`) with double quantization to improve QLoRA's resolution. The weights are converted back to `bfloat16` for weight updates, then the extra precision is discarded."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"mllA2Ka_ol13","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["CUDA extension not installed.\n","CUDA extension not installed.\n","/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/modeling_utils.py:4484: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n","  warnings.warn(\n","You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]}],"source":["model = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n","MODEL_NAME = model\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n","    quantization_config=bnb_config\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:24:48.697613Z","iopub.status.busy":"2024-05-23T12:24:48.697147Z","iopub.status.idle":"2024-05-23T12:25:25.163356Z","shell.execute_reply":"2024-05-23T12:25:25.162575Z","shell.execute_reply.started":"2024-05-23T12:24:48.697586Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n","Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\n"]}],"source":["model = \"google/gemma-2b-it\"\n","MODEL_NAME = model\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n","    quantization_config=bnb_config,\n","    token='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'\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token='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')\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["CUDA extension not installed.\n","CUDA extension not installed.\n"]},{"name":"stderr","output_type":"stream","text":["/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/modeling_utils.py:4484: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n","  warnings.warn(\n","You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]}],"source":["MODEL_NAME = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_NAME,\n","        torch_dtype=torch.float16,\n","        trust_remote_code=True,\n","        device_map=\"auto\",\n","    )\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"markdown","metadata":{},"source":["Below, we'll use a nice PEFT wrapper to set up our model for training / fine-tuning. Specifically this function sets the output embedding layer to allow gradient updates, as well as performing some type casting on various components to ensure the model is ready to be updated."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.165686Z","iopub.status.busy":"2024-05-23T12:25:25.165377Z","iopub.status.idle":"2024-05-23T12:25:25.184291Z","shell.execute_reply":"2024-05-23T12:25:25.183565Z","shell.execute_reply.started":"2024-05-23T12:25:25.165661Z"},"id":"na8DUq4IoqpB","trusted":true},"outputs":[],"source":["model = prepare_model_for_kbit_training(model)"]},{"cell_type":"markdown","metadata":{},"source":["Below, we define some helper functions - their purpose is to properly identify our update layers so we can... update them!"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.185688Z","iopub.status.busy":"2024-05-23T12:25:25.185329Z","iopub.status.idle":"2024-05-23T12:25:25.193947Z","shell.execute_reply":"2024-05-23T12:25:25.193083Z","shell.execute_reply.started":"2024-05-23T12:25:25.185654Z"},"trusted":true},"outputs":[{"data":{"text/plain":["GemmaForCausalLM(\n","  (model): GemmaModel(\n","    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n","    (layers): ModuleList(\n","      (0-17): 18 x GemmaDecoderLayer(\n","        (self_attn): GemmaSdpaAttention(\n","          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n","          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n","          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","          (rotary_emb): GemmaRotaryEmbedding()\n","        )\n","        (mlp): GemmaMLP(\n","          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n","          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n","          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n","          (act_fn): PytorchGELUTanh()\n","        )\n","        (input_layernorm): GemmaRMSNorm()\n","        (post_attention_layernorm): GemmaRMSNorm()\n","      )\n","    )\n","    (norm): GemmaRMSNorm()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",")"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.195344Z","iopub.status.busy":"2024-05-23T12:25:25.195023Z","iopub.status.idle":"2024-05-23T12:25:25.203441Z","shell.execute_reply":"2024-05-23T12:25:25.202469Z","shell.execute_reply.started":"2024-05-23T12:25:25.195319Z"},"trusted":true},"outputs":[],"source":["import re\n","def get_num_layers(model):\n","    numbers = set()\n","    for name, _ in model.named_parameters():\n","        for number in re.findall(r'\\d+', name):\n","            numbers.add(int(number))\n","    print(max(numbers))\n","    return max(numbers)\n","\n","def get_last_layer_linears(model):\n","    names = []\n","    \n","    num_layers = get_num_layers(model)\n","    for name, module in model.named_modules():\n","        if str(num_layers) in name and not \"encoder\" in name:\n","            if isinstance(module, torch.nn.Linear):\n","                names.append(name)\n","    print(names)\n","    return names"]},{"cell_type":"markdown","metadata":{},"source":["## LORA config\n","\n","Some key elements from this configuration:\n","1. `r` is the width of the small update layer. In theory, this should be set wide enough to capture the complexity of the problem you're attempting to fine-tune for. More simple problems may be able to get away with smaller `r`. In our case, we'll go very small, largely for the sake of speed.\n","2. `target_modules` is set using our helper functions - every layer identified by that function will be included in the PEFT update."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.205599Z","iopub.status.busy":"2024-05-23T12:25:25.205231Z","iopub.status.idle":"2024-05-23T12:25:25.236478Z","shell.execute_reply":"2024-05-23T12:25:25.235650Z","shell.execute_reply.started":"2024-05-23T12:25:25.205568Z"},"id":"C4Qk3fGLoraw","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["17\n","['model.layers.17.self_attn.q_proj', 'model.layers.17.self_attn.k_proj', 'model.layers.17.self_attn.v_proj', 'model.layers.17.self_attn.o_proj', 'model.layers.17.mlp.gate_proj', 'model.layers.17.mlp.up_proj', 'model.layers.17.mlp.down_proj']\n"]}],"source":["config = LoraConfig(\n","    r=2,\n","    lora_alpha=32,\n","    target_modules=get_last_layer_linears(model),\n","    # target_modules = [\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","model = get_peft_model(model, config)"]},{"cell_type":"markdown","metadata":{},"source":["## Load some data\n","\n","Here, we're loading a 200,000 question Jeopardy dataset. In the interests of time we won't load all of them - just the first 1000 - but we'll fine-tune our model using the question and answers. Note that what we're training the model to do is use its existing knowledge (plus whatever little it learns from our question-answer pairs) to answer questions in the *format* we want, specifically short answers."]},{"cell_type":"code","execution_count":125,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.237679Z","iopub.status.busy":"2024-05-23T12:25:25.237419Z","iopub.status.idle":"2024-05-23T12:25:25.298238Z","shell.execute_reply":"2024-05-23T12:25:25.297528Z","shell.execute_reply.started":"2024-05-23T12:25:25.237656Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(\"readme_qa.csv\")\n","df.columns = [str(q).strip() for q in df.columns]\n","\n","data = Dataset.from_pandas(df)"]},{"cell_type":"code","execution_count":95,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T06:36:39.569842Z","iopub.status.busy":"2024-05-08T06:36:39.569492Z","iopub.status.idle":"2024-05-08T06:36:39.577848Z","shell.execute_reply":"2024-05-08T06:36:39.576834Z","shell.execute_reply.started":"2024-05-08T06:36:39.569808Z"},"id":"_Pb9RA5NovNS","trusted":true},"outputs":[],"source":["# prompt = df[\"Question\"].values[0] + \". Answer as briefly as possible: \".strip()\n","# prompt"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:26:49.814219Z","iopub.status.busy":"2024-05-23T12:26:49.813856Z","iopub.status.idle":"2024-05-23T12:26:49.824269Z","shell.execute_reply":"2024-05-23T12:26:49.823392Z","shell.execute_reply.started":"2024-05-23T12:26:49.814190Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'You are an AI assistant for a software project called diagrams. You are trained on all the docs that makes up this project.\\n    The docs for the project is located at https://github.com/mingrammer/diagrams.\\n    You are given a repository which might contain several modules and each module will contain a set of files.\\n    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\\n    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\\n\\n    Assume the reader is a smart developer but is not deeply familiar with diagrams.\\n    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\\n    If you don\\'t know how to fill up the readme.md file in one of its sections, leave that part blank. Don\\'t try to make up any content.\\n    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\\n    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\\n\\n    Question: Provide the README content for the section with heading \"Getting Started\" starting with ## Getting Started.\\n    Context:\\n    headerLinks: [\\n    {doc: \\'getting-started/installation\\', label: \\'Docs\\'},\\n    {doc: \\'guides/diagram\\', label: \\'Guides\\'},\\n    {doc: \\'nodes/aws\\', label: \\'Nodes\\'},\\n    {href: \\'https://github.com/mingrammer/diagrams\\', label: \\'GitHub\\'},\\n    {href: \\'https://www.buymeacoffee.com/mingrammer\\', label: \\'Sponsoring\\'},\\n  ],\\n\\n  headerIcon: \\'img/diagrams.ico\\',\\n  footerIcon: \\'img/diagrams.ico\\',\\n  favicon: \\'img/diagrams.ico\\',\\n\\n  colors: {\\n    primaryColor: \\'#5E73E5\\',\\n    secondaryColor: \\'#5E89E5\\',\\n  },\\n\\n  copyright: `Copyright © ${new Date().getFullYear()} mingrammer`,\\n\\n  highlight: {\\n    // Highlight.js theme to use for syntax highlighting in code blocks.\\n    theme: \\'default\\',\\n  },\\n\\n  // Add custom scripts here that would be placed in <script> tags.\\n  scripts: [\\'https://buttons.github.io/buttons.js\\'],\\n\\n  // On page navigation for the current documentation page.\\n  onPageNav: \\'separate\\',\\n  cleanUrl: true,\"CommandLineInterface\": \"CLI\",\\n            \"DeveloperTools\": \"DevTools\",\\n        },\\n        \"engagement\": {\\n            \"SimpleEmailServiceSes\": \"SES\",\\n        },\\n        \"general\": {\\n            \"GenericOfficeBuilding\": \"OfficeBuilding\",\\n        },\\n        \"integration\": {\\n            \"SimpleNotificationServiceSns\": \"SNS\",\\n            \"SimpleQueueServiceSqs\": \"SQS\",\\n            \"StepFunctions\": \"SF\",\\n        },\\n        \"iot\": {\\n            \"Freertos\": \"FreeRTOS\",\\n            \"IotHardwareBoard\": \"IotBoard\",\\n        },\\n        \"management\": {\\n            \"SystemsManager\": \"SSM\",\\n            \"SystemsManagerParameterStore\": \"ParameterStore\",\\n        },\\n        \"migration\": {\\n            \"ApplicationDiscoveryService\": \"ADS\",\\n            \"CloudendureMigration\": \"CEM\",\\n            \"DatabaseMigrationService\": \"DMS\",\\n            \"MigrationAndTransfer\": \"MAT\",\\n            \"ServerMigrationService\": \"SMS\",\\n        },\\n        \"ml\": {\\n            \"DeepLearningContainers\": \"DLC\",FILE_PREFIXES = {\\n    \"onprem\": (),\\n    \"aws\": (\"Amazon-\", \"AWS-\"),\\n    \"azure\": (\"Azure-\",),\\n    \"digitalocean\": (),\\n    \"gcp\": (\"Cloud-\",),\\n    \"firebase\": (\"Cloud-\",),\\n    \"ibm\": (),\\n    \"k8s\": (),\\n    \"alibabacloud\": (),\\n    \"oci\": (\"OCI-icon-\",),\\n    \"programming\": (),\\n    \"saas\": (),\\n    \"elastic\": (),\\n    \"outscale\": (),\\n    \"generic\": (),\\n    \"openstack\": (),\\n}\\n\\n#########################\\n#  Doc Auto Generation  #\\n#########################\\n\\nTMPL_APIDOC = \"apidoc.tmpl\"\\n\\n#########################\\n# Class Auto Generation #\\n#########################\\n\\nTMPL_MODULE = \"module.tmpl\"},\\n        \"gitops\": {\\n            \"Argocd\": \"ArgoCD\",\\n        },\\n        \"logging\": {\\n            \"Fluentbit\": \"FluentBit\",\\n            \"Rsyslog\": \"RSyslog\",\\n        },\\n        \"network\": {\\n            \"Etcd\": \"ETCD\",\\n            \"Haproxy\": \"HAProxy\",\\n            \"OpenServiceMesh\": \"OSM\",\\n            \"Opnsense\": \"OPNSense\",\\n            \"Pfsense\": \"PFSense\",\\n            \"Vyos\": \"VyOS\"\\n        },\\n        \"proxmox\": {\\n            \"Pve\": \"ProxmoxVE\",\\n        },\\n        \"queue\": {\\n            \"Activemq\": \"ActiveMQ\",\\n            \"Emqx\": \"EMQX\",\\n            \"Rabbitmq\": \"RabbitMQ\",\\n            \"Zeromq\": \"ZeroMQ\",\\n        },\\n        \"storage\": {\\n            \"Ceph\": \"CEPH\",\\n            \"CephOsd\": \"CEPH_OSD\",\\n        },\\n        \"workflow\": {\\n            \"Kubeflow\": \"KubeFlow\",\\n            \"Nifi\": \"NiFi\",\\n        }\\n    },\\n    \"aws\": {\\n        \"analytics\": {\\n            \"ElasticsearchService\": \"ES\",\\n        },\\n        \"business\": {\\n            \"AlexaForBusiness\": \"A4B\"\\n        },\\n\\n    Answer in Markdown:'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["project_name = df[\"Repo\"].values[10820]\n","repository_url = df[\"Repo Url\"].values[10820]\n","target_audience = \"smart developer\"\n","question = df[\"Question\"].values[10820]\n","context = df[\"Context\"].values[10820]\n","content_type = \"docs\"\n","prompt = f\"\"\"You are an AI assistant for a software project called {project_name}. You are trained on all the {content_type} that makes up this project.\n","    The {content_type} for the project is located at {repository_url}.\n","    You are given a repository which might contain several modules and each module will contain a set of files.\n","    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\n","    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\n","\n","    Assume the reader is a {target_audience} but is not deeply familiar with {project_name}.\n","    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\n","    If you don't know how to fill up the readme.md file in one of its sections, leave that part blank. Don't try to make up any content.\n","    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\n","    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\n","\n","    Question: {question}\n","    Context:\n","    {context}\n","\n","    Answer in Markdown:\"\"\"\n","prompt"]},{"cell_type":"markdown","metadata":{"id":"VHYgWlyvwy4E"},"source":["## Let's generate!\n","\n","Below we're setting up our generative model:\n","- Top P: a method for choosing from among a selection of most probable outputs, as opposed to greedily just taking the highest)\n","- Temperature: a modulation on the softmax function used to determine the values of our outputs\n","- We limit the return sequences to 1 - only one answer is allowed! - and deliberately force the answer to be short."]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:25:25.363470Z","iopub.status.busy":"2024-05-23T12:25:25.363233Z","iopub.status.idle":"2024-05-23T12:25:25.371384Z","shell.execute_reply":"2024-05-23T12:25:25.370516Z","shell.execute_reply.started":"2024-05-23T12:25:25.363443Z"},"id":"YiqCdCD2oyPH","trusted":true},"outputs":[],"source":["generation_config = model.generation_config\n","generation_config.max_new_tokens = 512\n","generation_config.temperature = 0.7\n","generation_config.top_p = 0.7\n","generation_config.num_return_sequences = 1\n","generation_config.repetition_penalty = 1.1\n","generation_config.pad_token_id = tokenizer.eos_token_id\n","generation_config.eos_token_id = tokenizer.eos_token_id"]},{"cell_type":"markdown","metadata":{},"source":["Now, we'll generate an answer to our first question, just to see how the model does!\n","\n","It's fascinatingly wrong. :-)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:06:55.308570Z","iopub.status.busy":"2024-05-23T12:06:55.308190Z","iopub.status.idle":"2024-05-23T12:07:08.421661Z","shell.execute_reply":"2024-05-23T12:07:08.420686Z","shell.execute_reply.started":"2024-05-23T12:06:55.308539Z"},"id":"o2ELFG0no1xR","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:533: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:538: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["You are an AI assistant for a software project called diagrams. You are trained on all the docs that makes up this project.\n","    The docs for the project is located at https://github.com/mingrammer/diagrams.\n","    You are given a repository which might contain several modules and each module will contain a set of files.\n","    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\n","    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\n","\n","    Assume the reader is a smart developer but is not deeply familiar with diagrams.\n","    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\n","    If you don't know how to fill up the readme.md file in one of its sections, leave that part blank. Don't try to make up any content.\n","    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\n","    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\n","\n","    Question: Provide the README content for the section with heading \"Getting Started\" starting with ## Getting Started.\n","    Context:\n","    headerLinks: [\n","    {doc: 'getting-started/installation', label: 'Docs'},\n","    {doc: 'guides/diagram', label: 'Guides'},\n","    {doc: 'nodes/aws', label: 'Nodes'},\n","    {href: 'https://github.com/mingrammer/diagrams', label: 'GitHub'},\n","    {href: 'https://www.buymeacoffee.com/mingrammer', label: 'Sponsoring'},\n","  ],\n","\n","  headerIcon: 'img/diagrams.ico',\n","  footerIcon: 'img/diagrams.ico',\n","  favicon: 'img/diagrams.ico',\n","\n","  colors: {\n","    primaryColor: '#5E73E5',\n","    secondaryColor: '#5E89E5',\n","  },\n","\n","  copyright: `Copyright © ${new Date().getFullYear()} mingrammer`,\n","\n","  highlight: {\n","    // Highlight.js theme to use for syntax highlighting in code blocks.\n","    theme: 'default',\n","  },\n","\n","  // Add custom scripts here that would be placed in <script> tags.\n","  scripts: ['https://buttons.github.io/buttons.js'],\n","\n","  // On page navigation for the current documentation page.\n","  onPageNav: 'separate',\n","  cleanUrl: true,\"CommandLineInterface\": \"CLI\",\n","            \"DeveloperTools\": \"DevTools\",\n","        },\n","        \"engagement\": {\n","            \"SimpleEmailServiceSes\": \"SES\",\n","        },\n","        \"general\": {\n","            \"GenericOfficeBuilding\": \"OfficeBuilding\",\n","        },\n","        \"integration\": {\n","            \"SimpleNotificationServiceSns\": \"SNS\",\n","            \"SimpleQueueServiceSqs\": \"SQS\",\n","            \"StepFunctions\": \"SF\",\n","        },\n","        \"iot\": {\n","            \"Freertos\": \"FreeRTOS\",\n","            \"IotHardwareBoard\": \"IotBoard\",\n","        },\n","        \"management\": {\n","            \"SystemsManager\": \"SSM\",\n","            \"SystemsManagerParameterStore\": \"ParameterStore\",\n","        },\n","        \"migration\": {\n","            \"ApplicationDiscoveryService\": \"ADS\",\n","            \"CloudendureMigration\": \"CEM\",\n","            \"DatabaseMigrationService\": \"DMS\",\n","            \"MigrationAndTransfer\": \"MAT\",\n","            \"ServerMigrationService\": \"SMS\",\n","        },\n","        \"ml\": {\n","            \"DeepLearningContainers\": \"DLC\",FILE_PREFIXES = {\n","    \"onprem\": (),\n","    \"aws\": (\"Amazon-\", \"AWS-\"),\n","    \"azure\": (\"Azure-\",),\n","    \"digitalocean\": (),\n","    \"gcp\": (\"Cloud-\",),\n","    \"firebase\": (\"Cloud-\",),\n","    \"ibm\": (),\n","    \"k8s\": (),\n","    \"alibabacloud\": (),\n","    \"oci\": (\"OCI-icon-\",),\n","    \"programming\": (),\n","    \"saas\": (),\n","    \"elastic\": (),\n","    \"outscale\": (),\n","    \"generic\": (),\n","    \"openstack\": (),\n","}\n","\n","#########################\n","#  Doc Auto Generation  #\n","#########################\n","\n","TMPL_APIDOC = \"apidoc.tmpl\"\n","\n","#########################\n","# Class Auto Generation #\n","#########################\n","\n","TMPL_MODULE = \"module.tmpl\"},\n","        \"gitops\": {\n","            \"Argocd\": \"ArgoCD\",\n","        },\n","        \"logging\": {\n","            \"Fluentbit\": \"FluentBit\",\n","            \"Rsyslog\": \"RSyslog\",\n","        },\n","        \"network\": {\n","            \"Etcd\": \"ETCD\",\n","            \"Haproxy\": \"HAProxy\",\n","            \"OpenServiceMesh\": \"OSM\",\n","            \"Opnsense\": \"OPNSense\",\n","            \"Pfsense\": \"PFSense\",\n","            \"Vyos\": \"VyOS\"\n","        },\n","        \"proxmox\": {\n","            \"Pve\": \"ProxmoxVE\",\n","        },\n","        \"queue\": {\n","            \"Activemq\": \"ActiveMQ\",\n","            \"Emqx\": \"EMQX\",\n","            \"Rabbitmq\": \"RabbitMQ\",\n","            \"Zeromq\": \"ZeroMQ\",\n","        },\n","        \"storage\": {\n","            \"Ceph\": \"CEPH\",\n","            \"CephOsd\": \"CEPH_OSD\",\n","        },\n","        \"workflow\": {\n","            \"Kubeflow\": \"KubeFlow\",\n","            \"Nifi\": \"NiFi\",\n","        }\n","    },\n","    \"aws\": {\n","        \"analytics\": {\n","            \"ElasticsearchService\": \"ES\",\n","        },\n","        \"business\": {\n","            \"AlexaForBusiness\": \"A4B\"\n","        },\n","\n","    Answer in Markdown:\n","\n","## Getting Started\n","\n","To get started with Diagrams, follow these steps:\n","\n","1. Install Diagrams using npm by running `npm install diagrams` in your terminal.\n","2. Create a new diagram by running `npx diagrams init MyDiagram` (replace `MyDiagram` with the name of your diagram).\n","3. Edit the diagram by modifying the `src/diagrams/MyDiagram.json` file.\n","4. Run the diagram by running `npx diagrams run MyDiagram`.\n","\n","That's it! You can now start creating diagrams using Diagrams.\n","\n","Note: This section is just an example, you may want to add more details or links depending on the context of your project.\n","CPU times: user 34.3 s, sys: 7.33 s, total: 41.7 s\n","Wall time: 41.7 s\n"]}],"source":["%%time\n","device = \"cuda\"\n","\n","encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","with torch.no_grad():\n","    outputs = model.generate(\n","        input_ids = encoding.input_ids,\n","        attention_mask = encoding.attention_mask,\n","        generation_config = generation_config\n","    )\n","\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{"id":"QAe7n7T4jP-D"},"source":["## Format our fine-tuning data\n","\n","We'll match the prompt setup we used above."]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.status.busy":"2024-05-23T12:26:55.514848Z","iopub.status.idle":"2024-05-23T12:26:56.254319Z","shell.execute_reply":"2024-05-23T12:26:56.253352Z","shell.execute_reply.started":"2024-05-23T12:26:55.515569Z"},"id":"lm60o2_No7Jz","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map:   0%|          | 0/11372 [00:00<?, ? examples/s]"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 11372/11372 [00:22<00:00, 514.55 examples/s]\n"]}],"source":["def generate_prompt(data_point):\n","#     return f\"\"\"\n","#             {data_point[\"Question\"]}. \n","#             Answer as briefly as possible: {data_point[\"Answer\"]}\n","#             \"\"\".strip()\n","    return f\"\"\"You are an AI assistant for a software project called {data_point[\"Repo\"]}. You are trained on all the {content_type} that makes up this project.\n","    The docs for the project is located at {data_point[\"Repo Url\"]}.\n","    You are given a repository which might contain several modules and each module will contain a set of files.\n","    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\n","    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\n","\n","    Assume the reader is a smart developer but is not deeply familiar with {data_point[\"Repo\"]}.\n","    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\n","    If you don't know how to fill up the readme.md file in one of its sections, leave that part blank. Don't try to make up any content.\n","    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\n","    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\n","\n","    Question: {data_point[\"Question\"]}\n","    Context:\n","    {data_point[\"Context\"]}\n","\n","    Answer in Markdown:\n","    {data_point[\"Answer\"]}\n","    \"\"\"\n","\n","def generate_and_tokenize_prompt(data_point):\n","    full_prompt = generate_prompt(data_point)\n","    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n","    return tokenized_full_prompt\n","\n","data = Dataset.from_pandas(df)\n","data = data.shuffle().map(generate_and_tokenize_prompt)"]},{"cell_type":"markdown","metadata":{"id":"QCrTXUqXk0S9"},"source":["## Train!\n","\n","Now, we'll use our data to update our model. Using the Huggingface `transformers` library, let's set up our training loop and then run it. Note that we are ONLY making one pass on all this data."]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["import os\n","\n","os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n","os.environ['TORCH_USE_CUDA_DSA']=\"1\""]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T12:32:56.159367Z","iopub.status.busy":"2024-05-23T12:32:56.158408Z","iopub.status.idle":"2024-05-23T12:33:03.663596Z","shell.execute_reply":"2024-05-23T12:33:03.662176Z","shell.execute_reply.started":"2024-05-23T12:32:56.159329Z"},"id":"PGneIe1xpUJV","outputId":"8cdac9ac-d6bf-4d8f-b954-febf7f140591","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n","/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='275' max='11372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  275/11372 07:40 < 5:12:16, 0.59 it/s, Epoch 0.02/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 21\u001b[0m\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     18\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/trainer.py:1912\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1913\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1914\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1915\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1916\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1917\u001b[0m     )\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2245\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2248\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2250\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2251\u001b[0m ):\n\u001b[1;32m   2252\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2253\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/trainer.py:3284\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3282\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/accelerate/accelerator.py:2121\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/doc/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["training_args = transformers.TrainingArguments(\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=1,\n","    num_train_epochs=1,\n","    learning_rate=1e-4,\n","    fp16=True,\n","    output_dir=\"outputs_llama2-7b-chat-gptq_v3\",\n","    optim=\"paged_adamw_8bit\",\n","    lr_scheduler_type=\"cosine\",\n","    warmup_ratio=0.01,\n","    report_to=\"none\"\n",")\n","\n","trainer = transformers.Trainer(\n","    model=model,\n","    train_dataset=data,\n","    args=training_args,\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",")\n","model.config.use_cache = False\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"hQRUzbH9oaqG"},"source":["\n"]},{"cell_type":"markdown","metadata":{},"source":["## Loading and using the model later\n","\n","Now, we'll save the PEFT fine-tuned model, then load it and use it to generate some more answers."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T06:59:35.522808Z","iopub.status.busy":"2024-05-08T06:59:35.522441Z","iopub.status.idle":"2024-05-08T06:59:50.303758Z","shell.execute_reply":"2024-05-08T06:59:50.302828Z","shell.execute_reply.started":"2024-05-08T06:59:35.522781Z"},"id":"Vmce-aSesAHV","outputId":"4bf93e78-2a0b-404c-8b05-3748db1bdc52","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["CUDA extension not installed.\n","CUDA extension not installed.\n"]},{"name":"stderr","output_type":"stream","text":["/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/modeling_utils.py:4484: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n","  warnings.warn(\n","You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"]}],"source":["# model.save_pretrained(\"trained-model\")\n","\n","PEFT_MODEL = \"outputs_llama2-7b-chat-gptq_v2/checkpoint-11000\"\n","\n","config = PeftConfig.from_pretrained(PEFT_MODEL)\n","model = AutoModelForCausalLM.from_pretrained(\n","    config.base_model_name_or_path,\n","    return_dict=True,\n","    # quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    trust_remote_code=True\n",")\n","\n","tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","model = PeftModel.from_pretrained(model, PEFT_MODEL)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T07:00:06.408820Z","iopub.status.busy":"2024-05-08T07:00:06.407590Z","iopub.status.idle":"2024-05-08T07:00:06.414259Z","shell.execute_reply":"2024-05-08T07:00:06.413181Z","shell.execute_reply.started":"2024-05-08T07:00:06.408782Z"},"id":"vgIHyPUasD0b","trusted":true},"outputs":[],"source":["generation_config = model.generation_config\n","generation_config.max_new_tokens = 512\n","generation_config.temperature = 0.7\n","generation_config.top_p = 0.7\n","generation_config.num_return_sequences = 1\n","generation_config.repetition_penalty = 1.1\n","generation_config.pad_token_id = tokenizer.eos_token_id\n","generation_config.eos_token_id = tokenizer.eos_token_id"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T07:00:07.966509Z","iopub.status.busy":"2024-05-08T07:00:07.966076Z","iopub.status.idle":"2024-05-08T07:00:07.971440Z","shell.execute_reply":"2024-05-08T07:00:07.970162Z","shell.execute_reply.started":"2024-05-08T07:00:07.966474Z"},"id":"I--juWjcCGpS","trusted":true},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-05-08T07:00:10.373393Z","iopub.status.busy":"2024-05-08T07:00:10.373001Z","iopub.status.idle":"2024-05-08T07:00:12.135173Z","shell.execute_reply":"2024-05-08T07:00:12.134130Z","shell.execute_reply.started":"2024-05-08T07:00:10.373362Z"},"id":"63Zxai-isGhJ","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:533: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/home/scp6004/anaconda3/envs/doc/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:538: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["You are an AI assistant for a software project called diagrams. You are trained on all the docs that makes up this project.\n","    The docs for the project is located at https://github.com/mingrammer/diagrams.\n","    You are given a repository which might contain several modules and each module will contain a set of files.\n","    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\n","    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\n","\n","    Assume the reader is a smart developer but is not deeply familiar with diagrams.\n","    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\n","    If you don't know how to fill up the readme.md file in one of its sections, leave that part blank. Don't try to make up any content.\n","    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\n","    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\n","\n","    Question: Provide the README content for the section with heading \"Getting Started\" starting with ## Getting Started.\n","    Context:\n","    headerLinks: [\n","    {doc: 'getting-started/installation', label: 'Docs'},\n","    {doc: 'guides/diagram', label: 'Guides'},\n","    {doc: 'nodes/aws', label: 'Nodes'},\n","    {href: 'https://github.com/mingrammer/diagrams', label: 'GitHub'},\n","    {href: 'https://www.buymeacoffee.com/mingrammer', label: 'Sponsoring'},\n","  ],\n","\n","  headerIcon: 'img/diagrams.ico',\n","  footerIcon: 'img/diagrams.ico',\n","  favicon: 'img/diagrams.ico',\n","\n","  colors: {\n","    primaryColor: '#5E73E5',\n","    secondaryColor: '#5E89E5',\n","  },\n","\n","  copyright: `Copyright © ${new Date().getFullYear()} mingrammer`,\n","\n","  highlight: {\n","    // Highlight.js theme to use for syntax highlighting in code blocks.\n","    theme: 'default',\n","  },\n","\n","  // Add custom scripts here that would be placed in <script> tags.\n","  scripts: ['https://buttons.github.io/buttons.js'],\n","\n","  // On page navigation for the current documentation page.\n","  onPageNav: 'separate',\n","  cleanUrl: true,\"CommandLineInterface\": \"CLI\",\n","            \"DeveloperTools\": \"DevTools\",\n","        },\n","        \"engagement\": {\n","            \"SimpleEmailServiceSes\": \"SES\",\n","        },\n","        \"general\": {\n","            \"GenericOfficeBuilding\": \"OfficeBuilding\",\n","        },\n","        \"integration\": {\n","            \"SimpleNotificationServiceSns\": \"SNS\",\n","            \"SimpleQueueServiceSqs\": \"SQS\",\n","            \"StepFunctions\": \"SF\",\n","        },\n","        \"iot\": {\n","            \"Freertos\": \"FreeRTOS\",\n","            \"IotHardwareBoard\": \"IotBoard\",\n","        },\n","        \"management\": {\n","            \"SystemsManager\": \"SSM\",\n","            \"SystemsManagerParameterStore\": \"ParameterStore\",\n","        },\n","        \"migration\": {\n","            \"ApplicationDiscoveryService\": \"ADS\",\n","            \"CloudendureMigration\": \"CEM\",\n","            \"DatabaseMigrationService\": \"DMS\",\n","            \"MigrationAndTransfer\": \"MAT\",\n","            \"ServerMigrationService\": \"SMS\",\n","        },\n","        \"ml\": {\n","            \"DeepLearningContainers\": \"DLC\",FILE_PREFIXES = {\n","    \"onprem\": (),\n","    \"aws\": (\"Amazon-\", \"AWS-\"),\n","    \"azure\": (\"Azure-\",),\n","    \"digitalocean\": (),\n","    \"gcp\": (\"Cloud-\",),\n","    \"firebase\": (\"Cloud-\",),\n","    \"ibm\": (),\n","    \"k8s\": (),\n","    \"alibabacloud\": (),\n","    \"oci\": (\"OCI-icon-\",),\n","    \"programming\": (),\n","    \"saas\": (),\n","    \"elastic\": (),\n","    \"outscale\": (),\n","    \"generic\": (),\n","    \"openstack\": (),\n","}\n","\n","#########################\n","#  Doc Auto Generation  #\n","#########################\n","\n","TMPL_APIDOC = \"apidoc.tmpl\"\n","\n","#########################\n","# Class Auto Generation #\n","#########################\n","\n","TMPL_MODULE = \"module.tmpl\"},\n","        \"gitops\": {\n","            \"Argocd\": \"ArgoCD\",\n","        },\n","        \"logging\": {\n","            \"Fluentbit\": \"FluentBit\",\n","            \"Rsyslog\": \"RSyslog\",\n","        },\n","        \"network\": {\n","            \"Etcd\": \"ETCD\",\n","            \"Haproxy\": \"HAProxy\",\n","            \"OpenServiceMesh\": \"OSM\",\n","            \"Opnsense\": \"OPNSense\",\n","            \"Pfsense\": \"PFSense\",\n","            \"Vyos\": \"VyOS\"\n","        },\n","        \"proxmox\": {\n","            \"Pve\": \"ProxmoxVE\",\n","        },\n","        \"queue\": {\n","            \"Activemq\": \"ActiveMQ\",\n","            \"Emqx\": \"EMQX\",\n","            \"Rabbitmq\": \"RabbitMQ\",\n","            \"Zeromq\": \"ZeroMQ\",\n","        },\n","        \"storage\": {\n","            \"Ceph\": \"CEPH\",\n","            \"CephOsd\": \"CEPH_OSD\",\n","        },\n","        \"workflow\": {\n","            \"Kubeflow\": \"KubeFlow\",\n","            \"Nifi\": \"NiFi\",\n","        }\n","    },\n","    \"aws\": {\n","        \"analytics\": {\n","            \"ElasticsearchService\": \"ES\",\n","        },\n","        \"business\": {\n","            \"AlexaForBusiness\": \"A4B\"\n","        },\n","\n","    Answer in Markdown:\n","    Diagrams is a free, open-source diagramming library built on top of ReactJS. It provides a simple and intuitive API for creating diagrams from various types of nodes. It also includes a powerful editor for creating diagrams from scratch. [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![]( [![](\n","CPU times: user 1min 43s, sys: 25.2 s, total: 2min 8s\n","Wall time: 2min 9s\n"]}],"source":["%%time\n","project_name = df[\"Repo\"].values[10820]\n","repository_url = df[\"Repo Url\"].values[10820]\n","target_audience = \"smart developer\"\n","question = df[\"Question\"].values[10820]\n","context = df[\"Context\"].values[10820]\n","content_type = \"docs\"\n","prompt = f\"\"\"You are an AI assistant for a software project called {project_name}. You are trained on all the {content_type} that makes up this project.\n","    The {content_type} for the project is located at {repository_url}.\n","    You are given a repository which might contain several modules and each module will contain a set of files.\n","    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\n","    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\n","\n","    Assume the reader is a {target_audience} but is not deeply familiar with {project_name}.\n","    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\n","    If you don't know how to fill up the readme.md file in one of its sections, leave that part blank. Don't try to make up any content.\n","    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\n","    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\n","\n","    Question: {question}\n","    Context:\n","    {context}\n","\n","    Answer in Markdown:\"\"\"\n","    \n","device = \"cuda\"\n","encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","with torch.inference_mode():\n","  outputs = model.generate(\n","      input_ids = encoding.input_ids,\n","      attention_mask = encoding.attention_mask,\n","      generation_config = generation_config\n","  )\n","\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":412543,"sourceId":789660,"sourceType":"datasetVersion"},{"datasetId":5010308,"sourceId":8417065,"sourceType":"datasetVersion"},{"modelInstanceId":3093,"sourceId":4298,"sourceType":"modelInstanceVersion"},{"modelInstanceId":8318,"sourceId":28785,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
